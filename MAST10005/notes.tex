\documentclass[12pt]{report}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{pgfplots}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

% Create simple plots with consistent styling
\newenvironment{functionplot}[1][
    xmin = -5,
    xmax = 5,
    ymin = -5,
    ymax = 5
]{
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                #1,
                samples = 1000,
                width = 12cm,
                height = 9cm,
                xlabel = \(x\),
                ylabel = \(y\),
                axis lines = middle,
                restrict y to domain = -10:10
            ]
}{
            \end{axis}
        \end{tikzpicture}
    \end {center}
}

\begin{document}
\title{MAST10005 Notes}
\chapter*{Sets}
\begin{flushleft}

\section*{Common Sets}
\begin{itemize}
\item \textbb{N} is the set of positive integers: \(\{ 1, 2, 3, 4, ... \}\)
\item \textbb{Z} is the set of all integers: \(\{ ..., -1, 0, 1, 2, 3, ... \}\)
\item \textbb{Q} is the set of rational numbers, those that can be written as 
fractions
\item \textbb{R} is the set of real numbers.
\item \textbb{C} is the set of complex numbers.        
\end{itemize}


\[\N \subseteq \Z \subseteq \Q \subseteq \R \subseteq \C\]
Some numbers cannot be written as fractions, and these we describe as 
irrational. These include \(\pi\), \(e\), \(\sqrt{2}\), etc. These numbers are 
not in \textbb{Q}, but they are real numbers, i.e. in \textbb{R}. It can be
difficult to prove numbers are irrational. An even larger set of numbers 
exists, that of the complex numbers \textbb{C}. ``Most'' real numbers are 
inrrational.


\section*{Denoting Membership}
If \(A\) is a set, saying \(x \in A\) means \(x\) is in the set \(A\). 
We can also use \(\notin\) to denote non-membership. Sets can be expressed as 
2-dimensional regions, with points within them as elements.

\begin{itemize}
\item \(n \in \Z\) means \(n\) is an integer.
\item \(x \in \Q\) means \(x\) can be expressed as a fraction.
\end{itemize}

Note that \(n\) is used in the first example while \(x\) is used in the second; 
conventionally, \(n\) is used to represent integers.


\section*{Desciptive Notation}
Descriptive notation is a way of defining a set by stating a property which all
of its elements possess. 
For example: 
\[A = \{x \in \R | x^2 + 1 > 37\}\]
Which can be read as the set of all real numbers \(x\) such that 
\(x^2 + 1 > 37\). The statement must be a ``predicate''; it must be true or 
false for all values. The vertical bar is read as ``such that''. A colon is 
sometimes used instead.

\subsubsection*{Examples}

\begin{itemize}
\item Express the set of real numbers whose natural (base \(e\)) logarithm is 
positive with descriptive notation: \({x \in \R | log(x) > 0}\). Note that 
\(log(x)\) denotes \(log_{e}(x)\). There is however an issue with this answer; 
because \(log(x)\) is sometimes undefined, ``\(log(x) > 0\)'' is not a 
predicate. The set could instead be expressed as:
\[\{x \in \R_{\scriptscriptstyle{>0}} | log(x) > 0\}\;\;\;
\R_{\scriptscriptstyle{>0}} = \{x \in \R | x > 0\}\]
\item Express the set of integers whose cube is even in descriptive notation:
\[\{n \in \Z | n^3 \:\mathrm{is\:even}\}\]
\item Describe the set \(\{n \in \N | sin(n) > 0\}\) in words: 
\begin{quotation}
The set of all natural numbers n such that sin(n) is greater than 0.
\end{quotation}
\end{itemize}


\section*{Abbreviated Notation}
Set notation is often abbreviated. For example \(\{x \in \R | sin(x) = 0\}\) 
could be expressed as:
\[\{k\pi | k \in \Z\}\]
This will be all integer multiples of pi; i.e. all values for which sin(x) = 0. 
This could also be expressed as: 
\[\{x \in \R | x = k\pi \:\mathrm{for\:some}\:k \in \Z\}\]
This can be thought of as a kind of ``generating'' notation, whereas 
descriptive notation could be seen to excise a set from a larger set through a
condition.

\subsubsection*{Examples}

\begin{itemize}
\item Express the set of odd integers in abbreviated set notation: 
\[\{2k + 1 | k \in \Z\} \:\mathrm{or}\: \{2k - 1 | k \in \Z\}\]
\item Express the set \(\{x \in \R | cos(x) = 0\}\) in abbreviated set 
notation: 
\[\{\frac{\pi}{2} + k\pi | k \in \Z\}\]
\end{itemize}


\section*{Intervals}
\((a, b)\) means the set of real numbers between \(a\) and \(b\) (exclusive).
Therefore:
\[(a, b) = \{ x \in \R | a < x \:\mathrm{and}\: x < b\} = 
\{ x \in \R | a < x < b\} \]
This can be illustrated on a number line with an open circle at \(a\) and 
\(b\). This is an open interval. To denote a closed interval (inclusive of 
\(a\) and \(b\)), we use a square brackets (\([a, b]\)) and a filled circle.

\subsubsection*{Examples}
Give definitions of \([a, b)\) and \((a, b]\) with diagrams:
\begin{itemize}
\item \([a, b)\) represents all numbers from \(a\) (inclusive) to \(a\) 
(exclusive). On a number line, it would be drawn as a filled dot at \(a\) 
and an open dot at \(b\).
\item \((a, b]\) represents all numbers greater than \(a\) and less than or 
equal to \(b\). It could be illustrated with a number line with an empty dot
 at \(a\) and a filled dot at \(b\).
\end{itemize}


\section*{Unbounded Intervals}
For any \(a \in \R\), 
\[(-\infty, a) = \{x \in \R | x < a\}\]
Note that \(-\infty \:\mathrm{and}\: \infty\) are not elements of \textbb{R}.

\section*{Small Sets}
Sets with finitely many elements can be described with a list of elements.
\[\{x \in \R | x^2 - 1 = 0\} = \{-1, 1\}\]
\[\{x \in \Z | x^2 - 1 < 0\} = \{0\}\]
\[\{x \in \Z | x^3 - x = 0\} = \{-1, 0, 1\}\]

\subsubsection*{Example}
Write the set of prime numbers less than 20 in descriptive and list of elements
form:
\[\{2, 3, 5, 7, 11, 13, 17, 19\}\]

We can use a kind of "list of elements" notation to denote some infinite sets 
with ellipses. e.g.: 
\[(2k + 1)\pi | k \in \Z = \{..., -3\pi, -\pi, \pi, 3\pi, ...\}\]

The smallest set is taken to be the empty set, written as \(\emptyset\). It can
 also be written as \(\{\}\) in list of elements form.

\subsubsection*{Examples}
\[\{x \in \R | x^2 + 1 = 0\} = \emptyset\]
\[\{x \in \R | cos(x) > 1\} = \emptyset\]

\section*{Subsets}
If \(A\) and \(B\) are sets then \(A \subseteq B\) means that every element of
\(A\) is also an element of \(B\). It is read as \(A\) is a subset of \(B\).
\[x \in A \Rightarrow x \in B\]

The empty set is taken to be a subset of every set.

\section*{Proofs}
A proof begins with a set of true assumptions, from which mathematical 
reasoning is used to prove a conclusion.
To prove \(A \subseteq B\), we need to show that if something meets the 
criteria of \(A\), it must also meet those of \(B\).

\subsubsection*{Example}
Prove \(A \subseteq B\) where:
\[A = \{n \in \N | sin(n) > 0\}\]
\[B = \{n \in \N | sin^2(n) \leq sin(n)\}\]
\par
Let \(n \in A\), thus \(sin(n) > 0\).
\par
Since \(sin(n) \leq 1\) and if \(x \leq y\) and \(a > 0\) then \(ax \leq ay\),
we have \(sin(n)sin(n) \leq sin(n)\). Therefore \(n \in B\) so 
\(A \subseteq B\).

\bigskip

To prove a statement is false, we must find a counterexample - an example
for which a statement does not hold. Thus, to prove \(A\) is not a subset of 
\(B\), we must find any \(x\) in \(A\) but not in \(B\).

\subsubsection*{Example}
Prove that \(A \nsubseteq B\) where \(A = \{3n + 1 | n \in \Z\}\) and 
\(B = \{6m + 1 | m \in \Z\}\).
\par
\[A = \{..., -2, 1, 4, 7, ...\}\]
\[B = \{..., 1, 7, ...\}\]
\par
We guess \(4 \notin B\), \(4 \in A\).
To prove this:
\begin{itemize}
\item Claim \(4 \in A\); this is true since \(4 = 3*1 + 1\). 
\item Claim \(4 \notin B\). 
\item Suppose for a contradiction that \(4 \in B\).
\item This would mean \(4 = 6m + 1\) for some \(m \in \Z\).
\item But then, \(3 = 6m\)
\end{itemize}
\[\therefore m = \frac{3}{6} = \frac{1}{2}\]
This is a contradiction! Thus we conclude \(4 \notin B\).


\section*{Proof by Contradiction}

How do we prove that \(x^2 + x + 1 = 0\) has no real solutions?
Proof by contradiction. Assume that there is a solution, \(x \in \R\) and show
that this leads to an absurd conclusion.
\[x^2 + x + 1 = 0 \Rightarrow (x + \frac{1}{2})^2 + \frac{3}{4} = 0 \Rightarrow
(x + \frac{1}{2})^2 = \frac{-3}{4}\]

Note that \(p \Rightarrow q\) means if p is true, so is q. 
\(p \Leftrightarrow q\) means that if either is true, both are true; \(p\) is
true \textit{if and only if} \(q\) is true.

\section*{Union}
The elements of \(A \cup B\) are all the elements that are in either 
\(A\) or \(B\). 

\subsubsection*{Examples}
Express the following sets using a union of intervals.
\begin{itemize}
\item \(\{x \in \R | x^2 > 1\}\): \((-\infty, -1) \cup (1, \infty)\)
\item \(\{x \in (-2\pi, 2\pi] | sin(x) \leq 0\}\): 
\[\{x \in (-2\pi, 0] | sin(x) \leq 0\} \cup \{x \in (0, 2\pi] | sin(x) \leq 0\}
= [-\pi, 0] \cup [\pi, 2\pi]\]
\item \(\{x \in [-2, 2] | x \notin \Z\}\): \((-2, -1) \cup (-1, 0) \cup (0, 1)
\cup (1, 2)\)
\end{itemize}

\[(2, 8) \cup [3, 10] = (2, 10]\]
Thus, \((2, 8) \cup [3, 10]\) is an interval. However is \((0, \sqrt{2}) \cup 
[\frac{\pi}{2}, 3)\) an interval?
\par
It is not, because \(\sqrt{2} < \frac{\pi}{2}\), thus two pieces are necessary.

\section*{Intersection}
\(A \cap B\) is the set of all elements that are in both \(A\) and \(B\).

\subsubsection*{Examples}
\begin{itemize}
    \item Express \((2, 8) \cap [3, 10]\) as an interval: \([3, 8)\)
    \item Express \((0, \sqrt{2}) \cap [\frac{\pi}{2}, 3)\) in the simplest way
    possible: \(\emptyset\)
    \item Express \(\Z \cap [-\pi, \pi]\) in list of elements form: 
    \(\{-3, -2, -1, 0, 1, 2, 3\}\)
    \item Express \(\Z \cap \{x \in \R | x^2 - 5 < 0\}\) in list of elements
    form: \(\{-2, -1, 0, 1, 2\}\)
    \item Express the set of reals with positive sine and negative cosine with
    an intersection: \(\{x \in \R | \mathrm{sin}(x) > 0\} \cap \{x \in \R | 
    \mathrm{cos}(x) < 0\}\)
\end{itemize}

When interpreting questions, a question involving ``and'' is likely to be an intersection question.

\section*{Complement}
For \(A\) and \(B\) the \textit{relative complement} of \(A\) and \(B\) is the 
set of elements that are in \(A\) but not in \(B\). This is written as:
\[A \backslash B = \{x \in A | x \notin B\}\]

\subsubsection*{Examples}
\begin{itemize}
    \item Find \((0, 2) \backslash (1, 3)\): \((0, 1]\)
    \item Find \((1, 3) \backslash (0, 2)\): \([2, 3)\)
    \item Is it generally true that \(A \backslash B = B \backslash A\). 
    No. See previous questions for example. 
\end{itemize}
Express each of the following as a complemet:
\begin{itemize}
    \item \(\{x \in \R | x^2 > 1\}\): \(\R \backslash \{x \in \R | x^2 < 1\}\)
    \item \(\{x \in [-2, 2] | x \notin \Z\}\): \([-2, 2] \backslash \Z\)
    \item \((-\infty, 0) \cup (0, \infty)\): \(\R \backslash \{0\}\)
\end{itemize}

\section*{Cartesian Product}
For sets \(A\) and \(B\):
\[A \times B = \{(x, y) | x \in A \:\mathrm{and}\: y \in B\}\]
\(a, b\) only equals \(a, b\), not \(b, a\).
\par
e.g.:
\[\{0, 1, 5\} \times \{e, \pi\} = 
\{(0, e), (0, \pi), (1, e), (1, \pi), (5, e), (5, \pi)\}\]
We can also take \(A^2\) to be \(A \times A\). This is most often seen in 
\(\R^2\), which is used to represent
the cartesian plane. \(\Z^2 \subseteq \R^2\).


\chapter*{Inequalities}
When we add a constant to an inequality, the inequality is order preserved, 
it still faces the same direction. Subtraction, obviously, obeys the same 
principle.
\[x > y \Rightarrow x + a > y + a\]
When we multiply by a positive constant, \(a > 0\), is order preserving, while
multiplying by a negative constant, \(a < 0\), is order reversing.
As division is simply multiplication by reciprocal, it follows the same 
principles as multiplication; i.e. only order reversing if the divisor \(< 0\).

\subsubsection*{Examples}
Express the set \(A = \{x \in \R | -2 -\frac{1}{2}x > -4\}\) as an interval.
\begin{itemize}
    \item \(2 -\frac{1}{2}x > -2\)
    \item \(-\frac{1}{2} > -2\)
    \item \(x < 4\)
    \item \(x \in (-\infty, 4)\)
\end{itemize}
Express the set \(A = \{x \in \R | 1 - x < 3x + 2\}\) as an interval.
\begin{itemize}
    \item \(-1 -x < 3x\)
    \item \(-1 < 4x\)
    \item \(-\frac{1}{4} < x\)
    \item \(x > -\frac{1}{4}\)
    \item \(A = (-\frac{1}{4}, \infty)\)
\end{itemize}

\subsection*{Transitivity}
\[x < y \:\mathrm{and}\: y < z \Rightarrow x < z\] 
\subsubsection*{Example}
Prove that \(x < y \:\mathrm{and}\: a < b \Rightarrow x + a < y + b\).
\begin{itemize}
    \item \(0 < y - x\)
    \item \(a - b < 0\)
    \item \(a - b < y - x\)
    \item \(a < y - x + b\)
    \item \(a + x < y + b\)
\end{itemize}

\subsection*{Reversing and Non-Reversing Functions}
A function is order preserving if it is strictly increasing for the interval 
containing both sides of an inequailty. It is order-reversing if it is strictly
decreasing for the same. i.e. a function is order preserving if:
\[a < b \Rightarrow f(a) < f(b) \:\mathrm{for\:all}\:a, b \in I\]
and order reversing if:
\[a < b \Rightarrow f(a) > f(b) \:\mathrm{for\:all}\:a, b \in I\]
For example, the logarithm and exponential function are order preserving. A 
function need not be strictly decreasing across \textbb{R} to be applied to an 
inequality; if both sides of an inequality are known to lie within an interval
that is strictly decreasing or increasing, the function may still be applied.

\chapter*{Complex Numbers}
Complex numbers extend the real numbers by adding a new number with the 
property
\[i^2 = -1\]
The usual rules of algebra nonetheless still apply. Though one should note 
that:
\[\sqrt{-1}\sqrt{-1} \neq \sqrt{1}\]
This value can be used to solve equations such as:
\[x^2 + 1 = 0 \Rightarrow x^2 = -1\ \Rightarrow x = \pm i\]
\(i\) can also be used to solve equations such as \(x^2 + 4 = 0\):
\[x^2 + 4 = 0 \Rightarrow x^2 = \sqrt{-2} \Rightarrow x = i\sqrt{2}\]
Solutions of the form \(yi | y \in \R\) are useful for solving many 
polynomials. For some others, such as \(x^2 -2x + 5 = 0\) we need solutions
of the form \(x + iy\):
\[(x - 1)^2 + 4 = 0\ \Rightarrow (x - 1)^2 = -4 \Rightarrow x = 1 \pm 2i\]
Thus, we define complex numbers, typically denoted as \(z\) as a quantity 
consisting of a real number
added to a real multiple of \(i\):
\[z = x + iy\]
Thus \textbb{C} is defined as:
\[\C = \{x + iy | x, y \in \R\}\]

\subsubsection*{Examples}
\begin{itemize}
    \item Using \(i\) write down two square roots of \(-25\): \(5i\), \(-5i\)
    \item Simplify \(i^7\): \(i^0 = 1\), \(i^1 = i\), \(i^2 = -1\), 
    \(i^3 = ii^2 = -i\), \(i^4 = (i^2)^2 = 1\), \(i^5 = i\), \(i^6 = -1\), 
    \(i^7 = i(i^2)^3 = -1^3 = -i\)
\end{itemize}
For a complex number, the real part is \(x\) while \(y\) is known as the 
imaginary part.

\subsubsection*{Example}
For \(z = 2 - 3i\), write down:
\begin{itemize}
    \item \(\mathrm{Re}(z) = 2\)
    \item \(\mathrm{Im}(z) = -3\)
    \item \(\mathrm{Re}(z) - \mathrm{Im}(z) = 5\)
\end{itemize}


\section*{The Complex Plane}
A complex number \(z\) can be seen as a point on a plane, where 
\(\mathrm{Re}(z)\) denotes the \(x\) or horizontal component while 
\(\mathrm{Im}(z)\) denotes the \(y\) or vertical component. This is sometimes
known as the Argand plane.

\bigskip
Addition and subtraction work as expected with complex numbers.
\[z_1 + z_2 = (a + ib) + (c + id) = (a + c) + i(b + d)\]
This can be interpreted as a vector operation, in the same way that adding 
two vectors entails summing their components, complex numbers seen as points 
are effectively vectors which add in essentially the same way. Multiplication 
by a real number is essentially stretching the complex number, the same as 
multiplying a vector by a scalar.

\bigskip
Multiplication is a little more complex.
\[(a + ib)(c + id) = a(c +id) + ib(c + id) =\]
\[ac + aid + ibc + ibid = ac + adi + bci - bd = (ac -bd) + i(ad + bc)\]

The complex conjugate of a number \(z = a + ib\) is denoted \(\bar{z}\) and 
is defined:
\[\bar{z} = a - ib\]
The real part remains the same, while the imaginary part has its sign reversed.
In the plane, this operation represents reflection in the \(x\) or real axis.

\subsubsection*{Example}
By completing the square find the solutions of \(z^2 - 6z + 10 = 0\):
\[z^2 - 6z + 10 = (z - 3)^2 + 1\]
\[(z - 3)^2 = -1\]
\[z - 3 = \pm i\]
\[z = 3 \pm i\]

\subsubsection*{Properties of Conjugates}
Let \(z = x + iy\) and \(w = a + ib\) be complex numbers. Then:
\[z + \bar{z} = 2x = 2\mathrm{Re}(z)\:\mathrm{(real)}\]
\[z - \bar{z} = 2yi = 2 \mathrm{Im}(z)i\]
\[z\bar{z} = x^2 + y^2\:\mathrm{(real)}\]
\[\bar{z + w} = \bar{z} + \bar{w}\]
\[\bar{zw} = \bar{z}\bar{w}\]
\(\sqrt{z\bar{z}}\) can be understood as the length of a complex number by 
Pythagoras' theorem.

\subsubsection*{Division}
\[\frac{1}{i} = -i\]
In general:
\[\frac{\bar{z}}{x^2 + y^2} = \frac{1}{z}\]
We can express the division of two complex numbers as follows:
\[\frac{a + ib}{c + id} = \frac{a + ib}{c + id}\times\frac{c - id}{c - id} = 
\frac{(a + ib)(c - id)}{c^2 + d^2}\]
\[= \frac{1}{c^2 + d^2}(ac + bd + i(-ad + bc))\]
In essence, we rationalise the denominator, much as with surds.

\subsubsection*{Examples}
\[\frac{1 + 2i}{-1 + 3i} = \frac{1 + 2i}{-1 + 3i} \times 
\frac{-1 - 3i}{-1 - 3i} = \frac{(1 + 2i)(-1 - 3i)}{1 + 9}\]
\[=\frac{1 - i}{2}\]
Calculate: \((1 + \sqrt{3}i)^6\)
\[(1 + \sqrt{3}i)^6 = ((1 + \sqrt{3}i)^2)^2 \times (1 + \sqrt{3}i)^2\]
\[(1 + \sqrt{3}i)^2 = 1 + 2\sqrt{3}i + (\sqrt{3}i)^2 = 1 + 2\sqrt{3}i - 3 = 
-2 + 2\sqrt{3}i\]
\[((1 + \sqrt{3}i)^2)^2 = (-2 + 2\sqrt{3}i)^2 = (-2)^2 + 2 \times (-2) 
\times 2\sqrt{3}i + (2\sqrt{3}i)^2\]
\[= 4 - 8\sqrt{3}i - 12 = -8 - 8 \sqrt{3}i\]
\[(1 + \sqrt{3}i)^6 = (-8 -8\sqrt{3}i)(-2 + 2\sqrt{3}i)\]
\[= 16 - 16\sqrt{3}i + 16\sqrt{3}i - 8 \times 2 \times (\sqrt{3})^2i^2\]
\[= 16 + 8 \times 2 \times 3\]
\[= 64\]

\subsubsection*{Polar Form}
To solve this problem more easily, we can instead consider complex numbers in
polar form. This takes the length of the line and it's angle from the origin 
instead. In this case we take a distance \(r\) and an angle \(\theta\).
\[r = |z| = \sqrt{x^2 + y^2}\]
To find \(\theta\) we can draw \(z\) in the plane and use triangles to find 
\(\theta\). \(\theta \in (-\pi, \pi]\), and is called the principal argument of
\(z\) and can be denoted \(\mathrm{Arg}(z)\).

\subsubsection*{Examples}
\begin{itemize}
    \item \(1 + \sqrt{3}i\): \(|z| = 2, \theta = \frac{\pi}{3}\)
    \item \(-3 - 3i\): \(|z| = \sqrt{18} = 3\sqrt{2}, \theta = 
    \frac{-3\pi}{4}\)
    \item \(3 + 4i\): \(|z| = 5, \theta = \arctan(\frac{4}{3})\)
    \item \(-1\): \(|z| = 1, \theta = \pi\)
\end{itemize}

The trigonemtric polar form of a complex number is:
\[z = r(\cos(\theta) + i\sin(\theta)))\]

\subsection*{The Complex Exponential}

For any \(\theta\), the complex exponential is given by:

\[e^{i\theta} = 1 + i\theta + \frac{1}{2!}(i\theta)^2 + 
\frac{1}{3!}(i\theta)^3 \: ...\]
\[e^{i\theta} = \cos(\theta) + i\sin\theta\]
\[\abs{e^{i\theta}} = 1\]

\subsubsection*{Exponential Polar Form}

Knowing this, we can write a given complex number \(z\) in exponential polar 
form:
\[z = re^{i\theta}\]
This happens to be an extremely useful way of expressing complex numbers. An 
example:

\bigskip
Express \(z = -2 + 2\sqrt{3}i\) in exponential polar form.
\[z = 4(-\frac{1}{2} + \frac{\sqrt{3}}{2}i)\]
\[4e^{i\frac{2\pi}{3}}\]
The complex exponential indicates a point on the unit circle, with the scalar r
modifying this vector to point to the actual location of \(z\).

\bigskip
The conjugate of a number in exponential form is given by:
\[e^{-i\theta} = \overline{e^{i\theta}}\]
It takes the form of inversion across the x-axis.

\subsubsection*{Properties of the Complex Exponential}
\[e^{i0} = 1\]
\[e^{i\theta_1}e^{i\theta_2} = e^{i\theta_1+i\theta_2} = 
e^{i(\theta_1+\theta_2)}\]
\[\frac{e^{i\theta_1}}{e^{i\theta_2}} = e^{i\theta_1 - i\theta_2} = 
e^{i(\theta_1-\theta_2)}\]
\[e^{i\theta_1} = e^{i\theta_2} \Leftrightarrow \theta_2 = \theta_1 + 2k\pi\]

Multiplication by \(e^{i\theta}\) is equivalent to rotation by\(\theta\). 
For two complex numbers, \(z_1\) and \(z_2\), the product is given by:
\[z_1z_2 = r_1r_2e^{i(\theta_1+\theta_2)}\]
Thus, the modulus of the result is simply the product of the two inputs 
modulus. The angle is increased by \(\theta\). It is intuitive that 
multiplication by \(i^2\) constitutes inversion of the vector; reflection in
the y-axis. The resultant angle will not necessarily be the principle argument;
one can simply add or subtract \(2\pi\) to find the principle argument.

\bigskip
Division is simply given by:
\[\frac{z_1}{z_2} = \frac{r_1}{r_2}e^{i(\theta_1-\theta_2)}\]

Exponentiation works through:
\[z^n = r^ne^{in\theta}\]

\subsection*{Subsets of the Complex Plane}
We can represent certain shapes in the complex plane through sets. For example:
\(\{z \in \C | \mathrm{Re}(z) = 3\}\) describes the line \(x = 3\). A
circle can be drawn as \(\{z \in \C:  \abs{z - p} = r\}\), where \(p\) is
the center point and \(r\) is the radius.

\subsection*{Roots}
Suppose we are trying to find all of the \(n\)-th roots of a complex 
number \(w\). This means we need to find all \(z\)s that satisfy \(z^n = w\).
Writing \(z = re^{i\theta}\) gives us:
\[(re^{i\theta})^n = se^{i\alpha} \Rightarrow se^{i\alpha}\]
\[\Rightarrow \abs{z} = \abs{s} \Rightarrow r^n = s \Rightarrow 
r = s^\frac{1}{n} = \sqrt[^n]{s}\]
We also have:
\[e^{in\theta} = e^{i\alpha} \Rightarrow n\theta = \alpha + 2k\pi \Rightarrow 
\theta = \frac{1}{n}(\alpha + 2k\pi)\]
Curiously, for power \(n\), there will be \(n\) solutions. Using the 
\(\alpha + 2k\pi\) equation, we can find an infinite number of solutions. 
However, we can stop at \(n - 1\), because \(n = k\) will yield the same result
as \(k = 0\), as it will simply add \(2pi\) to the angle. Thus our solutions 
will be of the form:
\[\{\sqrt[^n]{s}e^{i\frac{1}{n}(\alpha + 2k\pi)} | k \in [0, n) \cap \N\}\]

\subsubsection*{Example}
Find the set of cube roots of \(-8\).
\begin{itemize}
    \item Solve to find \(z\) such that \(z^3 = -8\)
    \item Thus \(s = 8\), \(\alpha = \pi\)
    \item \(z = re^{i\theta}, r^3e^{i3\theta} = 8e^{i\pi}\)
    \item \(\therefore r^3 = 8 \Rightarrow r = 2\)
    \item \(3\theta = \pi + 2k\pi, k \in {0, 1, 2}\)
    \item \(\therefore \theta = \frac{\pi}{3}, \pi, \frac{5\pi}{3}\)
    \item \(z \in \{2e^{i\frac{pi}{3}}, 2e^{i\pi}, 2e^{i\frac{5\pi}{3}}\}\)
    \item Note we found the expected root; \(2e^{i\pi} = -2\)
\end{itemize}

These solutions represent 3 evenly spaced points on the circle with center 
\((0, 0)\) and radius 2.

\subsection*{Trigonometric Functions}
The complex exponential can be used to express \(\cos\) and \(\sin\) in a 
useful form for many calculations.
\[e^{i\theta} = \cos(\theta) + i\sin(\theta)\]
\[\overline{e^{i\theta}} = \cos(\theta) - i\sin(\theta)\]
\[e^{i\theta}+\overline{e^{i\theta}} = 2\cos(\theta)\]
\[\therefore \cos(\theta) = \frac{1}{2}(e^{i\theta}{e^{-i\theta}})\]
This is obvious when we consider that \(z + \bar{z} = 2\mathrm{Re}(z)\). 
\(\sin\) found through the same logic is:
\[\sin(\theta) = \frac{1}{2i}(e^{i\theta}-e^{-i\theta})\]

Using the binomial formula, we can expand statements 
such as \(\sin^4(\theta)\):
\[\sin^4(\theta) = (\frac{1}{2i})^4(e^{i\theta}-e^{-i\theta})^4\]
\[\frac{1}{2^4}((e^{i\theta})^4 - 4(e^{i\theta})^3e^{-i\theta} + 
6(e^{i\theta})^2(e^{-i\theta})^2 - 
4e^{i\theta}(e^{-i\theta})^3 + (e^{-i\theta})^4)\]
\[\frac{1}{2^4}(e^{i4\theta} - 4e^{i2\theta} + 6 - 4e^{-i2\theta} + 
e^{-i4\theta})\]
\[6 + 2\cos(4\theta) - 4 \times 2\cos(2\theta)\]
\[\sin^4(\theta) = \frac{1}{8}(3 + cos(4\theta) - 4cos(2\theta))\]

Using this approach, we can convert expressions of the form 
\(\sin^m(\theta)\cos^n(\theta)\) to an equivalent form that is often more 
useable.

\subsection*{Square Roots}
For \(w = re^{i\theta} \in \C \backslash \{0\}\), the root finding formula 
gives two solutions of \(z^2 = w\):
\[z_1 = \sqrt{r}e^{i(\frac{\theta}{2})}, z_2 = 
\sqrt{r}e^{i(\frac{\theta}{2} + \pi)} = -z_1\]
\(z_1\) is called the \textit{principal square root} of \(w\), thus the square
roots are \(\pm \sqrt{w}\). For a negative real number, \(\sqrt{w}\) will be 
equal to \(\sqrt{\:\abs{w}}i\).

\section*{Polynomials}
Real polynomials are those with coefficients exclusively in \(\R\), while 
complex polynomials extend this bank of coefficients to include all of \(\C\).
All real polynomials are complex polynomials, but the inverse is not 
necessarily true.

\subsection*{Factorising Polynomials}
To solve differential equations, it is important to be able to integrate 
polynomials of the form:
\[\frac{p(x)}{q(x)}\]
We can use the quadratic formula to solve complex polynomials just as we would
for real numbers.
\[\frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]
For example, factorising \(p(z) = z^2 -3iz - 2\):
\[\frac{3i \pm \sqrt{-9 -4 \times (1) \times (-2)}}{2}\]
\[= \frac{3}{2}i \pm \frac{1}{2}\sqrt{-1} = \frac{3}{2}i \pm \frac{1}{2}i\]
\[\therefore p(z) = (z - 2i)(z - i)\]
Any complex polynomial of degree \(n\) can be factorised into \(n\) linear 
factors over \(\C\).
\[p(z) = a(z - \alpha_1)(z - \alpha_2) ... (z - \alpha_n)\]
Thus, there is an \(x\) intercept at each value of \(\alpha\). For a 
polynomial of degree \(n\), there at \(\leq n\) roots in \(\C\).

\bigskip
If \(p(z)\) is a real polynomial; the non-real roots of \(p(z)\) occur 
in complex conjugate pairs \(z\) and \(\bar{z}\). For example:
\[p(z) = z^3 - 3iz^2 - 2z = z(z^2 - 3iz - 2) = 
z(z - 2i)(z - i) = (z - 0)(z - 2i)(z - i)\]
Because in this example, our polynomial is non-real, the roots are not in 
conjugate pairs. How about:
\begin{center}
    \[\{z \in \C | z^4 + z^2 -12 = 0\}\]
    \[\mathrm{set}\:w = z^2 \Rightarrow p(z) = w^2 + w - 12 = 0\]
    Now, we need to factor \(p(z)\) in terms of \(w\), and solve \(z^2 = w\).
    \[w^2 + w - 12 = (w - 3)(w + 4)\]
    \[z^2 = 3 \Rightarrow z = \sqrt{3}, -\sqrt{3}\]
    \[z^2 = -4 \Rightarrow z = 2i, -2i\]
    \[\{\sqrt{3}, -\sqrt{3}, 2i, -2i\}\]
\end{center}

\bigskip
In this example, because our roots are real, our solutions come in complex 
conjugate pairs.

\bigskip
If the coefficients of a polynomial \(p(x)\) are all real, then \(p(x)\) can be
written as a product of linear and irreducible quadratic factors with 
exclusively real coefficients. Considering the previous example:
\[p(z) = (z - \sqrt{3})(z + \sqrt{3})(z - 2i)(z + 2i)\]
\[= (z - \sqrt{3})(z + \sqrt{3})(z^2 + 2iz - 2iz - 4i^2)\]
\[= (z - \sqrt{3})(z + \sqrt{3})(z^2 + 4)\]

\subsubsection*{Geometric Progression}
For any \(z \in \C \backslash \{1\}\):
\[z^n + z^{n - 1} + ... + z + 1 = \frac{z^{n + 1} - 1}{z - 1}\]
Using this, let us factor \(p(z) = z^3 + z^2 + z + 1\) over \(\R\).
\begin{center}
    \[(z - 1)(z^3 + z^2 + z + 1) = z^4 - 1\]
    The roots of the RHS are the 4th roots of \(w = 1\) which are:
    \[1, -1, i, -i\]
    We can also discover these by the general method:
    \[z^4 = 1 \Rightarrow r^4e^{i4\theta} = 1e^{i0}\]
    \[\Rightarrow r = 1,\: 4\theta = 2k\pi,\: k = 0, 1, 2, 3 \:\:\: 
    \theta = \frac{1}{2}k\pi,\: k = 0, 1, 2, 3\]
    \[= 0, \frac{\pi}{2}, \pi, \frac{3\pi}{2} = 1, i, -1, -i\]
    Because these are the roots of \(z^4 - 1\), and we know that \(z - 1\) has
    the root of \(1\), we know that the roots of our polynomial are:
    \[z^3 + z^2 + z + 1 = (z + 1)(z - i)(z + i)\]
\end{center}

\section*{Functions}
Functions are a way of expressing quantities that depend on one another.
For example, a function could express the temperature of a beverage placed on a
counter with respect to the time passed since the object was placed there.

\bigskip
More technically, a function accepts an input and maps it to an output. These
inputs are drawn from a nonempty set \(A\) called the \textit{domain} of \(f\).
The outputs are drawn from a second nonempty set, \(B\), known as the codomain
of \(f\). Finally there is a rule that associates each element of \(A\) to a 
unique element of \(B\). To express a function \(f\) with domain \(A\) and 
codomain \(B\) we write:
\[f: A \rightarrow B\]
This can also be read as \(f\) maps \(A\) into \(B\). For example
\[f: \R \rightarrow \R, f(x) = \frac{x + 1}{x^2 + 4}\]
is a valid function. However,
\[h: \C \rightarrow \C, h(z) = \frac{x + 1}{x^2 + 4}\]
is not, as the rule \(h(z)\) is undefined for some \(z \in \C\).
To make it a valid function, we could change the domain to
\(\C \backslash \{2i, -2i\}\).
When a function has a domain and codomain as subsets of \(\R\), we can
visualise it as a graph. Formally, a graph of \(f: A \rightarrow B\) is the set
defined by:
\[\{(a, f(a)) | a \in A\} \subseteq A \times B\]
i.e. all points that can be generated by the form (input, output). The codomain
of a function does not necessarily need to be minimal or maximal. For 
\(f(x) = \sin(x)\), the range of the function is \([-1, 1]\), but any codomain
inclusive of this range (such as \(\R\)) can be chosen.

\bigskip
The image of a set under a function is the outputs of each element of that set 
when passed through the function. So, for a function \(f: A \rightarrow B\) and
a set \(S \subseteq A\), the image of \(S\) under \(f\) is:
\[f(S) = \{b \in B | b = f(s), s \in S\} = \{f(s) | s \in S\}\]
The range of a function is simply the image of the domain under the function. 
This range is the smallest valid codomain for the function, and thus 
\(\mathrm{range} \subseteq \mathrm{codomain}\) in all cases.

\subsubsection*{Description of Functions}
\textit{Injective} functions are one-to-one functions. This means that for a 
given input, there will be a unique output, shared by no other inputs; for two
different inputs, there will be two different outputs. To prove injectivity, 
we can show \(f(x) = f(y) \Rightarrow x = y\).

\bigskip
\textit{Monotone} functions are either increasing or decreasing across their 
entire domain. These functions are always injective.

\bigskip
\textit{Surjective} functions map at least one input value to each legal output
value. So for each element of the codomain, there is at least one element of 
the domain which maps to it. This can be thought of as the function having a 
minimal codomain; only possible outputs are included in the codomain.

\bigskip
\textit{Bijective} functions are both injective and surjective. There is a 
precise correspondence between each element of \(A\) and each element of \(B\);
each element in \(A\) has a partner in \(B\) with no excess elements in \(B\). 
For example a simple bijective function is
\[f: (0, \infty) \rightarrow (0, \infty)\:\:\:f(x) = x\]
A slightly more complex example could be:
\[g: (0, \infty) \rightarrow (0, \infty)\:\:\:g(x) = \frac{1}{x}\]

\subsubsection*{Function Equality}
For functions to be equal, all of their attributes must be the same. This 
implies that the domain, codomain and function are the same. However, the 
syntax of the function could be different; \(f(x) = sin(2x)\) is equivalent
to \(g(x) = 2sin(x)cos(x)\). 

\subsubsection*{Composition}
Composition of functions entails taking the output of one function and feeding
it into another. i.e. taking \(f(g(x))\). This is denoted by:
\[(f \circ g)(x) = f(g(x))\]
For a function composition to be valid the codomain of \(f\) must be a subset 
of the domain of \(g\). The function \(f \circ g\) will accept an argument 
from the domain of \(f\) and output from the codomain of \(g\). 
If \(f: A \rightarrow B\) and \(g: C \rightarrow D\):
\[(f \circ g): A \rightarrow D\]
Composition is not commutative: \(g \circ f \neq f \circ g\). It is however
associative: \(h \circ (f \circ g) = (h \circ f) \circ g\).

\bigskip
When composing functions, we may know the domain and codomain of each of the
parts. For example, taking the following functions, we know:
\[f: (0, \infty) \rightarrow \R \:\:\: f(x) = \log(x)\]
\[g: \R \rightarrow (-\infty, 1] \:\:\: g(x) = 1 - x^2\]
But what of the composition?
\[(f \circ g)(x) = \log(1 - x^2)\]
Let us consider this problem more generally. For two functions:
\[f: \mathrm{dom}(f) \rightarrow \mathrm{range}(f)\]
\[g: \mathrm{dom}(g) \rightarrow \mathrm{range}(g)\]
For composition to be possible, \(\mathrm{range}(g)\) and \(\mathrm{dom}(f)\) 
must intersect. Otherwise, the resultant function would have an empty domain. 
Thus, the implied domain of the resultant function is:
\[\mathrm{dom}(f \circ g) = \{x \in \mathrm{dom}(g) 
| g(x) \in \mathrm{dom}(f)\}\]
And the implied range is given by:
\[\mathrm{range}(f \circ g) = f(\mathrm{range}(g) \cap \mathrm{dom}(f))\]
So for our original functions:
\[\mathrm{dom}(f \circ g) = \{x \in R | g(x) \in (0, \infty)\} = (-1, 1)\]
\[\mathrm{range}(f \circ g) = f((-\infty, 1] \cap (0, \infty)) = f((0, 1]) 
= (-\infty, 0]\]

\subsubsection*{Inverses}
If \(f: A \rightarrow B\) the inverse of \(f\) will be \(g: B \rightarrow A\)
such that \(g \circ f(x) = x\) and \(f \circ g(y) = y\) i.e. \(f\) undoes \(g\)
and \(g\) undoes \(f\). Inverse functions are always unique. A function must be
bijective to have an inverse function. To find an inverse function, one can 
observe the operations carried out by the function and then execute the 
inverses of these operations in the reverse order.

\bigskip
Inverse functions are used implicitly to solve equations such as 
\(e^{x^3 + 1} = e^3\): it is our knowledge that an inverse function to \(e^x\)
exists (\(\log(x)\)) that allows us to state with confidence that 
\(x^3 + 1 = 3\) and solve the equation.

For example, we can ask does \(f: \C \rightarrow \C\:\:\:f(z) = \bar{z}\) 
have an inverse? Is it bijective?

\bigskip
Yes it does. Since \(\bar{\bar{z}} = z\), \(f\) is its own inverse.

\bigskip
Sometimes, we will want to find an inverse of a function which is not 
bijective. To create a bijective function, we can restrict domain or codomain 
to ensure these characteristics. To create an injective function, we can find a
range which is strictly increasing or decreasing and restrict the domain to 
this range. To create a surjective function, we can simply remove all elements 
of the codomain which the input doesn't map to i.e. we can restrict the 
codomain of the function to its range.

\bigskip
As an example, the \(\sin\) function is not injective. Thus, we take the domain
\([\frac{-\pi}{2}, \frac{\pi}{2}]\) to yield the inverse function arcsine. Note
that the range restrictions mean that while \(\theta = \arcsin(y) \Rightarrow 
\sin(\theta) = y\), \(\sin(\theta) = x \Rightarrow \theta = \arcsin(x)\) only 
if \(\theta \in [\frac{-\pi}{2}, \frac{\pi}{2}]\)

\bigskip
Graphically, an inverse function can be understood stood as reflection across the 
line \(x = y\).

\subsubsection*{Inverse Trigonometric Functions}
Alongside \(\arcsin\) we additionally have \(\arccos\) and \(\arctan\).
While \(\arctan\) has the same domain of \([\frac{-\pi}{2}, \frac{\pi}{2}]\) as
\(\arcsin\), \(\arccos\) takes the domain \([0, \pi]\) as it is strictly 
positive in \([\frac{-\pi}{2}, \frac{\pi}{2}]\).

\[\mathrm{cosec}(x) = \frac{1}{\sin(x)}\]
\[\mathrm{secant}(x) = \frac{1}{\cos(x)}\]
\[\mathrm{cot}(x) = \frac{\cos(x)}{\sin(x)}\]
It turns out that \(\mathrm{cot}(x)\) is not strictly the inverse function of 
\(\tan(x)\). Because \(\tan{x} = 0\) for multiples of \(\pi\), these values 
would be excluded from the function of \(\frac{1}{\tan(x)}\), which they are 
in \(\mathrm{cot}(x)\), however, the values for which \(\tan(x)\) is undefined,
(\(\{k\pi + \frac{\pi}{2} | k \in \Z\}\)) are defined for \(\mathrm{cot}(x)\) 
thus yielding a different domain to that of \(\frac{1}{\tan(x)}\).

\section*{Vectors}
Because not all quantities are measurable by a scalar value, we sometimes need
to reach for a vector, which by being a representation of multiple values can 
offer both magnitude and direction. Formally, a vector is a member of the set 
\(\R^2\) where
\[\R^2 = \R \times \R = \set{(a_1, a_2) | a_1 \in \R \:\mathrm{and}\: 
a_2 \in \R}\]
We can consider higher dimensions through \(n\)-dimensional space 
with \(\R^n\):
\[\R^n = \set{(a_1, a_2, ..., a_n) | a_1, a_2, ..., a_n \in \R}\]
Vectors have definitions of arithmetical operations \textit{component-wise}.
\begin{itemize}
    \item Addition: \(\vec{a} + \vec{b} = (a_1, a_2) + (b_1, b_2) = 
    (a_1 + b_1, a_2 + b_2)\)
    \item Subtraction: \(\vec{a} - \vec{b} = (a_1, a_2) - (b_1, b_2) = 
    (a_1 - b_1, a_2 - b_2)\)
    \item Multiplication \textit{by scalar}: \(n \times \vec{a} = 
    (n \times a_1, n \times a_2)\)
\end{itemize}
Vectors cannot be multiplied or divided by other vectors. It is important to 
understand what the elements of a vector stand for; in general vectors with 
differing numbers of elements may not be added.

\bigskip
For two points \(A = (a_1, a_2)\) and \(B = (b_1, b_2)\) the vector 
displacement \(\vec{AB}\) is defined as the vector subtraction of \(A\) 
from \(B\) i.e.
\[\vec{AB} = (b_1 - a_1, b_2 - a_2)\]
If we take the origin as \(O\), \(\vec{AB} = \vec{OB} - \vec{OA}\).

\bigskip
Two vectors are parallel if through multiplication by some constant one can
be transformed into the other. This is obvious when we consider that this is 
essentially the same as saying they point in the same direction, as a 
consequence of the fact that scalar multiplication doesn't change direction.

\bigskip
The length or \textit{norm} of a vector can be determined through Pythagoras's 
Theorem, and is denoted \(\norm{\vec{v}}\):
\[\norm{\vec{v}} = \sqrt{v_1^2 + v_2^2}\]
In higher dimensions, this extends simply through:
\[\norm{\vec{v}} = \sqrt{v_1^2 + v_2^2 + ... + v_n^2}\]
Which essentially corresponds to the recursive application of Pythagoras in each
dimension.

\bigskip
Some important properties of vector norms:
\begin{itemize}
    \item For any \(\vec{u}\), \(\norm{\vec{u}} = 0\) precisely if 
    \(\vec{u} = 0\).
    \item \(\norm{\lambda\vec{u}} = |\lambda|\norm{\vec{u}}\) for any 
    \(\lambda\in\R\) and and vector \(\vec{u}\).
    \item \(\norm{\vec{u} + \vec{v}} \leq \norm{\vec{u}} + \norm{\vec{v}}\) 
    for any \(\vec{u}, \vec{v}\).
\end{itemize}

Vectors of length \(1\) are called \textit{unit vectors}. They are denoted by
the ``hat'': \(\hat{u}\). These are special, because they describe a direction.
To find a unit vector for a given vector \(\vec{u}\):
\[\hat{u} = \frac{1}{\norm{\vec{u}}}\vec{u} = \frac{\vec{u}}{\norm{\vec{u}}}\]
So for a piecewise example:
\[\vec{u} = (1, 2, -1)\]
\[\norm{\vec{u}} = \sqrt{1^2 + 2^2 + (-1)^2} = \sqrt{6}\]
\[\hat{u} = (\frac{1}{\sqrt{6}}, \frac{2}{\sqrt{6}}, \frac{-1}{\sqrt{6}})\]

\bigskip
There are standard unit vectors for the axis vectors up to \(\R^3\):
\begin{itemize}
    \item \(\hat{i} = (1, 0, 0)\)
    \item \(\hat{j} = (0, 1, 0)\)
    \item \(\hat{k} = (0, 0, 1)\)
\end{itemize}
These can be used to write any vector in \(\R^3\).

\subsubsection*{The Dot Product}
For two vectors \(\vec{u}\) and \(\vec{v}\) we take the scalar or dot product 
to be
\[\vec{u}\cdot\vec{v} = u_1v_1 + u_2v_2 + ... + u_nv_n\]
This operation is commutative and distributive. Order does not matter, and the
operation applied to a set of parenthesis will affect all operations within the
parenthesis. In addition, scalar multiplication by either of the vectors before 
the operation is the same as scalar multiplication by the resultant vector. 
One other useful property is that:
\[\vec{u}\cdot\vec{u} = \norm{\vec{u}}^2\]
For \(\vec{u}\) and \(\vec{v}\), the \textit{angle between} them is \(\theta\),
which will always be \(\leq\pi\). This angle is closely related to the dot 
product \(\vec{u}\cdot\vec{v}\). Using the Law of Cosines 
(\(c^2 = a^2 + b^2 - 2ab\cos(C)\)), we can find that:
\[\norm{\vec{v} - \vec{u}}^2 = \norm{\vec{u}}^2 + 
\norm{\vec{v}}^2 - 2\norm{\vec{u}}\norm{\vec{v}}\cos(\theta)\]
Rearranging this, we can solve for \(\cos(\theta)\)
\[\cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{\norm{\vec{u}}\norm{\vec{v}}}\]
\[\theta = \arccos\left(\
\frac{\vec{u}\cdot\vec{v}}{\norm{\vec{u}}\norm{\vec{v}}}\right)\]
Some information can be drawn from the dot product with using trigonemtric
functions. Depending on the sign of the dot product we can determine the 
following:
\begin{itemize}
    \item \(\theta\) is acute if \(\vec{u}\cdot\vec{v} > 0\)
    \item \(\theta\) is obtuse if \(\vec{u}\cdot\vec{v} < 0\)
    \item \(\theta = \frac{\pi}{2}\) if \(\vec{u}\cdot\vec{v} = 0\)
\end{itemize}

\subsubsection*{Vector Projections}
Vector projections describe the subsection of a vector which lies along another
vector. A common example is the representation of vectors through \((a\hat{i},
b\hat(j), c\hat(k))\). In this form, \(a\hat{i}\) is the parallel projection of
the vector onto \(\hat{i}\).

\bigskip
If we have vectors \(\vec{u}\) and \(\vec{v}\) and we want to find the vector
projection of \(\vec{v}\) onto \(\vec{u}\), essentially what we are trying to
do is find some scalar \(k\) such that a third vector \(\vec{w}\) is 
perpendicular (or \textit{orthogonal}) to \(\vec{u}\) when defined as:
\[\vec{w} = \vec{v} - k\vec{u}\]
Then, the parallel projection of \(\vec{v}\) onto \(\vec{u}\), denoted 
\(\vec{v}_{\parallel \vec{u}}\), is \(k\vec{u}\). The perpendicular 
projection of \(\vec{v}\) onto \(\vec{u}\), written as 
\(\vec{v}_{\perp \vec{u}}\), is \(\vec{w}\), or \(\vec{v} - k\vec{u}\).
Since vectors are perpendicular when their dot product is \(0\), this means
we want the solution to:
\[\vec{u}\cdot(\vec{v} - k\vec{u}) = 0\]
After rearranging, this formula is:
\[k = \frac{\vec{u}\cdot\vec{v}}{\vec{u}\cdot\vec{u}}\]
If the value of \(k\) is negative, the means the projetion is in the opposite
direction to \(\vec{u}\) and that the angle between \(\vec{u}\) and \(\vec{v}\)
is obtuse. When \(k\) is \(0\), the angle is \(\frac{\pi}{2}\) and If \(k\) is 
positive, the angle is acute.  

\subsubsection*{Example}
Find the closest point \(Q\) on a line \(L\) which passes through 
\(A = (0, 1)\) and \(B = (4, 3)\) to the point \(P = (1, 3)\). How far away
is this point from \(L\).
\begin{itemize}
    \item The closest point to a line will be perpendicular to the line.
    \item We can use a perpendicular projection to find this point.
    \item Taking our base vector to be \(\vec{AP} = (1, 3) - (0, 1) = (1, 2)\)
    \item And projecting on to \(\vec{AB} = (4, 3) - (0, 1) = (4, 2)\)
    \item Then \(k = \vec{AB}\cdot\vec{AP} \div \vec{AB}\cdot\vec{AB}
    = \frac{8}{20} = \frac{2}{5}\)
    \item Our perpendicular projection is given by \(\vec{AP} - k\vec{AB}\)
    \item \(\vec{AP}_{\perp\vec{AB}} = (1, 2) - \frac{2}{5}(4, 2) = (1, 2) - 
    (\frac{8}{5}, \frac{4}{5}) = (\frac{-3}{5}, \frac{6}{5})\)
    \item The distance from \(Q\) to \(P\) is 
    \(\norm{\vec{AP}_{\perp\vec{AB}}} = \frac{3}{\sqrt{5}}\)
    \item To find \(Q\), we can use \(\vec{OQ} = \vec{OA} = \vec{AQ}\)
    \item \(\vec{OA} = A, \vec{AQ} = (\frac{8}{5}, \frac{4}{5}\) as 
    calculated previously
    \item Thus, \(\vec{OQ} = (\frac{8}{5}, \frac{9}{5})\)
\end{itemize}

\subsubsection*{Parametric Curves}
With the tool of vectors of the form \((x\hat{i}, y\hat{j})\), we gain access
to a new construct of vectors which vary with time.
\[\vec{r}(t) = x(t)\hat{i} + y(t)\hat{j}\]
Here, \(\vec{r}\) is a function which maps a real number \(t\), usually time, 
to a vector. \(x(t)\) and \(y(t)\) are known as \textit{parametric equations} 
as they depend on the parameter \(t\). The curve which \(\vec{r}\) traces
out in \(\R^2\) is known as a \textit{parametric curve}. Functions of this form
are useful for tasks such as describing the motion of a particle with time. The
graph resultant from this, i.e. the set of points, is the range of the 
function.

\bigskip
Sometimes, it can be useful to find the equation of a path. For example, 
consider
\[\vec{r}(t) = t\hat{i} + t^2\hat{j}\]
For this equation we can solve for a function with the simultaneous 
equations
\[t = x\]
\[y = t^2 = x^2\]
Thus, the cartesian equal of this curve is \(y = x^2\). In many cases it is
difficult or impossible to find such an equation. The goal is essentially
to eliminate \(t\) from the equation of one of the equations.

\bigskip
An application of parametric functions is in the modelling of ellipses. An
ellipse of width \(a\) and height \(b\) has a cartesian equation of
\[\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 \:\: a, b > 0\]
This path can be parametrised through
\[\vec{r}(t) = a\cos(t)\hat{i} + b\sin(t)\hat{j}\]
This is proved by substituting \(a\cos(t)\) for \(x\) and \(b\sin(t)\) for 
\(y\):
\[\left(\frac{a\cos(t)}{a}\right)^2 + \left(\frac{b\sin(t)}{b}\right)^2 = 1\]
\[\cos^2(t) + \sin^2(t) = 1\]
Which is simply a trigonemtric identity. Ellipses are fairly easy to analyse.
Let us use an example to practice this:
\[\vec{r}(t) = \cos(2t)\hat{i} - 2\sin(2t)\hat{j}\]
Here, we can immediately determine \(a = 1\) and \(b = 2\) from the 
coefficients of \(\cos\) and \(\sin\). We can tell that the direction is 
reversed from the \(-\) in front of \(\sin\). Finally, we can tell that the
period is reduced from \(2\pi\) to \(\pi\) by the \(2t\) inner term.

\bigskip
A parametric equation is distinct from a path, because it contains more 
information. The equation can tell us where the particle is at any given time,
it tells us which direction the particle is moving in at any given time, and it
tells us how long the particle takes to traverse the path.

\bigskip
When we have a pair of parametric functions, it can be useful to determine
when the two particles collide. To find these points, we can take the \(x\)
and \(y\) coordinate of each function and set them equal to each other for an
arbritary value of \(t\). It is worth noting that the \textit{paths} of two
particles might intersect without the particles every colliding, as the two 
might pass through the same point at different times. As an example, let us
consider the following functions:
\[\vec{r}_1(t) = (t + 1)\hat{i} + (t^2 - 4t)\hat{j} \:\:\mathrm{and}\:\:
\vec{r}_2(t) = (2t)\hat{i} + (6t - 9)\hat{j}\] 
\[t + 1 = 2t \:\:\mathrm{and}\:\: t^2 - 4t = 6t - 9\]
\[t = 1 \:\:\mathrm{and}\:\: 1^2 - 4 \times 1 = 6 \times 1 - 9 = -3\]
Thus, the two particles collide at \(t = 1\).

\section*{Differential Calculus}
While it is difficult to model real world systems like, say, the population
of rabbits in Australia, it serendipitously happens to be less difficult to
write a function which is the \textit{derivative} of such a quantity. This is
the fundamental basis of differential equations, which are in turn fundamental
to many modern scientific fields.

\bigskip
At a very basic level, a differentiable function is one for which we can find
some function \(P^\prime\) which can be used to solve the following:
\[P(t_0 + \Delta t) \approx P(t_0) + P^\prime(t_0)\Delta t\]
i.e. we can find a value which tells us the rate of change of \(P\) across 
\(\Delta t\). The goal of differential calculus is to find this value 
\(P\prime\).

\bigskip
More precisely, we can define a derivative as a \textit{limit}. If \(f\) is a
function on an interval \(I\), the derivative of \(f\) at any value \(x \in I\)
is the gradient of the tangent line to the graph of \(f\) at the point 
\((x, f(x))\), given by this limit
\[f^\prime(x) = \lim_{h\rightarrow0}\frac{f(x + h) - f(x)}{h}\]
If and only if this limit exists is \(f\) \textit{differentiable} at \(x\). 
This in essence is stating that, for some very small value of \(h\):
\[f^\prime(x) \approx \frac{f(x + h) - f(x)}{h}\]
\[f^\prime(x)h \approx f(x + h) - f(x)\]
\[f^\prime(x)h + f(x) \approx f(x + h)\]
This is the ``fundamental formula of calculus''. It tells us that we can 
determine the value of \(f\) at a point \(x + h\) based on the derivative of
\(f\) and the value of \(f\) at \(x\). Essentially that a tangent line at this
point provides a good approximation of the function for a very small interval.

\bigskip
Let us take a close look at the limit formula. If we notice that \(f(x + h)\)
is simply a point \(h\) units along from \(f(x)\), then \(f(x + h) - f(x)\) is
the change in height of the function across the interval \(h\), i.e. the rise.
\(h\), obviously is the distance across which this occurs, the run. Thus, this
term must give us the average gradient over the interval \([x, x + h]\). When
\(h \approx 0\), it makes sense that this will be the instantaneous gradient at
the point \(x\). The tangent line to \(f\) at this point is the line with 
gradient given by this limit.

\bigskip
This lets us understand why a function may not be differentiable at a point.
To differentiate, we need to find the gradient of a tangent line. If a tangent
line cannot be drawn at a point, the function cannot be differentiated at that
point. No value \(f^\prime\) can be found. This can occur at vertical sections 
of a graph or at sections where a graph has a sharp point, for instance.

\subsubsection*{Standard Derivatives}
The function \(f^\prime\) is often written as 
\[\frac{d}{dx}[f(x)]\]
This is perhaps more common than \(f^\prime\) notation. For many common 
function structures, predetermined derivatives can be used rather than 
calculating on a case by case basis. Some of these common cases follow.
\begin{center}
    \begin{tabular}{||c||}
        \(\frac{d}{dx}[x^a] = ax^{a-1}\) \\[7pt]
        \(\frac{d}{dx}[\sin(x)] = \cos(x)\) \\[7pt]
        \(\frac{d}{dx}[\cos(x)] = -\sin(x)\) \\[7pt]
        \(\frac{d}{dx}[e^x] = e^x\) \\[7pt]
        \(\frac{d}{dx}[e^{f(x)}] = f^\prime(x)e^{f(x)}\) \\[7pt]
        \(\frac{d}{dx}[\log(x)] = \frac{1}{x}\) \\[7pt]
        \(\frac{d}{dx}[\log(f(x))] = \frac{f^\prime(x)}{f(x)}\) \\[7pt]
        \(\frac{d}{dx}[\tan(x)] = \sec^2(x)\) \\[7pt]
        \(\frac{d}{dx}[\arcsin(x)] = \frac{1}{\sqrt{1 - x^2}}\) \\[7pt]
        \(\frac{d}{dx}[\arccos(x)] = \frac{-1}{\sqrt{1-x^2}}\) \\[7pt]
        \(\frac{d}{dx}[\arctan(x)] = \frac{1}{1 + x^2}\) \\[7pt]
    \end{tabular}
\end{center}

\subsubsection*{Properties of Derivatives}
Differentiation is a linear operator. What this means is that we can easily
use derivatives for parts of a function to find a derivative for the whole
of a function:
\[\frac{d}{dx}[cf(x)] = c\frac{d}{dx}[f(x)]\]
\[\frac{d}{dx}[f(x) + g(x)] = \frac{d}{dx}[f(x)] + \frac{d}{dx}[g(x)]\]
For two functions multiplied together, \(f(x)g(x)\), we can find their 
derivative through the product rule.
\[\frac{d}{dx}[f(x)g(x)] = \frac{d}{dx}[f(x)]g(x) + f(x)\frac{d}{dx}[g(x)]\]
In the case of one function dividing another, say \(\frac{f(x)}{g(x)}\), we use
the quotient rule.
\[\frac{d}{dx}\left[\frac{f(x)}{g(x)}\right] = \frac{\frac{d}{dx}[f(x)]g(x) -
f(x)\frac{d}{dx}[g(x)]}{(g(x))^2}\]
If a function is operating on a second function in a super-function to be 
differentiated, the structure is often too complex to solve with the previous
rules. In these cases we use the chain rule.
\[\frac{d}{dx}[f(g(x))] = \frac{d}{dx}[f(x)](g(x))\frac{d}{dx}[g(x)]\]
These rules can be expressed simply by replacing the functions with single
letter variables.
\begin{center}
    \begin{tabular}{||c|c||}
        \(\mathrm{Rule}\) & \(\mathrm{Representation}\) \\
        \hline
        & \\[-5pt]
        \(\mathrm{Product}\) & \(\frac{d}{dx}[uv] = 
        vu^\prime + uv^\prime\) \\[7pt]
        \(\mathrm{Quotient}\) & \(\frac{d}{dx}\left[\frac{u}{v}\right] = 
        \frac{vu^\prime - uv^\prime}{v^2}\) \\[7pt]
        \(\mathrm{Chain}\) & \(\frac{dy}{dx} = 
        \frac{dy}{du}\frac{du}{dx}\) \\[7pt]
    \end{tabular}
\end{center}

\subsubsection*{Applications}
If we know that a function has a positive derivative at some value, we know
that the function is increasing at that point. The same is true for a negative
derivative, though the function is decreasing. For an interval, if the 
derivative of a function is \(> 0\) at every point in that interval, then the
function is increasing on that interval. The inverse does not apply; if \(f\) 
is increasing on an interval, that does not imply that \(f^\prime > 0\) on that
same interval.

\bigskip
This understanding of the implications of a positive or negative derivative
suggest that points where the derivative \(= 0\) have some significance. We 
call these points local maxima or local minima. Maxima occur where the function
has a greater value at a point than at any other nearby point, minima for the
inverse. These points are called \textit{local} if the range of ``nearby'' is
less than the domain of the function, and global if they are the highest or 
lowest points in the path of the function.

\bigskip
In addition to points where \(f^\prime = 0\), points where the gradient of a
function is undefined may also be local minima or maxima. It is also worth 
noting that a stationary point is not necessarily a local extrema and that a
local extreme is not necessarily a stationary point. To check if a local 
extrema is, we need to show that the points on either side have opposite sign.
These points include the start and end points of a function.

\bigskip
It is worth noting that a derivative can change sign only when a function is 
undefined or when it passes through \(0\), which means we can find the extrema
of a function by finding the sign of the function between each of the 
stationary points.

\subsubsection*{Higher Derivatives}
We are not limited to differentiating functions once; we can take the 
derivative of these derivatives; what we call the \textit{second} or 
\textit{third} derivative of a function. This is very relevant in areas such
as kinematics, where velocity is the first derivative of position and 
acceleration is the derivative of velocity, the second derivative of position.
The second derivative is denoted \(f^{\prime\prime}\) to the first derivative's
\(f^\prime\), and this trend continues to higher derivative with the alternate
notation of \(f^{(n)}\) for the \(n\)th derivative becoming useful for third 
and higher derivatives.

\bigskip
Just as with the first derivative, the second derivative has conditions to 
exist. For \(f^{\prime\prime}\) to exist, \(f^\prime\) must differentiable for
the interval considered. If this is true, \(f\) is twice differentiable on the
interval. We the second derivative as follows:
\[f^{\prime\prime}(x) = \frac{d}{dx}[f^\prime(x)] = \frac{d^2}{dx^2}[f(x)]\]
Higher derivatives follow the same pattern. Some information can be drawn from
a second derivative. The second derivative tells us the \textit{concavity} of a
function. A function is \textit{concave up} on an interval if it displays a 
kind of cup shape; this occurs when \(f^\prime\) is increasing on the interval
or when \(f^{\prime\prime}\) is positive for the interval. The function is 
concave down when it displays an inverted bowl shape; when \(f^\prime\) is
decreasing or \(f^{\prime\prime}\) is less than \(0\).

\bigskip
Concavity is a geometric property; it describes the direction of curvature of 
the graph. We can understand it by thinking about chords; if a graph is concave
up, such as a normal parabola, the chords will all be above the graph, while
for a concave down graph they will all be below the graph. Let us consider an
example:
\begin{itemize}
    \item Use the second derivative to determine where \(f(x) = x^4\) is 
    concave up and where it is concave down.
    \item \(f^\prime(x) = 4x^3\)
    \item \(f^{\prime\prime}(x) = 12x^2\)
    \item \(12x^2 > 0\) for \(x \in (-\infty, 0) \cup (0, \infty)\)
    \item Thus \(f(x)\) is concave up for \((-\infty, 0))\) 
    and \((0, \infty)\).
    \item It is worth noting that this implies that the first derivative is
    increasing on \(\R\), so \(f(x)\) is actually concave up for all \(\R\).
\end{itemize}

A \textit{point of inflection} is a point where a function changes between 
being concave up and concave down. These can be found where 
\(f^{\prime\prime} = 0\). In addition, they can occur at points where the
function is undefined, or at start or end points. For example, in the function
\(f(x) = x^3\), \(f^{\prime\prime}(x) = 6x\) and we can see that the only point
of inflection of this function is \(x = 0\).

\subsubsection*{Asymptotes}
Often when a graph has a gap in its domain, an \textit{asymptote} exists at 
this location. An asymptote is an area of the domain where the graph becomes
increasingly (arbritrarily) closer to a value without ever reaching it. 
Mathematically, this can be represented as
\[\lim_{x \rightarrow a}f(x) = \pm \infty\]
Where \(x = a\) is the equation of the asymptote in \(f\). A vertical asymptote
exists at \(y = a\) where
\[\lim_{x \rightarrow \pm\infty} f(x) = a\]
A third case exists, that of an \textit{oblique asymptote}, which is an 
asymptote defined by a linear function of the form \(y = mx + b\). In this
case, we can say a limit exists if
\[\lim_{x \rightarrow \pm\infty}[f(x) - (mx + b)] = 0\]
i.e. the value of the function converges with that of the line. It is worth
noting that an asymptote is not impassable; it simply states that the function
can be approximated by the asymptote for appropriate values of \(x\). These
asymptotes can be found through polynomial long division. As an example, let us
consider the following
\[f(x) = \frac{2x^3 - 3x^2 + 2x - 2}{x^2 + 1}\]
\[2x^3 - 3x^2 + 2x - 2 = (x^2 + 1)(2x - 3) + 1\]
\[f(x) = 2x - 3 + \frac{1}{x^2 + 1}\]
Thus, \(y = 2x - 3\) is an oblique asymptote to \(f\).

\bigskip
Vertical asymptotes are often encountered when dividing one function by 
another, as a function spits out smaller and smaller values as it approaches
\(0\), resulting in values tending towards \(\infty\), eventually reaching
the case of division by \(0\) and an undefined value. They can also occur
in functions like the logarithm. Some functions can have surprising behaviours.
\[f(x) = \frac{x^2 + 2x + 1}{x^2 - 1}\]
Looking at this function, we would assume that it was undefined for \(-1\) and
\(1\), as we would have a \(0\) denominator. However, if we factorise it we
find
\[f(x) = \frac{(x + 1)(x + 1)}{(x - 1)(x + 1)} = \frac{x + 1}{x - 1}\]
So the function is indeed defined for \(x = -1\). This teaches us to check
for common factors before crying asymptote.

\subsubsection*{Example}
Using the tools for analysis of functions discussed thus far, we can analyse
\[f(x) = \frac{x^2}{x + 1}\]
\begin{itemize}
    \item The implied domain of \(f\) is \(\R \backslash \set{-1}\).
    \item Performing long division, we can find that 
    \[x^2 = (x+1)(x-1)+1 \Rightarrow f(x) = x-1 + \frac{1}{x+1}\]
    This tells us that there is a vertical asymptote at \(x = -1\), approaching
    \(-\infty\) on the negative side and \(\infty\) on the positive side.
    \item In addition, we can see that the line \(y = x - 1\) is an oblique
    asymptote.
    \item The \(y\) intercept of the function is \(0\) and the \(x\) intercept
    of the function is also \(0\).
    \item Using the quotient rule we find that the derivative of the function
    is \[\frac{x(x + 2)}{(x + 1)^2}\]
    \item From this we can find the stationary points of the function at
    \(f^\prime(x) = 0\), \(x = 0\) and \(x = -2\).
    \item By looking at the sign of the derivative around these stationary 
    points we can see that the function is increasing on 
    \((-\infty, -2) \cup (0, \infty)\) and decreasing on 
    \((-2, -1) \cup (-1, 0)\)
    \item Through the same logic we can see that a local maximum exists at 
    \(x = -2\) and a local minimum exists at \(x = 0\).
    \item Again using the quotient rule we can find the second derivative
    \[f^{\prime\prime} = \frac{2}{(x + 1)^3}\]
    \item Looking at this function, we can tell there is only one stationary
    point at \(x = -1\), and noticing that it is negative for \(x < -1\) and
    positive for \(x > -1\) we can say \(f\) is concave down on 
    \((-\infty, 1)\) and concave up on \(-1, \infty\).
    \item Because \(-1\) is the only place where the sign of the second 
    derivative changes, and this is known to be an undefined point, we can tell
    the function has no points of inflection.
\end{itemize}

We can use this information to plot the function.

\begin{functionplot}[    
    xmin = -4,
    xmax = 2,
    ymin = -10,
    ymax = 10
]
    \addplot [dashed] {x-1};
    \addplot [dashed] coordinates {(-1, 10)(-1, -10)};
    \addplot [ultra thick, blue, domain = {-4:-1}, <->] {(x^2)/(x+1)};
    \addplot [ultra thick, blue, domain = {-1:2}, <->] {(x^2)/(x+1)};
\end{functionplot}

\subsubsection*{Implicit Differentiation}
In some cases, we might have a curve defined by some rule, but this rule might
not express y in terms of \(x\), it may instead be a function of both \(x\) and
\(y\) or some other arcane mess of symbols. Nonetheless, it is evident that we
should be able to find a rule which yields the slope of the tangent line to 
this curve at some point. In this case, we say there is an \textit{implicit}
dependence of \(y\) on \(x\), and we find the derivative through 
\textit{implicit differentiation}.

\bigskip
To do this we assume that near any given \(x\), it is possible to write \(y\) 
as a function of \(x\). For example:
\[x^2y = 1\]
\[\frac{d}{dx}[x^2y] = \frac{d}{dx}[1] = 0\]
\[\frac{d}{dx}[x^2]y + x^2\frac{d}{dx}[y] = 0\]
\[2xy + x^2\frac{d}{dx}[y] = 0\]
\[\frac{dy}{dx} = \frac{-2xy}{x^2} = \frac{-2y}{x} \:\:(x \neq 0)\]
This tells us that a tangent line to a point \(x, y\) on this curve has a slope
given by this formula.

\bigskip
In general, implicit differentiation is performed as follows.
\begin{itemize}
    \item For an equation involving \(x\) and \(y\).
    \item Take the derivative of each side, with respect to \(x\).
    \item Simplify each side as much as possible. Remember that \(y\) is 
    assumed to be a function of \(x\), so the product, chain and quotient rules
    must be appropriately applied.
    \item Rearrange to solve for \(\frac{dy}{dx}\). Usually this solution will
    involve both \(x\) and \(y\).
\end{itemize}

A more complex example:
\[x^4 + y^2 = 1 + x^2y\]
\[\frac{d}{dx}[x^4 + y^2] = \frac{d}{dx}[1 + x^2y]\]
\[4x^3 + 2y\frac{dy}{dx} = \frac{d}{dx}[x^2y]\]
\[4x^3 + 2y\frac{dy}{dx} = \frac{d}{dx}[x^2]y + x^2\frac{dy}{dx}\]
\[4x^3 + 2y\frac{dy}{dx} = 2xy + x^2\frac{dy}{dx}\]
\[(2y - x^2)\frac{dy}{dx} = 2xy - 4x^3\]
At this stage we run into a small problem; we want to divide through by 
\(2y - x^2\), but if this is ever \(0\), our solution will be undefined.
We can solve to see if this is ever the case:
\[2y - x^2 = 0 \Rightarrow y = \frac{1}{2}x^2\]
Subbing this into our original equation:
\[x^4 + \left(\frac{1}{2}x^2\right)^2 = 1 + x^2\times\frac{1}{2}x^2\]
\[x^4 + \frac{1}{4}x^4 = 1 + \frac{1}{2}x^4\]
\[\frac{3}{4}x^4 = 1 \Rightarrow x^4 = \frac{4}{3} \Rightarrow x^2 = 
\pm \frac{2}{\sqrt{3}}\]
\[x = \pm \sqrt{\frac{2}{\sqrt{3}}}\]
So at these points, our derivative will be undefined, which tells us the curve
is vertical at these points. With this caveat, we can continue solving.
\[\frac{dy}{dx} = \frac{2xy - 4x^3}{2y - x^2}\]
It is important to note that at points where both the numerator and denominator
are \(0\), it is impossible to know what happens at this location without 
further analysis; the derivative could be horizontal, vertical or otherwise.

\subsubsection*{Derivatives of Inverse Functions}
To find a derivative of \(y = \log(x)\), we could rewrite as
\[e^y = x\]
\[\frac{d}{dx}[e^y] = \frac{d}{dx}[x]\]
\[\frac{d}{dx}[y]e^y = 1\]
\[\frac{dy}{dx} = \frac{1}{e^y} = \frac{1}{x}\]
Here, we have used implicit differentiation to find a derivative of \(\log(x)\)
based on the knowledge of the derivative of the exponential function. We can
use the technique to find the derivatives of the inverse trigonemtric 
functions. For example, to find the derivative of \(y = \arcsin(x)\)
\[\sin(y) = x \:\:\: x \in [-1, 1]\]
\[\frac{d}{dx}[sin(y)] = \frac{d}{dx}[x]\]
\[\frac{dy}{dx}\cos(y) = 1\]
\[\frac{dy}{dx} = \frac{1}{\cos(y)}\]
We can rewrite \(\cos(y)\) in terms of \(x\):
\[\sin^2(y) + \cos^2(y) = 1\]
\[\cos^2(y) = 1 - \sin^2(y)\]
\[\cos(y) = \sqrt{1 - \sin^2(y)}\]
\[\frac{d}{dx}[\arcsin(x)] = \frac{1}{\sqrt{1 - x^2}}\]
Using implicit differentiation we once again find that we can find a derivative
for this function using knowledge of other derivatives.

\subsubsection*{Derivatives of Parametric Curves}
Where for a function \(\R \rightarrow \R\) our derivative supplies a linear 
approximation of the curve near some \(x\) value, for a parametric curve, we
need to find a \textit{velocity vector} which approximates the movement of a
parametric function fora small time period. As opposed to
\[f(x_0 + h) \approx f(x_0) + f^\prime(x_0)\times h\]
we have the vector equation
\[\vec{r}(t_0 + \Delta t) \approx \vec{r}(t_0) + \vec{v}\Delta t\]
Formally the definition of this vector \(\vec{v}\) is
\[\vec{r}^{\:\prime}(t) = \lim_{h\rightarrow0}\frac{1}{h}(\vec{r}(t + h) - 
\vec{r}(t))\]
If and only if this limit exists is \(\vec{r}\) differentiable at time \(t\).
We can understand this by examining it's components:
\[\frac{1}{h}(\vec{r}(t + h) - \vec{r}(t))\]
Here we have simply the vector displacement between the times \(t\) and 
\(t + h\), divided by \(\Delta t = h\). Thus, this is simply the instantaneous
velocity at time \(t\). This makes clear the relationship between velocity
and \(\vec{r}^{\:\prime}\). To find this derivative, if the parametric function
is of the form \(\vec{r} = x(t)\hat{i} + y(t)\hat{j}\) then the derivative is
simply
\[\vec{r}^{\:\prime} = x^\prime(t)\hat{i} + y^\prime(t)\hat{j}\]
For example:
\[\vec{r}(t) = (t^2 + 1)\hat{i} + (\frac{1}{3}t^3 + 1)\hat{j}\]
\[\vec{r}^{\:\prime}(t) = 2t\hat{i} - (3t^3 + 1)\hat{j}\]
Or:
\[\vec{r}(t) = 2\cos(t)\hat{i} + \sin(t)\hat{j}\]
\[\vec{r}^{\:\prime}(t) = -2\sin(t)\hat{i} + \cos(t)\hat{j}\]
Calculating the velocity vectors for a few values we find that:
\begin{itemize}
    \item \(\vec{r}^{\:\prime}(0) = \hat{j}\)
    \item \(\vec{r}^{\:\prime}(\frac{\pi}{2}) = -2\hat{i}\)
    \item \(\vec{r}^{\:\prime}(\pi) = -\hat{j}\)
    \item \(\vec{r}^{\:\prime}(\frac{3\pi}{2}) = 2\hat{i}\)
\end{itemize}
And when plotting this graph, we find these vectors (in blue) look exactly how
we would expect for the velocity of an object undergoing circular motion. The
length of the resultant vectors is the instantaneous velocity of the particle
at the time.

\begin{functionplot}[    
    xmin = -4,
    xmax = 4,
    ymin = -3,
    ymax = 3
]
    \addplot[ultra thick, dashed] {(-1*(x/2)^2 + 1)^0.5};
    \addplot[ultra thick, dashed] {-1*(-1*(x/2)^2 + 1)^0.5};

    \addplot[ultra thick, blue, ->] coordinates {(2, 0) (2, 1)};
    \addplot[ultra thick, blue, ->] coordinates {(0, 1) (-2, 1)};
    \addplot[ultra thick, blue, ->] coordinates {(-2, 0) (-2, -1)};
    \addplot[ultra thick, blue, ->] coordinates {(0, -1) (2, -1)};

    \addplot[ultra thick, red, ->] coordinates {(2, 0) (0.05, 0)};
    \addplot[ultra thick, red, ->] coordinates {(0, 1) (0, 0.05)};
    \addplot[ultra thick, red, ->] coordinates {(-2, 0) (-0.05, 0)};
    \addplot[ultra thick, red, ->] coordinates {(0, -1) (0, -0.05)};
\end{functionplot}

We can differentiate again to find
\[\vec{r}^{\:\prime\prime}(t) = -2\cos(t)\hat{i} - \sin(t)\hat{j}\]
Solving for the same values as earlier:
\begin{itemize}
    \item \(\vec{r}^{\:\prime\prime}(0) = -2\hat{i}\)
    \item \(\vec{r}^{\:\prime\prime}(\frac{\pi}{2}) = -\hat{j}\)
    \item \(\vec{r}^{\:\prime\prime}(\pi) = 2\hat{i}\)
    \item \(\vec{r}^{\:\prime\prime}(\frac{3\pi}{2}) = \hat{j}\)
\end{itemize}
These vectors are marked in red on the graph; the acceleration is of
course toward the center.

\bigskip
We can study the motions of objects through their derivatives. In the case
that the derivative of a parametric function is \(0\), the velocity of the
particle is \(0\). These points usually correspond to a sharp point on a
curve, known as a \textit{cusp}. To check, one can check the signs of the two
components either side of the point; at least one must change sign for a cusp
to exist at the point. There is no tangent line at the curve at this point, 
however it is still differentiable. If a curve has no cusps, it is smooth.

\bigskip
The angle between the acceleration and velocity vectors contains some 
information. When the angle is acute, the speed is increasing. When it is 
obtuse, the speed is decreasing. If the two are perpendicular, the speed is
at a turning point. This is sensible; when the angle is acute, the acceleration
is adding in the direction of the velocity, while if it is obtuse, it is
working against the velocity. This can be stated through the dot product of
the two vectors:
\begin{itemize}
    \item If \(\vec{r}^\prime(t) \cdot \vec{r}^{\prime\prime}(t) < 0\), the 
    speed is decreasing.
    \item If \(\vec{r}^\prime(t) \cdot \vec{r}^{\prime\prime}(t) = 0\), \(t\)
    is at a stationary point.
    \item If \(\vec{r}^\prime(t) \cdot \vec{r}^{\prime\prime}(t) > 0\), the
    speed is increasing.
\end{itemize}

\bigskip
The reason we have sharp points in differentiable parametric functions is 
because the plot of the curve is not a plot of the graph; the plot of the
graph would be in \(\R^3\), as the input is one dimensional and the output
is two dimensional, much as the input to a single dimensional function needs
a two dimensional graph in a plane.

\bigskip
An application of vector valued functions is projectile motion. For a 
projectile with initial velocity \(a\hat{i}, b\hat{j}\) and launch height 
\(y_0\) the particles position at a given time \(t\) is given by
\[\vec{r}(t) = at\hat{i} + (y_0 + bt - \frac{1}{2}gt^2)\hat{j}\]

\section*{Integral Calculus}
The definite integral of a function \(f\) in the interval \([a, b]\) is
denoted
\[\int_a^b f(x) dx\]
It is defined to be the sum of the \textit{signed areas} bounded by the curve
\(y = f(x)\), the \(x\) axis and the lines \(x = a\) and \(x = b\). Signed area
here refers to the fact that areas which are under the graph and above the 
\(x\) axis are added to the area while those above the graph and below the axis
are subtracted. \(a\) and \(b\) are the \textit{terminals} of the integral, and
together for the interval of integration.

\bigskip
Definite integrals share some of the properties of derivatives
\[\int_a^b f(x) + g(x) dx = \int_a^b f(x) dx + \int_a^b g(x) dx\]
\[\int_a^b kf(x) + g(x) dx = k\int_a^b f(x) dx\]
i.e. linearity. Integrals also have the property that for an interval 
\([a, a]\) the result will be \(0\). An integral of \(f\) on \([a, b]\)
and an integral on \([b, c]\) will add to the same value as an integral
on \([a, c]\). 

\bigskip
Conceptually, integrals can be understood as being an approximation of the area
under a graph through a method using vertical rectangles. These rectangles have
width \(h\) and height given by \(f(x)\) at their boundary (upper or lower 
depending on implementation). By taking a limit as \(h\) approaches \(0\), we
can find the exact area under the curve. However, rather than doing this 
manually each time we can use antidifferentiation. For a function \(f\), we
call \(F\) an antiderivative of \(f\) if
\[F^\prime(x) = f(x)\]
A property of this gives rise to the \textit{Fundamental Theorem of Calculus}:
\[\int_a^b f(x) dx = F(b) - F(a) = [F(x)]_a^b\]
This tells us that the area under \(f(x)\) is given by the value of an 
antiderivative of \(f\) at \(b\) minus the value of that antiderivative at 
\(a\). We can use this to easily find the area under a curve.
\[\int_0^1 x^2 dx\]
\[\frac{d}{dx}[\frac{1}{3}x^3] = x^2\]
\[\int_0^1 x^2 dx = F(1) - F(0) = \frac{1}{3}1^3 - \frac{1}{3}0^3=\frac{1}{3}\]
It is important to note that antiderivatives are not unique. For any \(F\) that
is an antiderivative of \(f\), there is an arbritary number of possible 
variations. We thus introduce the concept of the indefinite integral.
\[\int f(x) = F(x) + C\]
Because the constant term \(C\) dissapears when differentiating, all possible
values of \(C\) are valid.

\bigskip
If we have two curves and we want to find the area enclosed horizontally 
between them, all we need to do is to rearrange to find \(x\) as the dependent
variable, and we can then integrate \(dy\) to find the area.

\subsubsection*{Standard Antiderivatives}
\begin{center}
    \begin{tabular}{||c||}
        \(\int x^a \:dx = \frac{x^{a + 1}}{a + 1} + C \: (a \neq -1)\) \\[7pt]
        \(\int \frac{1}{x} \:dx = \log(\;\abs{x}\;) + C\) \\[7pt]
        \(\int e^x \:dx = e^x + C\) \\[7pt]
        \(\int e^{ax} \:dx = \frac{1}{a}e^{ax} + C\) \\[7pt]
        \(\int \sin(x) \:dx = -\cos(x) + C\) \\[7pt]
        \(\int \cos(x) \:dx = \sin(x) + C\) \\[7pt]
        \(\int \sec^2(x) \:dx = \tan(x) + C\) \\[7pt]
        \(\int \frac{1}{\sqrt{s^2 - x^2}} \:dx = 
            \arcsin(\frac{x}{s}) + C\) \\[7pt]
        \(\int \frac{-1}{\sqrt{s^2 - x^2}} \:dx = 
            \arccos(\frac{x}{s}) + C\) \\[7pt]
        \(\int \frac{1}{s^2 + x^2} \:dx = 
            \frac{1}{s}\arctan(\frac{x}{s}) + C\) \\[7pt]
    \end{tabular}
\end{center}
For the inverse trigonometric functions, \(s > 0\). To find the area between
two curves, we can simply substitute \(f(x) - g(x)\) into our integral.

\subsubsection*{Integration by Substitution}
Integration has no general rules like the product or quotient rules.
We can however use the process of integration by substitution in some 
situations. If we have a function \(f(x)\) that we can't differentiate, but we
can replace some sections of this function with a second function \(h(x)\), 
then we can use integration by substitution. For example, we might replace 
\(x^4\) with  \(u\). The \textit{substitution rule} which makes this work is
\[\int g(u)\frac{du}{dx} \:dx = \int g(u) du\]
So we then find an antiderivative in terms of \(u\) and then substitute back.
For example
\[\int 2x(x^2 - 5)^4 \:dx\]
\[u = x^2 - 5 \Rightarrow g(u) = u^4\]
\[\frac{du}{dx} = 2x\]
\[\int 2x(x^2 - 5)^4 \:dx = \int u^4\frac{du}{dx} \:dx = \int u^4 du  
= \frac{1}{5}u^5\]
\[\frac{1}{5}(x^2 - 5)^5 + C\]
Note that here \(dx\) cancels with the denominator of \(\frac{du}{dx}\). 
Another example:
\[\int cos(3x) \sqrt{\sin(3x + 4)} \:dx\]
\[u = \sin(3x) + 4 \Rightarrow g(u) = \sqrt{u}\]
\[\frac{du}{dx} = 3\cos(3x) \Rightarrow \frac{1}{3}\frac{du}{dx} = \cos(3x)\]
\[\int cos(3x) \sqrt{u} \:dx = \int \sqrt{u}\frac{1}{3}\frac{du}{dx} \:dx
= \frac{1}{3}\int \sqrt{u}\frac{du}{dx} \:dx = \frac{1}{3}\int u^\frac{1}{2} 
\:du\]
\[= \frac{1}{3}\frac{2}{3}u^\frac{3}{2} = \frac{2}{9}u^\frac{3}{2}\]
\[\Rightarrow \int cos(3x) \sqrt{\sin(3x + 4)} \:dx = 
\frac{2}{9}(\sin(3x) + 4)^\frac{3}{2}\]
Integration by substitution is essentially a backwards application of the chain
rule. To demonstrate, the chain rule gives us
\[\frac{d}{dx}\left[\sin^5(x)\right] = 5\sin^4(x)\cos(x)\]
Using the Fundamental Theorem
\[\int_a^b 5\sin^4(x)\cos(x) \:dx = \int_a^b \frac{d}{dx}\left[\sin^5(x)\right]
\:dx = \sin^5(b) - \sin^5(a)\]
The key here is noticing that the integrand \(5\sin^4(x)\cos(x)\) can be 
written as a product of a function on a function multiplied by a derivative,
the form of output of the chain rule.
\[\frac{d}{dx}\left[G(h(x))\right] = g(h(x))h^\prime(x)\]
Where \(g(x) = 5x^4\) and \(h(x) = \sin(x)\). We can integrate any function in
this form, as long as we know an antiderivative of \(g\) (that is \(G\) above).
\[\int_a^b g(h(x))h^\prime(x) \:dx = \int_a^b G^\prime(h(x))h^\prime(x) \: dx
= \int_a^b \frac{d}{dx}\left[G(h(x))\right] \: dx = \]
\[\left[G(h(x))\right]_a^b = G(h(b)) - G(h(a))\]
Here, an inverse chain rule is applied to transform the second integral into
the third, by recognising that \(G^\prime(h(x))h^\prime(x)\) is the output of a
chain rule on \(G(h(x))\). One interesting factor of this form is that
\[\left[G(h(x))\right]_a^b = \left[G(x)\right]_{h(a)}^{h(b)}\]
This knowledge lets us skip a few steps. Backtracking up the process we find
\[\left[G(x)\right]_{h(a)}^{h(b)} = \int_{h(a)}^{h(b)}G^\prime(x) \:dx
= \int_{h(a)}^{h(b)}g(x) \:dx\]
This gives us the final rule for integration by substitution for definite 
integrals.
\[\int_a^b g(h(x))h^\prime(x) \:dx = \int_{h(a)}^{h(b)} g(x) \:dx\]
As an example:
\[\int_1^{e^\frac{\pi}{2}} \frac{\sin(\log(x))}{x} \:dx\]
\[u = \log(x) \Rightarrow \frac{du}{dx} = \frac{1}{x}\]
\[\int_1^{e^\frac{\pi}{2}} \sin(u)\frac{du}{dx} \:dx = 
\int_0^\frac{\pi}{2} \sin(u) \:du\]
Note that at this stage the bounds of integration have changed; we have moved
from \([a, b]\) to \([\log(a), \log(b)]\), because in this case \(\log\) was 
our \(h\).
\[\int_0^\frac{\pi}{2} \sin(u) \:du = \left[-\cos(u)\right]_0^\frac{\pi}{2}\] 

\subsubsection*{Linear Substitution}
Not all functions are obviously transformed to use integration by substitution.
However, sometimes we can use the trick of \textit{linear substitution} to find
a way to do so. The key to using integration by substitution is to rearrange
the function into the form
\[\int g(u) \frac{du}{dx} \: dx\]
So one way we can do this is by finding a linear function \(u(x)\) and 
rearranging to find \(x\) in terms of \(u\). Example:
\[\int (2x + 1)\sqrt{x - 3} \:dx\]
\[u = x - 3 \Rightarrow x = u + 3\]
\[\int (2(u + 3) + 1)\sqrt{u} \:dx = \int (2u + 7)\sqrt{u}\frac{du}{dx} \:dx
\left(\frac{du}{dx} = 1\right)\]
\[= \int 2u^{\frac{3}{2}} + 7u^{\frac{1}{2}} \:du = 
2\frac{1}{\frac{3}{2} + 1}u^{\frac{3}{2} + 1} + 
7\frac{1}{\frac{1}{2} + 1}u^{\frac{1}{2} + 1} + C\]
\[= 2\frac{2}{5}u^\frac{5}{2} + 7\frac{2}{3}u^\frac{3}{2} + C\]
\[= \frac{4}{5}(x - 3)^\frac{5}{2} + \frac{14}{3}(x - 3)^\frac{3}{2} + C\]
Here, we used the fact that we could easily find \(x\) in terms of \(u\) to
simplify the integral. This trick can also be used for high powers of \(\sin\)
and \(\cos\).
\[\int \sin^4\cos^3 \:dx = \int u^4\cos^2(x)\cos(x) \:dx = 
\int u^4\cos^2(x)\frac{du}{dx} \:dx\]
\[\cos^2(x) + sin^2(x) = 1 \Rightarrow cos^2(x) = 1 - \sin^2(x) = 1 - u^2\]
\[\int u^4(1 - u^2) \:du = \int u^4 - u^6 = 
\frac{1}{5}u^5 - \frac{1}{7}u^7 + C\]
\[\int \sin^4\cos^3 \:dx = \frac{1}{5}\sin^5(x) - \frac{1}{7}\sin^7(x) + C\]
This trick will work as long as the power of either \(\sin\) or \(\cos\) is odd
for any similar function. If both are odd, either can be used. If both are 
even, the complex exponential can be used.
\bigskip
Linear substitution happens to make one particular common form of substitution 
very simple. For a function of the form \(f(kx)\) where \(\int f(x)\) is 
already known, linear substitution tells us that \(\int f(kx)\) is simply
\[\int f(kx) \:dx = \frac{1}{k}F(kx) + C\]
A couple of examples of this
\[\int \sin(kx) \:dx = -\frac{1}{k}\cos(kx) + C\]
\[\int e^{kx} \: dx = \frac{1}{k}e^{kx} + C\]

\subsubsection*{Integration of Rational Functions}
Functions of the form
\[\frac{p(x)}{q(x)}\]
Turn out to be a very important structure to integrate, and a general process 
for this exists. We already know that
\[\int \frac{1}{x} \: dx = \log(\;\abs{x}\;) + C\]
And now we extend this to examine integrals of the form
\[\int \frac{1}{ax + b} \:dx\]
Here, we use the substitution \(u = ax + b\).
\[u = ax + b \Rightarrow \frac{du}{dx} = a\]
So for an integral
\[\int \frac{1}{2x - 3} \:dx\]
\[u = 2x - 3 \Rightarrow \frac{du}{dx} = 2\]
We can multiply \(\frac{du}{dx}\) by \(\frac{1}{2}\) to simplify this process
somewhat.
\[\int \frac{1}{u} \frac{1}{2}\frac{du}{dx} \:dx 
= \frac{1}{2} \int\frac{1}{u}\:du = \frac{1}{2}\log(\;\abs{u}\;) + C\]
\[\int \frac{1}{2x - 3} \:dx = \frac{1}{2}\log(\;\abs{2x - 3}\;) + C\]

\end{flushleft}
\end{document}
