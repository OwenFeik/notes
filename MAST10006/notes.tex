\documentclass[12pt]{report}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{commath}
\usepackage{environ}
\usepackage{pgfplots}
\usepackage{placeins}

\usetikzlibrary{external}
\tikzexternalize

\NewEnviron{plot}[1][
    xmin = -5,
    xmax = 5,
    ymin = -5,
    ymax = 5
]{
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                samples = 1000,
                #1,
                width = 12cm,
                height = 9cm,
                xlabel = \(x\),
                ylabel = \(y\),
                axis lines = middle,
                restrict y to domain = -10:10
            ]
            \BODY\                
            \end{axis}
        \end{tikzpicture}
    \end {center}
}

\NewEnviron{smallplot}[1][
    xmin = -5,
    xmax = 5,
    ymin = -5,
    ymax = 5
]{
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                samples = 1000,
                #1,
                width = 8cm,
                height = 6cm,
                xlabel = \(x\),
                ylabel = \(y\),
                axis lines = middle,
                restrict y to domain = -10:10
            ]
            \BODY\
            \end{axis}
        \end{tikzpicture}
    \end {center}
}

\newenvironment{formulalist}{
    \renewcommand{\arraystretch}{2}
    \begin{center}    
        \begin{tabular}{||c||}
}{
        \end{tabular}
    \end{center}
    \renewcommand{\arraystretch}{1}
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\dx}{\:\mathrm{d}x}
\newcommand{\dy}{\:\mathrm{d}y}
\newcommand{\mand}{\:\mathrm{and}\:}
\newcommand{\limit}{\lim\limits}
\newcommand{\sumninf}[1][1]{\sum\limits_{n = #1}^\infty}
\newcommand{\derivx}[1]{\frac{d}{dx}\left[#1\right]}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\cosech}{cosech}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\arctanh}{arctanh}

\pgfplotsset{compat = 1.16}

\begin{document}
\begin{flushleft}

\section*{Notation}
In this subject, a variety of notation is used.
\begin{itemize}
    \item \(\mid\) read ``such that'' used largely in set definitions: 
        \(\set{x \in \R \mid x \geq 2}\)
    \item \(\forall\) read ``for all''.
    \item \(\exists\) read ``there exists''.
    \item \(\equiv\) read ``is equivalent to' used to signify function
        equality, etc.
    \item \(\ll\) read ``much less than''.
    \item \(\log\) denotes the natural logarithm, \(\ln\).
    \item Inverse trigonometric functions are written \(\arcsin\) rather than
        \(\sin^{-1}\).
\end{itemize}

Some common sets have specific symbols associated with them.
\begin{itemize}
    \item Natural numbers, \(\N = \set{1, 2, 3, \ldots}\). Exclusive of \(0\)
        in this course.
    \item Integers, \(\Z = \set{\ldots, -2, -1, 0, 1, 2, \ldots}\)
    \item Rational numbers, 
        \(\Q = \set{\frac{m}{n} \mid m, n \in \Z, n \neq 0}\)
    \item Real numbers, \(\R\)
    \item Complex numbers, \(\C = \set{x + iy \mid x, y \in \R, i^2 = -1}\)
    \item \(xy\) plane, \(\R^2 = \set{(x, y) \mid x, y \in \R}\)
    \item Three dimensional space, 
        \(\R^3 = \set{(x, y, z) \mid x, y, z \in \R}\)
\end{itemize}

\section*{Limits, Continuity, Sequences, Series}
\subsection*{Limits}
Limits form the fundamental concept behind the definition of a derivative;
the instantaneous rate of change of a function at a point is defined in terms
of a limit. \par
A limit is defined as follows. For a function \(f(x)\), the limit of \(f(x)\) 
as \(x\) approaches \(a\) is \(L\), written as
\[\lim_{x\rightarrow a}f(x) = L\]
If \(f(x)\) gets continually closer to \(L\) but \(x \neq a\). If a limit 
exists, it must be a unique finite real number.

\subsubsection*{Examples}
If \(f(x)\) is defined as follows 
\[
    f(x) =
    \begin{cases}
        2x, x \neq 1 \\
        4, x = 1 \\
    \end{cases}
\]
Evaluate the following
\[\lim_{x\rightarrow 1} f(x)\]
As \(f(x)\) gets arbritrarily close to \(2\) whenever \(x\) is close to but not
equal to \(1\), therefore the limit as \(x\rightarrow1\) is \(2\).

\bigskip\bigskip
Evaluate the following limit.
\[\lim_{x\rightarrow0}f(x) = \frac{1}{x^2}\]
As \(f(x)\) is unbounded as \(x\rightarrow0\). Therefore, \(f(x)\) cannot be 
made arbritrarily close to any one number and the limit does not exist.

\bigskip\bigskip
Evaluate the following limit.
\[
    \lim_{x\rightarrow0}
    \begin{cases}
        1, x < 0 \\
        2, x \geq 0 \\
    \end{cases}
\]
As \(x\) approaches \(0\) from the right, it grows arbritrarily close to \(2\),
while as it approaches from the left it grows arbritrarily close to \(1\). As 
it doesn't grow arbritrarily close to a single value, no limit exists here.

\subsubsection*{Additional Limt Notation}
In the previous examples, we noticed that a common way to examine limits is by
considering the values the function approaches from each side. Because this is
such a common construct, notation exists for the left and right limits
independently. The right and left limits of the function \(f\) approaching 
\(0\), examined in the final example above are written as
\[\lim_{x\rightarrow0^+}f(x) = 2\]
\[\lim_{x\rightarrow0^-}f(x) = 1\]
In general, for a limit to exist the following statement must be true
\[\lim_{x\rightarrow a} = L \Leftrightarrow \lim_{x\rightarrow a^-} = L
\mand \lim_{x\rightarrow a^+} = L\]
i.e. for the limit to be \(L\), both the left and right limits must be \(L\).

\subsubsection*{Limit Laws}
For two real valued functions \(f\) and \(g\), and \(c \in \R\) a constant. If
the limits \(\limit_{x\rightarrow a} f(x)\) and 
\(\limit_{x\rightarrow a} g(x)\) exist, then the following limit laws 
apply.

\begin{formulalist}
    \(\limit_{x\rightarrow a}\left[f(x) + g(x)\right]
    = \limit_{x\rightarrow a} f(x) 
    + \limit_{x\rightarrow a} g(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[cf(x)\right] 
    = c\limit_{x \rightarrow a}f(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[f(x)g(x)\right] 
    = \limit_{x \rightarrow a}f(x) \cdot
    \limit_{x \rightarrow a}g(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[\frac{f(x)}{g(x)}\right] 
    = \frac{\limit_{x \rightarrow a}f(x)}{
    \limit_{x \rightarrow a}g(x)} 
    \:\left(\limit_{x \rightarrow a}g(x) \neq 0\right)\) \\ 
    \(\limit_{x \rightarrow a} c = c\) \\ 
    \(\limit_{x \rightarrow a} x = a\) \\ 
\end{formulalist}

\subsubsection*{Example}
Using the limit laws, evaluate the limit
\[\lim_{x\rightarrow2}\frac{x^3 + 2x^2 - 1}{5 - 3x}\]
\[\lim_{x\rightarrow2}\frac{x^3 + 2x^2 - 1}{5 - 3x} 
= \frac{\limit_{x\rightarrow2} x^3 + 2x^2 - 1}{
\limit_{x\rightarrow2} 5 - 3x} = \frac{8 + 8 - 1}{5 - 6} 
= \frac{15}{-1} = -15\]
One could use more of the limit laws to break this down further; for instance
breaking the added terms into individual limits, or breaking the \(x^n\) terms
down using the product law.

\subsubsection*{Limits and Infinity}
We can talk about a limit as \(x\rightarrow\infty\), referring to what happens
to the limit as \(x\) is made arbritrarily large. So the limit
\[\lim_{x\rightarrow\infty} f(x) = L\]
Is stating that as \(x\) is made arbritrarily larger, \(f(x)\) becomes 
arbritrarily closer to \(L\). \(L\) must be finite for \(f(x)\) to take it's
value.

\begin{plot}
    \addplot[blue, thick] {2.71828^(-1*x)}
    node[above, pos = 0.9] {\(e^{-x}\)};
\end{plot}

For example, in the above plot we can see that as \(x\) becomes arbritrarily 
close to infinity, \(e^{-x}\) becomes arbritrarily close to \(0\). Therefore,
\[\lim_{x\rightarrow\infty} e^{-x} = 0\]
While this process of examining a graph and noting its convergence is practical
for certain examples, it is a little laborious in the long term, so we use the 
constructs of standard limits to solve many problems.

\subsubsection*{Standard Limits}
The following limits can be used without proof in this subject, with their 
truth taken as gospel.

\begin{formulalist}
    \(\limit_{x\rightarrow\infty}\frac{1}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty}r^x = 0 \:\:\:(0 \leq r < 1)\) \\
    \(\limit_{x\rightarrow\infty}\frac{1}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty} r^x = 0 \:\:\:(0 \leq r < 1)\) \\
    \(\limit_{x\rightarrow\infty} a^{\frac{1}{x}} = 1 \:\:\:(a > 0)\) \\
    \(\limit_{x\rightarrow\infty} x^{\frac{1}{x}} = 1\) \\
    \(\limit_{x\rightarrow\infty} \frac{\log(x)}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty} \left(1 + \frac{a}{x}\right)^x 
    = e^a \:\:\:(a \in \R)\) \\
    \(\limit_{x\rightarrow\infty} \frac{x^p}{a^x} 
    = 0 \:\:\:(p \in \R, a > 1)\) \\  
\end{formulalist}

For example, in the case of \(e^{-x}\), we can use the second limit from above
by taking \(r = \frac{1}{e}\).

\subsubsection*{Terminology}
If a limit exists, we can state that \(f(x)\) \textit{converges} as \(x\) 
approaches \(a\). Inversely, we can state \(f(x)\) \textit{diverges} as \(x\)
approaches \(a\). \par
For example, as \(\sin(x)\) oscillates between \(-1\) and \(1\), it cannot 
approach a single number and therefore diverges as \(x\rightarrow\infty\). \par
It is important to note that \(\infty\) isn't a number; in general it denotes
``any arbritrarily large number''. A limit cannot be equal to infinity. Because
infinity is not a number, certain cases become indeterminate.
\[\limit_{x\rightarrow\infty} \frac{3x^2 - 2x + 3}{x^2 + 4x + 4}\]
Here, we can see that both the numerator and the denominator approach infinity
as \(x\rightarrow\infty\). We cannot therefore divide their individual limits 
to find the overall limit and must instead alter the form to find the limit. 
\par
In this case, we can do this by dividing numerator and denominator by 
\(\frac{1}{x^2}\).
\[\frac{\frac{1}{x^2}}{\frac{1}{x^2}} \cdot \frac{3x^2 - 2x + 3}{x^2 + 4x + 4}
= \frac{3 - \frac{2}{x} + \frac{3}{x^2}}{1 - \frac{4}{x} + \frac{4}{x^2}}\]
\[\frac{\limit_{x\rightarrow\infty} 3 - \frac{2}{x} + \frac{3}{x^2}}{
\limit_{x\rightarrow\infty} 1 - \frac{4}{x} + \frac{4}{x^2}} = 
\frac{3}{1} = 3\]
By modifying the fraction and then applying limit laws, we can solve this 
initially indeterminate limit.

\subsubsection*{The Sandwhich Theorem}
The Sandwhich Theorem states that if \(g(x) \leq f(x) \leq h(x)\) when 
\(x \approx a\) but \(x \neq a\) and
\[\limit_{x\rightarrow a}g(x) = \limit_{x\rightarrow a}h(x) = L\]
Then \(\limit_{x\rightarrow a}f(x) = L\). In essence, it states that if a 
function lies between two other functions who each converge to \(L\) at \(a\),
then that function must also converge to \(a\). This theorem can also be used
to solve functions of the indeterminate form \(\infty - \infty\). For example,
\(\limit_{x\rightarrow\infty} f(x) = \sqrt{x^2 + 1} - x\) is of this form. We
can simplify to some degree, but we need the Sandwhich Theorem to finish the 
problem.
\[\limit_{x\rightarrow\infty} \left(\sqrt{x^2 + 1} - x\right) \cdot 
\frac{\sqrt{x^2 + 1} + x}{\sqrt{x^2 + 1 + x}} 
= \limit_{x\rightarrow\infty} \frac{x^2 + 1 - x^2}{\sqrt{x^2 + 1} + x} 
= \limit_{x\rightarrow\infty} \frac{1}{\sqrt{x^2 + 1} + x}\]
Looking at this function, it looks like both \(\sqrt{x^2 + 1}\) and \(x\) 
become arbritrarily large as \(x\rightarrow\infty\). Thus, we would expect this
function to converge to \(0\) as it approaches \(\infty\). To prove this with 
the Sandwhich Theorem, we must find a lower bound and upper bound that each 
converge to \(0\). \par
A lower bound for this function is easy; it never drops below \(0\), so 
\(g(x)\equiv0\) will do nicely. To find an upper bound, we can simply make the 
denominator smaller, so perhaps \(h(x) = \frac{1}{x}\) is a good fit. As we
know that both of these functions converge to \(0\) as \(x\rightarrow\infty\)
through the limit laws and standard limits, we can confidently state per the
Sandwhich Theorem that \(f\rightarrow0\) as \(x\rightarrow\infty\).

\subsubsection*{Example}
A function which lends itself to use of the Sandwhich Theorem is 
\(x^2\sin(\frac{1}{x})\)

\begin{plot}[
    xmax = 0.5,
    xmin = -0.5,
    ymax = 0.2,
    ymin = -0.2
]
    \addplot[blue, thick] {x^2*sin(deg(1/x))} 
    node[left, pos = 0.48, xshift = -0.2cm] 
    {\(x^2\sin\left(\frac{1}{x}\right)\)};
    \addplot[red, thick] {x^2} 
    node[left, pos = 0.52, xshift = -0.2cm] {\(x^2\)};
    \addplot[red, thick] {-1*x^2} 
    node[left, pos = 0.52, xshift = -0.2cm] {\(-x^2\)};
\end{plot}

As shown on the above plot, the function never strays beyond the bounds of
two parabolas. We can therefore evaluate its limit through the limits of the
two bounding functions.
\[\limit_{x\rightarrow0}x^2 = \limit_{x\rightarrow0} -x^2 = 0
\Rightarrow \limit_{x\rightarrow0} x^2\sin\left(\frac{1}{x}\right) = 0\]

\subsection*{Continuity}
Continuity is a property of a function which essentially describes the 
``smoothness'' of the function. For a function \(f\) to be continuous at a 
point \(x\), the limit
\[\limit_{x\rightarrow a}f(x) = f(a)\]
Must be true; i.e. the value of \(f\) at \(x\) must be the value that \(f\) 
approaches as it becomes arbritrarily close to \(x\). As a simple example, let
us check if \(f\) is continuous at \(x = 1\).
\[f(x) = 
    \begin{cases}
        2x, x \neq 1 \\
        4, x = 1 \\
    \end{cases}
\]
\[\limit_{x\rightarrow1} f(x) = \limit_{x\rightarrow1} 2x = 2\]
\[f(1) = 4 \neq 2\]
\[\therefore f \mathrm{\:is\:not\:continuous\:at\:}1\]

\subsubsection*{Continuity Theorems}
If \(f\) and \(g\) are real valued functions and \(c\) is a constant, then
assuming \(f\) and \(g\) are continuous at \(x = a\), the following functions
are additionally continuous at \(x = a\).
\begin{itemize}
    \item \(f + g\)
    \item \(cf\)
    \item \(fg\)
    \item \(\frac{f}{g} \mathrm{\:if\:} g(a) \neq 0\)
\end{itemize}
If \(f\) is continuous at \(x = a\) and \(g\) is continuous at \(x = f(a)\),
then \(g \circ f\) is continuous at \(x = a\). Thus if two continuous functions
are composed, the resultant function will in addition be continuous.

\bigskip
All of the following function types are continuous across their domain.
\begin{itemize}
    \item Polynomials
    \item Trigonometric functions
    \item Exponential functions
    \item Logarithmic functions
    \item \(n\)th root functions
    \item Hyperbolic functions
\end{itemize}

\subsubsection*{Example}
For which values of \(x\) is \(f\) continuous?
\[f(x) = \frac{\sin(x^2 + 1)}{\log(x)}\]
We know that \(\sin\) is continuous across \(\R\), as is \(x^2 + 1\). 
\(\log(x)\) is defined for \(\R^+\). Using function composition, we
know that \(\sin(x^2 + 1)\) is continuous on \(\R\), and using division we can
see that \(f\) will be continuous for all values in the domain of \(\log(x)\) 
where \(\log(x) \neq 0\), i.e. \(\R^+ \backslash \set{1}\).

\bigskip
If \(f\) is continuous at \(b\) and \(\limit_{x\rightarrow a} g(x) = b\) then
\[\limit_{x\rightarrow a} f\left[g(x)\right] 
= f\left[\limit_{x\rightarrow a}g(x)\right] = f(b)\]

\subsubsection*{Example}
\[\limit_{x\rightarrow\infty} \sin\left(e^{-x}\right)
= \sin\left(\limit_{x\rightarrow\infty} e^{-x}\right) = \sin(0) = 0\]
We can only do this because \(\sin\) is continuous on \(\R\).

\subsubsection*{Derivatives}
The derivative is defined using a limit.
\[f^\prime(a) = \limit_{h\rightarrow0}\frac{f(a + h) - f(a)}{h}\]
If this limit exists, the function is differentiable at \(a\). Geometrically,
this implies that a tangent line can be drawn at \(a\) on the graph with 
gradient yielded by the above limit.

\begin{plot}
    \addplot[thick, red] {(x-2)^2};
    \addplot[thick, blue] {2(x-2)};
\end{plot}

If a function is differentiable at \(x = a\), the the function is continuous
at that point.

\subsubsection*{L'H\^{o}pital's Rule}
Given \(f\) and \(g\) are differentiable functions near some value \(x = a\)
and \(g^\prime(x) \neq 0\) near \(a\) but \(\neq a\). If the limit
\[\limit_{x\rightarrow a} \frac{f(x)}{g(x)}\]
Has an indeterminate form of
\[\frac{0}{0} \:\mathrm{or}\: \frac{\infty}{\infty}\]
Then L'H\^{o}pital's Rule is applicable.
\[\limit_{x\rightarrow a}\frac{f(x)}{g(x)} 
= \limit_{x\rightarrow a}\frac{f^\prime(x)}{g^\prime(x)}\]
For example, we can solve limits like the one below much more easily.
\[\limit_{x\rightarrow0}\frac{\sin(x)}{x} 
= \limit_{x\rightarrow0}\frac{\cos(x)}{1} = \frac{\cos(0)}{1} = 1\]
L'H\^{o}pital's Rule can be applied repeatedly to the same function as long
as its conditions still hold. Sometimes the form of a function must be modified
to have a quotient before the rule can be applied to it.

\subsubsection*{Rigour}
Thus far, we have used the phrase \textit{arbritrarily close} without a proper
definition of what that means. A more formal definition of this exists, where
we pick an arbritray positive number \(\epsilon\). When we do this, there is
another positive number \(\delta\) for which \(\abs{f(x) - L} < \epsilon\) when
\(0 < \abs{x - a} < \delta\).

\subsection*{Sequences}
A sequence is a function which maps the natural numbers to \(\R\) i.e. of the
form \(f: \N \rightarrow \R\). They can be thought of as ordered lists of real
numbers, or as functions where \(f(n)\) is the \(n\)th number in the sequence.
A common notation for a sequence \(a\) is \(\set{a_n}\), where \(a_n\) is a 
function defining the \(n\)th element. \par
A sequence has the limit \(L\) if \(a_n\) can be made arbritrarily close to 
\(L\) by making \(n\) sufficiently large. To indicate this, we write
\[\limit_{n\rightarrow\infty} a_n = L\]
If a limit \(L\) exists, then the sequence converges, otherwise it diverges.
The only major difference between limits on sequences and limits on functions
is that sequences deal with discrete rather than continuous values. This 
matters significantly in cases like trigonometric functions. If a function
converges, then a sequence with that function will also converge. \par
The limit laws and Sandwhich Theorem apply to limits for sequences as well.
For the Sandwhich Theorem, the sandwhiching sequences can consider only a range
of \(n\), greater than some value \(N\).

\subsubsection*{Examples}
For each of the following sequences, determine whether they converge or 
diverge.
\begin{itemize}
    \item \(\set{\frac{1}{n}}\) converges to \(0\).
    \item \(\set{-1^{n - 1}}\) diverges, oscillating between \(1\) and \(-1\).
    \item \(\set{n}\) diverges to \(\infty\).
\end{itemize}

\subsubsection*{Standard Limits of Sequences}
A set of standard limits of sequences exist, which can be used to solve other
limits involving sequences.

\begin{formulalist}
    \(\limit_{n\rightarrow\infty}\frac{1}{n^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{n\rightarrow\infty} r^n = 0 \:\:\:(\abs{r} < 1)\) \\
    \(\limit_{n\rightarrow\infty} a^{\frac{1}{n}} = 1 \:\:\:(a > 0)\) \\
    \(\limit_{n\rightarrow\infty} n^{\frac{1}{n}} = 1\) \\
    \(\limit_{n\rightarrow\infty} \frac{a^n}{n!} = 0 \:\:\:(a \in \R)\) \\
    \(\limit_{n\rightarrow\infty} \frac{\log(n)}{n^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{n\rightarrow\infty} \left(1 + \frac{a}{n}\right)^n 
    = e^a \:\:\:(a \in \R)\) \\
    \(\limit_{n\rightarrow\infty} \frac{n^p}{a^n} 
    = 0 \:\:\:(p \in \R, a > 1)\) \\  
\end{formulalist}

\subsubsection*{Examples}
\[\limit_{n\rightarrow\infty}\left[\left(\frac{n - 2}{n}\right)^n 
+ \frac{4n^2}{3^n}\right] = 
\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n + 
\limit_{n\rightarrow\infty} \frac{4n^2}{3^n}\]
\[\left(\frac{n - 2}{n}\right)^n = \left(1 - \frac{2}{n}\right)^n \Rightarrow
\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n = e^{-2} 
\Rightarrow\]
\[\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n + 
\limit_{n\rightarrow\infty} \frac{4n^2}{3^n} = e^{-2} + 0 = e^{-2}\]
Using standard limits, we can easily find the solution to this fairly 
complex-looking limit. The key here is recognising that the first term can
be easily rearranged to the exponential standard limit, with the second term
fairly naturally simplifying to another standard limit.
\[a_n = \frac{3^n + 2}{4^n + 2^n}, n \geq 1\]
\[\frac{3^n + 2}{4^n + 2^n}\cdot\frac{\frac{1}{4^n}}{\frac{1}{4^n}} = 
\frac{\left(\frac{3}{4}\right)^n + \frac{2}{4^n}}{1 +
\left(\frac{1}{2}\right)^n}\]
\[\limit_{n\rightarrow\infty} a_n = \frac{0 + 0}{1 + 0} = 0\]
By dividing through by the largest term, we find out that it outweighs the 
terms in the denominator, an that the whole limit collapses to \(0\). A tool
for doing this is the \textit{order hierachy}, which indicates the growth rate
of various forms for large \(n\)
\[\log(n) \ll n^p \ll a^n \ll n!\]
A situation where the Sandwhich Theorem is very applicable is one where the 
function in question clearly falls into some well defined range. For example
\[\limit_{n\rightarrow\infty}\frac{1 + \sin^2\left(\frac{n\pi}{3}\right)}{
\sqrt{n}}\]
\[0 \leq \sin^2\left(\frac{n\pi}{3}\right) \leq 1 \Rightarrow 1 \leq 1 + 
sin^2\left(\frac{n\pi}{3}\right) \leq 2\]
\[\frac{1}{\sqrt{n}} \leq \frac{\sin^2\left(\frac{n\pi}{3}\right)}{\sqrt{n}} 
\leq \frac{2}{\sqrt{n}}\]
\[\limit_{n\rightarrow\infty}\frac{1}{\sqrt{n}} 
= \limit_{n\rightarrow\infty}\frac{1}{\sqrt{n}} = 0 \Rightarrow\]
\[\limit_{n\rightarrow\infty}\frac{1 + \sin^2\left(\frac{n\pi}{3}\right)}{
\sqrt{n}} = 0\]
Here, we attack the most complex part of the problem and construct two 
equations which elegantly border it. We then use the limits of these equations
to solve the overall limit.

\subsection*{Series}
A series arises when one attempts to sum up the values in a sequence. For
a sequence \(\set{a_n}\), if we add each \(a_n\) in order, we create another
sequence \(\set{s_n}\).
\[s_1 = a_1\]
\[s_2 = a_1 + a_2\]
\[s_3 = a_2 + a_2 + a_2\]
This \textit{sequence of partial sums} \(\set{s_n}\) may or may not converge.
In the case that it does, we describe the sum \(S\) as
\[S = \limit_{n\rightarrow\infty} s_n = \limit_{n\rightarrow\infty} 
(a_1 + a_2 + \ldots + a_n)\]
For example, we can find the sum of a sequence like so:
\[a_n = \left(\frac{1}{2}\right)^n, n\geq 1\]
\[s_1 = a_2 = \frac{1}{2}\]
\[s_2 = a_1 + a_2 = \frac{3}{4}\]
\[s_n = \frac{2^n - 1}{2^n} = 1 - \frac{1}{2^n}\]
\[\limit_{n\rightarrow\infty}s_n = \limit_{n\rightarrow\infty} 1 - 
\frac{1}{2^n} = \limit_{n\rightarrow\infty} 1 - \limit_{n\rightarrow\infty} 
\frac{1}{2^n} = 1 - 0 = 1\]
This is example is a \textit{geometric series}. More generally, a series with
terms \(a_n\) is denoted with the sum
\[s_n = \sumninf a_n\]
Where the value on the bottom (\(n = 1\)) is the starting value of \(n\) and
the value on the top is the value we limit towards. If the limit of \(s_n\)
exists, the series converges. If it does not, the series diverges. For
the sequence \(\set{n} = 1, 2, 3, 4, \ldots\) we have the series
\[\sumninf n = 1 + 2 + 3 + 4 + 5 + \ldots\]
Because the sequence and series both diverge to infinity, the series diverges.
Decimal representations of numbers can be thought of as a series. If we take
\[\set{\frac{1}{10^n}} = 0.1, 0.01, 0.001, \ldots\]
\[\sumninf \frac{1}{10^n} = 0.1 + 0.01 + 0.001 + \ldots = 
0.1111\ldots\]
A general case exists, for a number \(x \in (0, 1)\) with the decimal digits
\(d_1, d_2, \ldots\) then
\[x = 0.d_1d_2d_3\ldots = \sumninf \frac{d_n}{10^n}\]
It is important to note that a sequence differs from a sequence of partial sums
differs from a series. The sequence of partial sums represents a \textit{finite
approximation} of the sum of the sequence.

\subsubsection*{Properties of Series}
If \(\sumninf a_n\) and \(\sumninf b_n\) are converging series, and 
\(c\in\R\backslash\set{0}\) is a constant then

\begin{itemize}
    \item \(\sumninf (a_n + b_n) = \sumninf a_n + \sumninf b_n\)also converges.
    \item \(\sumninf (ca_n) = c\sumninf a_n\) and still converges. If 
        \(\sumninf a_n\) diverges, then \(\sumninf (ca_n)\) also diverges.
\end{itemize}

\subsubsection*{Geometric Series}
A geometric series has the form
\[\sumninf[0] ar^n = \sumninf ar^{n - 1} = a + ar + ar^2 + ar^3 + \ldots\]
Where \(a\in\R\backslash\set{0}\) and \(r\in\R\). If \(\abs{r} < 1\), the
series will converge while if \(\abs{r} \geq 1\) the series will diverge.
In the case that \(\abs{r} < 1\), we have that
\[\sumninf[0]ar^n = \frac{a}{1 - r}\]
In addition, there is a general form for a geometric series (though it is 
somewhat less elegant)
\[\sum_{k = 0}^n ar^k = \frac{a\left(1 - r^{n + 1}\right)}{1 - r}\]
Though it must be noted that this formula is only valid for \(r\neq1\).

\subsubsection*{Harmonic \(p\) Series}
A harmonic \(p\) series has the form
\[\sumninf\frac{1}{n^p}\]
These series converge if \(p > 1\) and diverge if \(p \leq 1\). For instance
\[\sumninf \frac{1}{n^2}\]
Converges while
\[\sumninf \frac{1}{n}\]
Diverges. \par
It is worth noting that thanks to the previously explored properties of series,
we know that any multiple of a harmonic \(p\) series has the same divergence
behaviour as it's base.

\subsubsection*{Divergence Test}
A test exists to determine whether a series diverges. If the limit of the 
sequence the series is based on is not zero, then the series diverges. More
precisely
\[\limit_{n\rightarrow\infty} a_n \neq 0 \Rightarrow \sumninf a_n 
\:\mathrm{diverges}\]
If the limit is exactly \(0\), the series may converge or diverge and another
test must be used to identify convergence or divergence.

\subsubsection*{Comparison Test}
For two positive series (all elements non-negative) of the forms
\[\sumninf a_n \mand \sumninf b_n\]
\begin{itemize}
    \item if \(a_n \leq b_n\) for all \(n\) and \(\sumninf b_n\) converges, 
        then \(\sumninf a_n\) converges.
    \item if \(a_n \geq b_n\) for all \(n\) and \(\sumninf b_n\) diverges,
        then \(\sumninf a_n\) diverges.
\end{itemize}
In general, we compare a given unknown series (\(a\) above) to a harmonic \(p\)
series or a geometric series, because we know the behaviour of these in some 
detail. As an example, we can try and identify the convergence or divergence
of the following series.
\[\sumninf \frac{3 + \frac{5}{n}}{2n^2 + n + 2}\]
\[\frac{3 + \frac{5}{n}}{2n^2 + n + 2} \approx \frac{3}{2n^2}\]
By identifying the most significant terms in the denominator and the numerator,
we have identified that we expect the series to converge.
\[\frac{3 + \frac{5}{n}}{2n^2 + n + 2} \leq \frac{8}{2n^2} = \frac{4}{n^2}\]
Here, we wanted to simplify the denominator into something that we recognise
and can deal with while making it larger. To do that, we would ideally like
to reduce the term to have only a single term in both the numerator and 
denominator. To simplify and enlarge the denominator, we can simply remove the
\(n\) denominator of the \(5\), as we know \(n \geq 1\). Removing terms from 
the denominator inherently makes the fraction larger, so we can just remove the
\(n + 2\) term, rearranging into a harmonic \(p\) series.
\[\sumninf \frac{4}{n^2} \:\mathrm{converges} \Rightarrow \sumninf \frac{3 + 
\frac{5}{n}}{2n^2 + n + 2} \:\mathrm{converges}\]

\subsubsection*{Ratio Test}
For a positive term series of the form
\[\sumninf a_n\]
With the limit
\[L = \limit_{n\rightarrow\infty} \frac{a_{n + 1}}{a_n}\]
Then, this series has the following properties.
\begin{itemize}
    \item If \(L < 1\), the series converges
    \item If \(L > 1\), the series diverges
    \item If \(L = 1\), the ratio test is inconclusive.
\end{itemize}
The ratio test is most useful in situations where \(a_n\) contains an 
exponential or factorial function of \(n\). For instance, we can figure out
whether the following series converges or diverges.
\[\sumninf \frac{10^n}{n!}\]
\[\limit_{n\rightarrow\infty} \frac{10^{n + 1}}{(n + 1)!}\div\frac{10^n}{n!} = 
\limit_{n\rightarrow\infty} \frac{10^{n + 1}}{(n + 1)!}\times\frac{n!}{10^n}
= \limit_{n\rightarrow\infty} \frac{10^1}{1} \times \frac{n!}{(n + 1)!}\]
\[ = \limit_{n\rightarrow\infty} \frac{10}{n + 1} = 0 \Rightarrow 
\sumninf \frac{10^n}{n!}\:\mathrm{converges}\]

\section*{Hyperbolic Functions}

To deal with hyperbolic functions, it can help to revise a few function 
properties. Even functions have the property that
\[f(-x) = f(x)\]
For example, \(x^2\) is an even function, as is \(cos(x)\). An odd function
has the property that
\[f(-x) = -f(x)\]
They have a kind of rotational symmetry, where a rotation by \(\pi\) doesn't
alter the graph. \(sin(x)\) and \(x^3\) are odd functions.

\bigskip
The first hyperbolic function is the hyperbolic cosine function, \(\cosh\), 
often pronounced as it's written (``cosh'').
\[\cosh(x) = \frac{1}{2}(e^x + e^{-x})\]
\(\cosh\) is an even function. At \(0\), it is equal to one, and the two sides
rise evenly on each side. It essentially looks like a parabola. 
The plot of \(\cosh\) is below.

\begin{plot}[
    xmin = -3,
    xmax = 3,
    ymin = -1,
    ymax = 6
]
    \addplot[thick, blue] {cosh(x)} 
    node[right, pos = 0.6, xshift = 0.2cm] {\(\cosh(x)\)};
\end{plot}

The second hyperbolic function is hyperbolic sine, \(\sinh\), often read as
``shine''. It is defined as
\[\sinh(x) = \frac{1}{2}(e^x - e^{-x})\]
\(\sinh\) has a \(x\)-intercept at \(0\). It is an odd function. The plot of
\(\sinh\) is below.

\begin{plot}[
    xmin = -3,
    xmax = 3,
    ymin = -6,
    ymax = 6
]
    \addplot[thick, blue] {sinh(x)} 
    node[right, pos = 0.6, xshift = 0.2cm] {\(\sinh(x)\)};
\end{plot}

To complete the set, we have hyperbolic tangent, \(\tanh\), often read as 
``than''. It is defined as the quotient of \(\sinh\) on \(\cosh\), i.e.
\[\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]
\(\tanh\) has the range \((-1, 1)\), with an \(x\)-intercept at \(0\). It is an
odd function, due to the presence of \(\cosh\) in it's definition. The plot of
\(\tanh\) is below.

\begin{plot}[
    xmin = -5,
    xmax = 5,
    ymin = -2,
    ymax = 2
]
    \addplot[thick, red] {tanh(x)}
    node[above, pos = 0.8] {\(\tanh(x)\)};
\end{plot}

The trigonometric functions have the trigonometric identity
\[\cos^2(\theta) + \sin^2(\theta) = 1\]
Which mirrors the form of the equation of a circle, \(x^2 + y^2 = r\). This
tells us that a \((\sin(\theta), \cos(\theta))\) pair encodes a position on
the unit circle. \par
The hyperbolic functions mirror this behaviour for a hyperbola 
(\(x^2 - y^2 = 1)\)), having the property that \(\cosh^2(t) - \sinh^2(t) = 1\).
i.e. for a pair \((\cosh(t), \sinh(t))\), a point on the hyperbola 
\(x^2 - y^2 = 1\) is uniquely encoded. Only points on the right hand side of 
the hyperbola are encoded in this way.

\begin{plot}[
    xmin = 0,
    xmax = 3,
    ymin = -3,
    ymax = 3
]
    \addplot[thick, red] {sqrt(x^2 - 1)};
    \addplot[thick, red] {-sqrt(x^2 - 1)};
    \node[above left, red] at (axis cs: 2, 2) {\(x^2 + y^2 = 1\)};

    \node[circle, fill, inner sep = 3pt] at (axis cs: 1, 0) {};
    \node[above left, yshift = 0.2cm] at (axis cs: 1, 0) 
    {\(\cosh(0), \sinh(0)\)};
\end{plot}

\subsubsection*{Hyperbolic Identities}

We can use the relationship between \(\cosh\) and \(\sinh\) to solve equations.
For example, in the following case we can find \(\sinh\) and \(\tanh\) from the
value of \(\cosh\) and the fact that \(x < 0\).
\[\cosh(x) = \frac{13}{12}\]
\[\cosh^2(x) - \sinh^2(x) = 1 \Rightarrow \sinh^2(x) = 
\left(\frac{13}{12}\right)^2 - 1 \Rightarrow \]
\[\sinh(x) = \sqrt{\frac{169}{144} - 1} = \sqrt{\frac{25}{144}} 
= \pm \frac{5}{12}\]
\[x < 0 \Rightarrow \sinh(x) = -\frac{5}{12}\]
\[\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{-\frac{5}{12}}{\frac{13}{12}} 
= -\frac{5}{13}\]

The key identity that allowed us to solve this was the fundamental hyperbolic
identity, that of

\begin{formulalist}
    \(\cosh^2(x) - \sinh^2(x) = 1\) \\
\end{formulalist}

A variety of other more specialised identities also exist, beginning with 
the addition formulae.

\begin{formulalist}
    \(\sinh(x + y) = \sinh(x)\cosh(y) + \cosh(x)\sinh(y)\) \\
    \(\cosh(x + y) = \cosh(x)\cosh(y) + \sinh(x)\sinh(y)\) \\
    \(\sinh(x - y) = \sinh(x)\cosh(y) - \cosh(x)\sinh(y)\) \\
    \(\sinh(x + y) = \cosh(x)\cosh(y) - \sinh(x)\sinh(y)\) \\
\end{formulalist}

We also have the double angle formulae, just as with trigonometric functions.

\begin{formulalist}
    \(\sinh(2x) = 2\sinh(x)\cosh(x)\) \\
    \(\cosh(2x) = \cosh^2(x) + \sinh^2(x)\) \\
    \(\cosh(2x) = 2\cosh^2(x) - 1\) \\
    \(\cosh(2x) = 2\sinh^2(x) + 1\) \\
\end{formulalist}

\subsubsection*{Reciprocal Hyperbolic Functions}

Three reciprocal hyperbolic functions exist. These are
\[\sech(x) = \frac{1}{\cosh(x)}\]
\[\cosech(x) = \frac{1}{\sinh}(x)\]
\[\coth(x) = \frac{1}{\tanh(x)} = \frac{\cosh(x)}{\sinh(x)}\]
The plots of these functions are below.

\FloatBarrier
\begin{figure}[h!]
    \makebox[\textwidth]{
        \hspace{-2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, blue] {1/cosh(x)}
                node[above right, pos = 0.6] {\(\sech(x)\)};
            \end{smallplot}
        \end{minipage}
        \hspace{2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, blue] {1/sinh(x)}
                node[above right, pos = 0.8] {\(\sinh(x)\)};
            \end{smallplot}
        \end{minipage}
        \hspace{2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, red] {1/tanh(x)}
                node[above right, pos = 0.8] {\(\coth(x)\)};
            \end{smallplot}
        \end{minipage}
    }
\end{figure}
\FloatBarrier

These functions have the associated identities

\begin{formulalist}
    \(\cosh^2(x) - \sinh^2(x) = 1\) \\
    \(\coth^2(x) - 1 = \cosech^2(x)\) \\
    \(1 - \tanh^2(x) = \sech^2(x)\) \\
\end{formulalist}

\subsubsection*{Derivatives of Hyperbolic Functions}

\begin{formulalist}
    \(\derivx{\cosh(x)} = \sinh(x)\) \\
    \(\derivx{\sinh(x)} = \cosh(x)\) \\
    \(\derivx{\tanh(x)} = \sech^2(x)\) \\
    \(\derivx{\sech(x)} = -\sech(x)\tanh(x)\) \\
    \(\derivx{\cosech(x)} = -\cosech(x)\coth(x) \:(x \neq 0)\) \\
    \(\derivx{\coth(x)} = -\cosech^2(x) \:(x \neq 0\) \\ 
\end{formulalist}

These derivatives bear some resemblance to trigonometric derivatives,
however it is worth noting that no negative signs appear when taking
derivatives of \(\sinh\) and \(\cosh\) as they do with trigonometric
functions.

\subsubsection*{Inverse Hyperbolic Functions}

Finally, inverses of some of the hyperbolic functions exist. For
\(\sinh(x)\), we have the inverse function \(\arcsinh\). This function can
also be written in terms of a logarithm, which makes sense as it is the inverse
of an exponential function.
\[\arcsinh = \log(x + \sqrt{x^2 + 1})\]
Because \(\sinh\) is a nice one-to-one function, it has a simple inversion 
function, pictured below.

\begin{plot}
    \addplot[thick, blue, dashed, <->] {sinh(x)}
    node[left, pos = 0.3] {\(\sinh(x)\)};
    \addplot[thick, blue, <->] {ln(x + sqrt(x^2 + 1))}
    node[above left, pos = 0.9] {\(\arcsinh(x)\)};
\end{plot}

For \(\cosh\), the domain of \(\cosh\) must be restricted to yield a one-to-one
function. The domain is therefore restricted to \([0, \infty)\). Because the 
range of \(\cosh\) is \([1, \infty)\), this is the domain of \(\arccosh\).
\[\arccosh = \log(x + \sqrt{x^2 - 1}), \:(x \geq 1)\]
The plot of \(\arccosh\) is below.

\begin{plot}
    \addplot[thick, blue, dashed, <->] {cosh(x)}
    node[left, pos = 0.4, xshift = -0.2cm] {\(\cosh(x)\)};
    \addplot[thick, blue, ->] {ln(x + sqrt(x^2 - 1))}
    node[below right, pos = 0.6] {\(\arccosh(x)\)};
\end{plot}

This means that \(\arccosh(\cosh(x)) = \abs{x}\) rather than \(x\). The final
inverse is \(\arctanh\). As \(\tanh\) is one-to-one, this function is a perfect
inverse.
\[\frac{1}{2}\log\left(\frac{1 + x}{1 - x}\right), \:(-1 < x < 1)\]
The range of \(\tanh\) because the domain of \(\arctanh\), so \(\arctanh\)
accepts values in the range \((-1, 1)\) and spits out values in \(\R\).

\begin{plot}
    \addplot[thick, red, dashed, <->] {tanh(x)}
    node[above, pos = 0.8] {\(\tanh(x)\)};
    \addplot[thick, red, <->] {0.5*ln((1 + x)/(1 - x))}
    node[left, pos = 0.1] {\(\arctanh(x)\)};
\end{plot}

The inverse reciprocal hyperbolic functions are also defined, though they are 
seldom used. The formulas for the inverse functions are obtained in the usual
way of obtaining inverse functions, by taking \(y = \arcsinh(x)\) and then 
solving for \(x\) in terms of \(y\).

\subsubsection*{Derivatives of Inverse Hyperbolic Functions}

\begin{formulalist}
    \(\derivx{\arcsinh(x)} = \frac{1}{\sqrt{x^2 + 1}}\) \\
    \(\derivx{\arccosh(x)} = \frac{1}{\sqrt{x^2 - 1}}, \:(x > 1)\) \\
    \(\derivx{\arctanh(x)} = \frac{1}{1 - x^2}, \:(-1 < x < 1)\) \\
\end{formulalist}

\section*{Complex Numbers}

Complex numbers extend the natural, integer, rational and real numbers. By 
introducing the number \(i\) with the property that \(i^2 = -1\), we gain
access to another set of numbers of the form
\[\C = \set{x + yi | x, y \in \R}\]
This allows us to do some powerful things, like to solve any polynomial
function with complex coeffecients. Their basic operations follow. The most
basic is of course addition, where one just adds the relevant terms.
\[(2 + 3i) + (4 + i) = 6 + 4i\]
Multiplication is performed piecewise in F.O.I.L. style.
\[(1 + i)(2 + i) = 2 + i + 2i + i^2 = 1 + 3i\]
The complex conjugate of a complex number is found by multiplying the imaginary
part (the multiple of \(i\)) by \(-1\).
\[z = x + iy \Rightarrow \bar{z} = x - iy\]
The modulus of a complex number can be thought of as the vector norm of the
number and is calculated as
\[z = x + iy \Rightarrow \abs{z} = \sqrt{x^2 + y^2}\]
The argument of a complex number is the angle it forms with the positive real
axis (usually the \(x\)-axis). This argument \(\theta\) has the property that
\[\tan(\theta) = \frac{y}{x}\]
Using this argument we can write a complex number in trigonometric polar form,
so where \(r\) is the norm of the complex number it can be written as
\[z = r(\cos(\theta) + i\sin(\theta))\]
The argument \(\theta\) is non-unique, and so the concept of the principal 
argument as the single possible \(\theta\) which lies in the interval 
\((-\pi, \pi]\) is called the \textit{principal argument} and is unique.

\end{flushleft}
\end{document}
