\documentclass[12pt]{report}

\usepackage{import}
\import{../}{preamble.tex}

\begin{document}
\begin{flushleft}

\section*{Notation}
In this subject, a variety of notation is used.
\begin{itemize}
    \item \(\mid\) read ``such that'' used largely in set definitions: 
        \(\set{x \in \R \mid x \geq 2}\)
    \item \(\forall\) read ``for all''.
    \item \(\exists\) read ``there exists''.
    \item \(\equiv\) read ``is equivalent to' used to signify function
        equality, etc.
    \item \(\ll\) read ``much less than''.
    \item \(\log\) denotes the natural logarithm, \(\ln\).
    \item Inverse trigonometric functions are written \(\arcsin\) rather than
        \(\sin^{-1}\).
\end{itemize}

Some common sets have specific symbols associated with them.
\begin{itemize}
    \item Natural numbers, \(\N = \set{1, 2, 3, \ldots}\). Exclusive of \(0\)
        in this course.
    \item Integers, \(\Z = \set{\ldots, -2, -1, 0, 1, 2, \ldots}\)
    \item Rational numbers, 
        \(\Q = \set{\frac{m}{n} \mid m, n \in \Z, n \neq 0}\)
    \item Real numbers, \(\R\)
    \item Complex numbers, \(\C = \set{x + iy \mid x, y \in \R, i^2 = -1}\)
    \item \(xy\) plane, \(\R^2 = \set{(x, y) \mid x, y \in \R}\)
    \item Three dimensional space, 
        \(\R^3 = \set{(x, y, z) \mid x, y, z \in \R}\)
\end{itemize}

\section*{Limits, Continuity, Sequences, Series}
\subsection*{Limits}
Limits form the fundamental concept behind the definition of a derivative;
the instantaneous rate of change of a function at a point is defined in terms
of a limit. \par
A limit is defined as follows. For a function \(f(x)\), the limit of \(f(x)\) 
as \(x\) approaches \(a\) is \(L\), written as
\[\lim_{x\rightarrow a}f(x) = L\]
If \(f(x)\) gets continually closer to \(L\) but \(x \neq a\). If a limit 
exists, it must be a unique finite real number.

\subsubsection*{Examples}
If \(f(x)\) is defined as follows 
\[
    f(x) =
    \begin{cases}
        2x, x \neq 1 \\
        4, x = 1 \\
    \end{cases}
\]
Evaluate the following
\[\lim_{x\rightarrow 1} f(x)\]
As \(f(x)\) gets arbritrarily close to \(2\) whenever \(x\) is close to but not
equal to \(1\), therefore the limit as \(x\rightarrow1\) is \(2\).

\bigskip\bigskip
Evaluate the following limit.
\[\lim_{x\rightarrow0}f(x) = \frac{1}{x^2}\]
As \(f(x)\) is unbounded as \(x\rightarrow0\). Therefore, \(f(x)\) cannot be 
made arbritrarily close to any one number and the limit does not exist.

\bigskip\bigskip
Evaluate the following limit.
\[
    \lim_{x\rightarrow0}
    \begin{cases}
        1, x < 0 \\
        2, x \geq 0 \\
    \end{cases}
\]
As \(x\) approaches \(0\) from the right, it grows arbritrarily close to \(2\),
while as it approaches from the left it grows arbritrarily close to \(1\). As 
it doesn't grow arbritrarily close to a single value, no limit exists here.

\subsubsection*{Additional Limt Notation}
In the previous examples, we noticed that a common way to examine limits is by
considering the values the function approaches from each side. Because this is
such a common construct, notation exists for the left and right limits
independently. The right and left limits of the function \(f\) approaching 
\(0\), examined in the final example above are written as
\[\lim_{x\rightarrow0^+}f(x) = 2\]
\[\lim_{x\rightarrow0^-}f(x) = 1\]
In general, for a limit to exist the following statement must be true
\[\lim_{x\rightarrow a} = L \Leftrightarrow \lim_{x\rightarrow a^-} = L
\mand \lim_{x\rightarrow a^+} = L\]
i.e. for the limit to be \(L\), both the left and right limits must be \(L\).

\subsubsection*{Limit Laws}
For two real valued functions \(f\) and \(g\), and \(c \in \R\) a constant. If
the limits \(\limit_{x\rightarrow a} f(x)\) and 
\(\limit_{x\rightarrow a} g(x)\) exist, then the following limit laws 
apply.

\begin{formulalist}
    \(\limit_{x\rightarrow a}\left[f(x) + g(x)\right]
    = \limit_{x\rightarrow a} f(x) 
    + \limit_{x\rightarrow a} g(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[cf(x)\right] 
    = c\limit_{x \rightarrow a}f(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[f(x)g(x)\right] 
    = \limit_{x \rightarrow a}f(x) \cdot
    \limit_{x \rightarrow a}g(x)\) \\ 
    \(\limit_{x \rightarrow a}\left[\frac{f(x)}{g(x)}\right] 
    = \frac{\limit_{x \rightarrow a}f(x)}{
    \limit_{x \rightarrow a}g(x)} 
    \:\left(\limit_{x \rightarrow a}g(x) \neq 0\right)\) \\ 
    \(\limit_{x \rightarrow a} c = c\) \\ 
    \(\limit_{x \rightarrow a} x = a\) \\ 
\end{formulalist}

\subsubsection*{Example}
Using the limit laws, evaluate the limit
\[\lim_{x\rightarrow2}\frac{x^3 + 2x^2 - 1}{5 - 3x}\]
\[\lim_{x\rightarrow2}\frac{x^3 + 2x^2 - 1}{5 - 3x} 
= \frac{\limit_{x\rightarrow2} x^3 + 2x^2 - 1}{
\limit_{x\rightarrow2} 5 - 3x} = \frac{8 + 8 - 1}{5 - 6} 
= \frac{15}{-1} = -15\]
One could use more of the limit laws to break this down further; for instance
breaking the added terms into individual limits, or breaking the \(x^n\) terms
down using the product law.

\subsubsection*{Limits and Infinity}
We can talk about a limit as \(x\rightarrow\infty\), referring to what happens
to the limit as \(x\) is made arbritrarily large. So the limit
\[\lim_{x\rightarrow\infty} f(x) = L\]
Is stating that as \(x\) is made arbritrarily larger, \(f(x)\) becomes 
arbritrarily closer to \(L\). \(L\) must be finite for \(f(x)\) to take it's
value.

\begin{plot}
    \addplot[blue, thick] {2.71828^(-1*x)}
    node[above, pos = 0.9] {\(e^{-x}\)};
\end{plot}

For example, in the above plot we can see that as \(x\) becomes arbritrarily 
close to infinity, \(e^{-x}\) becomes arbritrarily close to \(0\). Therefore,
\[\lim_{x\rightarrow\infty} e^{-x} = 0\]
While this process of examining a graph and noting its convergence is practical
for certain examples, it is a little laborious in the long term, so we use the 
constructs of standard limits to solve many problems.

\subsubsection*{Standard Limits}
The following limits can be used without proof in this subject, with their 
truth taken as gospel.

\begin{formulalist}
    \(\limit_{x\rightarrow\infty}\frac{1}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty}r^x = 0 \:\:\:(0 \leq r < 1)\) \\
    \(\limit_{x\rightarrow\infty}\frac{1}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty} r^x = 0 \:\:\:(0 \leq r < 1)\) \\
    \(\limit_{x\rightarrow\infty} a^{\frac{1}{x}} = 1 \:\:\:(a > 0)\) \\
    \(\limit_{x\rightarrow\infty} x^{\frac{1}{x}} = 1\) \\
    \(\limit_{x\rightarrow\infty} \frac{\log(x)}{x^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{x\rightarrow\infty} \left(1 + \frac{a}{x}\right)^x 
    = e^a \:\:\:(a \in \R)\) \\
    \(\limit_{x\rightarrow\infty} \frac{x^p}{a^x} 
    = 0 \:\:\:(p \in \R, a > 1)\) \\  
\end{formulalist}

For example, in the case of \(e^{-x}\), we can use the second limit from above
by taking \(r = \frac{1}{e}\).

\subsubsection*{Terminology}
If a limit exists, we can state that \(f(x)\) \textit{converges} as \(x\) 
approaches \(a\). Inversely, we can state \(f(x)\) \textit{diverges} as \(x\)
approaches \(a\). \par
For example, as \(\sin(x)\) oscillates between \(-1\) and \(1\), it cannot 
approach a single number and therefore diverges as \(x\rightarrow\infty\). \par
It is important to note that \(\infty\) isn't a number; in general it denotes
``any arbritrarily large number''. A limit cannot be equal to infinity. Because
infinity is not a number, certain cases become indeterminate.
\[\limit_{x\rightarrow\infty} \frac{3x^2 - 2x + 3}{x^2 + 4x + 4}\]
Here, we can see that both the numerator and the denominator approach infinity
as \(x\rightarrow\infty\). We cannot therefore divide their individual limits 
to find the overall limit and must instead alter the form to find the limit. 
\par
In this case, we can do this by dividing numerator and denominator by 
\(\frac{1}{x^2}\).
\[\frac{\frac{1}{x^2}}{\frac{1}{x^2}} \cdot \frac{3x^2 - 2x + 3}{x^2 + 4x + 4}
= \frac{3 - \frac{2}{x} + \frac{3}{x^2}}{1 - \frac{4}{x} + \frac{4}{x^2}}\]
\[\frac{\limit_{x\rightarrow\infty} 3 - \frac{2}{x} + \frac{3}{x^2}}{
\limit_{x\rightarrow\infty} 1 - \frac{4}{x} + \frac{4}{x^2}} = 
\frac{3}{1} = 3\]
By modifying the fraction and then applying limit laws, we can solve this 
initially indeterminate limit.

\subsubsection*{The Sandwhich Theorem}
The Sandwhich Theorem states that if \(g(x) \leq f(x) \leq h(x)\) when 
\(x \approx a\) but \(x \neq a\) and
\[\limit_{x\rightarrow a}g(x) = \limit_{x\rightarrow a}h(x) = L\]
Then \(\limit_{x\rightarrow a}f(x) = L\). In essence, it states that if a 
function lies between two other functions who each converge to \(L\) at \(a\),
then that function must also converge to \(a\). This theorem can also be used
to solve functions of the indeterminate form \(\infty - \infty\). For example,
\(\limit_{x\rightarrow\infty} f(x) = \sqrt{x^2 + 1} - x\) is of this form. We
can simplify to some degree, but we need the Sandwhich Theorem to finish the 
problem.
\[\limit_{x\rightarrow\infty} \left(\sqrt{x^2 + 1} - x\right) \cdot 
\frac{\sqrt{x^2 + 1} + x}{\sqrt{x^2 + 1 + x}} 
= \limit_{x\rightarrow\infty} \frac{x^2 + 1 - x^2}{\sqrt{x^2 + 1} + x} 
= \limit_{x\rightarrow\infty} \frac{1}{\sqrt{x^2 + 1} + x}\]
Looking at this function, it looks like both \(\sqrt{x^2 + 1}\) and \(x\) 
become arbritrarily large as \(x\rightarrow\infty\). Thus, we would expect this
function to converge to \(0\) as it approaches \(\infty\). To prove this with 
the Sandwhich Theorem, we must find a lower bound and upper bound that each 
converge to \(0\). \par
A lower bound for this function is easy; it never drops below \(0\), so 
\(g(x)\equiv0\) will do nicely. To find an upper bound, we can simply make the 
denominator smaller, so perhaps \(h(x) = \frac{1}{x}\) is a good fit. As we
know that both of these functions converge to \(0\) as \(x\rightarrow\infty\)
through the limit laws and standard limits, we can confidently state per the
Sandwhich Theorem that \(f\rightarrow0\) as \(x\rightarrow\infty\).

\subsubsection*{Example}
A function which lends itself to use of the Sandwhich Theorem is 
\(x^2\sin(\frac{1}{x})\)

\begin{plot}[
    xmax = 0.5,
    xmin = -0.5,
    ymax = 0.2,
    ymin = -0.2
]
    \addplot[blue, thick] {x^2*sin(deg(1/x))} 
    node[left, pos = 0.48, xshift = -0.2cm] 
    {\(x^2\sin\left(\frac{1}{x}\right)\)};
    \addplot[red, thick] {x^2} 
    node[left, pos = 0.52, xshift = -0.2cm] {\(x^2\)};
    \addplot[red, thick] {-1*x^2} 
    node[left, pos = 0.52, xshift = -0.2cm] {\(-x^2\)};
\end{plot}

As shown on the above plot, the function never strays beyond the bounds of
two parabolas. We can therefore evaluate its limit through the limits of the
two bounding functions.
\[\limit_{x\rightarrow0}x^2 = \limit_{x\rightarrow0} -x^2 = 0
\Rightarrow \limit_{x\rightarrow0} x^2\sin\left(\frac{1}{x}\right) = 0\]

\subsection*{Continuity}
Continuity is a property of a function which essentially describes the 
``smoothness'' of the function. For a function \(f\) to be continuous at a 
point \(x\), the limit
\[\limit_{x\rightarrow a}f(x) = f(a)\]
Must be true; i.e. the value of \(f\) at \(x\) must be the value that \(f\) 
approaches as it becomes arbritrarily close to \(x\). As a simple example, let
us check if \(f\) is continuous at \(x = 1\).
\[f(x) = 
    \begin{cases}
        2x, x \neq 1 \\
        4, x = 1 \\
    \end{cases}
\]
\[\limit_{x\rightarrow1} f(x) = \limit_{x\rightarrow1} 2x = 2\]
\[f(1) = 4 \neq 2\]
\[\therefore f \mathrm{\:is\:not\:continuous\:at\:}1\]

\subsubsection*{Continuity Theorems}
If \(f\) and \(g\) are real valued functions and \(c\) is a constant, then
assuming \(f\) and \(g\) are continuous at \(x = a\), the following functions
are additionally continuous at \(x = a\).
\begin{itemize}
    \item \(f + g\)
    \item \(cf\)
    \item \(fg\)
    \item \(\frac{f}{g} \mathrm{\:if\:} g(a) \neq 0\)
\end{itemize}
If \(f\) is continuous at \(x = a\) and \(g\) is continuous at \(x = f(a)\),
then \(g \circ f\) is continuous at \(x = a\). Thus if two continuous functions
are composed, the resultant function will in addition be continuous.

\bigskip
All of the following function types are continuous across their domain.
\begin{itemize}
    \item Polynomials
    \item Trigonometric functions
    \item Exponential functions
    \item Logarithmic functions
    \item \(n\)th root functions
    \item Hyperbolic functions
\end{itemize}

\subsubsection*{Example}
For which values of \(x\) is \(f\) continuous?
\[f(x) = \frac{\sin(x^2 + 1)}{\log(x)}\]
We know that \(\sin\) is continuous across \(\R\), as is \(x^2 + 1\). 
\(\log(x)\) is defined for \(\R^+\). Using function composition, we
know that \(\sin(x^2 + 1)\) is continuous on \(\R\), and using division we can
see that \(f\) will be continuous for all values in the domain of \(\log(x)\) 
where \(\log(x) \neq 0\), i.e. \(\R^+ \backslash \set{1}\).

\bigskip
If \(f\) is continuous at \(b\) and \(\limit_{x\rightarrow a} g(x) = b\) then
\[\limit_{x\rightarrow a} f\left[g(x)\right] 
= f\left[\limit_{x\rightarrow a}g(x)\right] = f(b)\]

\subsubsection*{Example}
\[\limit_{x\rightarrow\infty} \sin\left(e^{-x}\right)
= \sin\left(\limit_{x\rightarrow\infty} e^{-x}\right) = \sin(0) = 0\]
We can only do this because \(\sin\) is continuous on \(\R\).

\subsubsection*{Derivatives}
The derivative is defined using a limit.
\[f^\prime(a) = \limit_{h\rightarrow0}\frac{f(a + h) - f(a)}{h}\]
If this limit exists, the function is differentiable at \(a\). Geometrically,
this implies that a tangent line can be drawn at \(a\) on the graph with 
gradient yielded by the above limit.

\begin{plot}
    \addplot[thick, red] {(x-2)^2};
    \addplot[thick, blue] {2(x-2)};
\end{plot}

If a function is differentiable at \(x = a\), the the function is continuous
at that point.

\subsubsection*{L'H\^{o}pital's Rule}
Given \(f\) and \(g\) are differentiable functions near some value \(x = a\)
and \(g^\prime(x) \neq 0\) near \(a\) but \(\neq a\). If the limit
\[\limit_{x\rightarrow a} \frac{f(x)}{g(x)}\]
Has an indeterminate form of
\[\frac{0}{0} \:\mathrm{or}\: \frac{\infty}{\infty}\]
Then L'H\^{o}pital's Rule is applicable.
\[\limit_{x\rightarrow a}\frac{f(x)}{g(x)} 
= \limit_{x\rightarrow a}\frac{f^\prime(x)}{g^\prime(x)}\]
For example, we can solve limits like the one below much more easily.
\[\limit_{x\rightarrow0}\frac{\sin(x)}{x} 
= \limit_{x\rightarrow0}\frac{\cos(x)}{1} = \frac{\cos(0)}{1} = 1\]
L'H\^{o}pital's Rule can be applied repeatedly to the same function as long
as its conditions still hold. Sometimes the form of a function must be modified
to have a quotient before the rule can be applied to it.

\subsubsection*{Rigour}
Thus far, we have used the phrase \textit{arbritrarily close} without a proper
definition of what that means. A more formal definition of this exists, where
we pick an arbritrary positive number \(\epsilon\). When we do this, there is
another positive number \(\delta\) for which \(\abs{f(x) - L} < \epsilon\) when
\(0 < \abs{x - a} < \delta\).

\subsection*{Sequences}
A sequence is a function which maps the natural numbers to \(\R\) i.e. of the
form \(f: \N \rightarrow \R\). They can be thought of as ordered lists of real
numbers, or as functions where \(f(n)\) is the \(n\)th number in the sequence.
A common notation for a sequence \(a\) is \(\set{a_n}\), where \(a_n\) is a 
function defining the \(n\)th element. \par
A sequence has the limit \(L\) if \(a_n\) can be made arbritrarily close to 
\(L\) by making \(n\) sufficiently large. To indicate this, we write
\[\limit_{n\rightarrow\infty} a_n = L\]
If a limit \(L\) exists, then the sequence converges, otherwise it diverges.
The only major difference between limits on sequences and limits on functions
is that sequences deal with discrete rather than continuous values. This 
matters significantly in cases like trigonometric functions. If a function
converges, then a sequence with that function will also converge. \par
The limit laws and Sandwhich Theorem apply to limits for sequences as well.
For the Sandwhich Theorem, the sandwhiching sequences can consider only a range
of \(n\), greater than some value \(N\).

\subsubsection*{Examples}
For each of the following sequences, determine whether they converge or 
diverge.
\begin{itemize}
    \item \(\set{\frac{1}{n}}\) converges to \(0\).
    \item \(\set{-1^{n - 1}}\) diverges, oscillating between \(1\) and \(-1\).
    \item \(\set{n}\) diverges to \(\infty\).
\end{itemize}

\subsubsection*{Standard Limits of Sequences}
A set of standard limits of sequences exist, which can be used to solve other
limits involving sequences.

\begin{formulalist}
    \(\limit_{n\rightarrow\infty}\frac{1}{n^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{n\rightarrow\infty} r^n = 0 \:\:\:(\abs{r} < 1)\) \\
    \(\limit_{n\rightarrow\infty} a^{\frac{1}{n}} = 1 \:\:\:(a > 0)\) \\
    \(\limit_{n\rightarrow\infty} n^{\frac{1}{n}} = 1\) \\
    \(\limit_{n\rightarrow\infty} \frac{a^n}{n!} = 0 \:\:\:(a \in \R)\) \\
    \(\limit_{n\rightarrow\infty} \frac{\log(n)}{n^p} = 0 \:\:\:(p > 0)\) \\
    \(\limit_{n\rightarrow\infty} \left(1 + \frac{a}{n}\right)^n 
    = e^a \:\:\:(a \in \R)\) \\
    \(\limit_{n\rightarrow\infty} \frac{n^p}{a^n} 
    = 0 \:\:\:(p \in \R, a > 1)\) \\  
\end{formulalist}

\subsubsection*{Examples}
\[\limit_{n\rightarrow\infty}\left[\left(\frac{n - 2}{n}\right)^n 
+ \frac{4n^2}{3^n}\right] = 
\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n + 
\limit_{n\rightarrow\infty} \frac{4n^2}{3^n}\]
\[\left(\frac{n - 2}{n}\right)^n = \left(1 - \frac{2}{n}\right)^n \Rightarrow
\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n = e^{-2} 
\Rightarrow\]
\[\limit_{n\rightarrow\infty} \left(\frac{n - 2}{n}\right)^n + 
\limit_{n\rightarrow\infty} \frac{4n^2}{3^n} = e^{-2} + 0 = e^{-2}\]
Using standard limits, we can easily find the solution to this fairly 
complex-looking limit. The key here is recognising that the first term can
be easily rearranged to the exponential standard limit, with the second term
fairly naturally simplifying to another standard limit.
\[a_n = \frac{3^n + 2}{4^n + 2^n}, n \geq 1\]
\[\frac{3^n + 2}{4^n + 2^n}\cdot\frac{\frac{1}{4^n}}{\frac{1}{4^n}} = 
\frac{\left(\frac{3}{4}\right)^n + \frac{2}{4^n}}{1 +
\left(\frac{1}{2}\right)^n}\]
\[\limit_{n\rightarrow\infty} a_n = \frac{0 + 0}{1 + 0} = 0\]
By dividing through by the largest term, we find out that it outweighs the 
terms in the denominator, an that the whole limit collapses to \(0\). A tool
for doing this is the \textit{order hierachy}, which indicates the growth rate
of various forms for large \(n\)
\[\log(n) \ll n^p \ll a^n \ll n!\]
A situation where the Sandwhich Theorem is very applicable is one where the 
function in question clearly falls into some well defined range. For example
\[\limit_{n\rightarrow\infty}\frac{1 + \sin^2\left(\frac{n\pi}{3}\right)}{
\sqrt{n}}\]
\[0 \leq \sin^2\left(\frac{n\pi}{3}\right) \leq 1 \Rightarrow 1 \leq 1 + 
sin^2\left(\frac{n\pi}{3}\right) \leq 2\]
\[\frac{1}{\sqrt{n}} \leq \frac{\sin^2\left(\frac{n\pi}{3}\right)}{\sqrt{n}} 
\leq \frac{2}{\sqrt{n}}\]
\[\limit_{n\rightarrow\infty}\frac{1}{\sqrt{n}} 
= \limit_{n\rightarrow\infty}\frac{1}{\sqrt{n}} = 0 \Rightarrow\]
\[\limit_{n\rightarrow\infty}\frac{1 + \sin^2\left(\frac{n\pi}{3}\right)}{
\sqrt{n}} = 0\]
Here, we attack the most complex part of the problem and construct two 
equations which elegantly border it. We then use the limits of these equations
to solve the overall limit.

\subsection*{Series}
A series arises when one attempts to sum up the values in a sequence. For
a sequence \(\set{a_n}\), if we add each \(a_n\) in order, we create another
sequence \(\set{s_n}\).
\[s_1 = a_1\]
\[s_2 = a_1 + a_2\]
\[s_3 = a_2 + a_2 + a_2\]
This \textit{sequence of partial sums} \(\set{s_n}\) may or may not converge.
In the case that it does, we describe the sum \(S\) as
\[S = \limit_{n\rightarrow\infty} s_n = \limit_{n\rightarrow\infty} 
(a_1 + a_2 + \ldots + a_n)\]
For example, we can find the sum of a sequence like so:
\[a_n = \left(\frac{1}{2}\right)^n, n\geq 1\]
\[s_1 = a_2 = \frac{1}{2}\]
\[s_2 = a_1 + a_2 = \frac{3}{4}\]
\[s_n = \frac{2^n - 1}{2^n} = 1 - \frac{1}{2^n}\]
\[\limit_{n\rightarrow\infty}s_n = \limit_{n\rightarrow\infty} 1 - 
\frac{1}{2^n} = \limit_{n\rightarrow\infty} 1 - \limit_{n\rightarrow\infty} 
\frac{1}{2^n} = 1 - 0 = 1\]
This is example is a \textit{geometric series}. More generally, a series with
terms \(a_n\) is denoted with the sum
\[s_n = \sumninf a_n\]
Where the value on the bottom (\(n = 1\)) is the starting value of \(n\) and
the value on the top is the value we limit towards. If the limit of \(s_n\)
exists, the series converges. If it does not, the series diverges. For
the sequence \(\set{n} = 1, 2, 3, 4, \ldots\) we have the series
\[\sumninf n = 1 + 2 + 3 + 4 + 5 + \ldots\]
Because the sequence and series both diverge to infinity, the series diverges.
Decimal representations of numbers can be thought of as a series. If we take
\[\set{\frac{1}{10^n}} = 0.1, 0.01, 0.001, \ldots\]
\[\sumninf \frac{1}{10^n} = 0.1 + 0.01 + 0.001 + \ldots = 
0.1111\ldots\]
A general case exists, for a number \(x \in (0, 1)\) with the decimal digits
\(d_1, d_2, \ldots\) then
\[x = 0.d_1d_2d_3\ldots = \sumninf \frac{d_n}{10^n}\]
It is important to note that a sequence differs from a sequence of partial sums
differs from a series. The sequence of partial sums represents a \textit{finite
approximation} of the sum of the sequence.

\subsubsection*{Properties of Series}
If \(\sumninf a_n\) and \(\sumninf b_n\) are converging series, and 
\(c\in\R\backslash\set{0}\) is a constant then

\begin{itemize}
    \item \(\sumninf (a_n + b_n) = \sumninf a_n + \sumninf b_n\)also converges.
    \item \(\sumninf (ca_n) = c\sumninf a_n\) and still converges. If 
        \(\sumninf a_n\) diverges, then \(\sumninf (ca_n)\) also diverges.
\end{itemize}

\subsubsection*{Geometric Series}
A geometric series has the form
\[\sumninf[0] ar^n = \sumninf ar^{n - 1} = a + ar + ar^2 + ar^3 + \ldots\]
Where \(a\in\R\backslash\set{0}\) and \(r\in\R\). If \(\abs{r} < 1\), the
series will converge while if \(\abs{r} \geq 1\) the series will diverge.
In the case that \(\abs{r} < 1\), we have that
\[\sumninf[0]ar^n = \frac{a}{1 - r}\]
In addition, there is a general form for a geometric series (though it is 
somewhat less elegant)
\[\sum_{k = 0}^n ar^k = \frac{a\left(1 - r^{n + 1}\right)}{1 - r}\]
Though it must be noted that this formula is only valid for \(r\neq1\).

\subsubsection*{Harmonic \(p\) Series}
A harmonic \(p\) series has the form
\[\sumninf\frac{1}{n^p}\]
These series converge if \(p > 1\) and diverge if \(p \leq 1\). For instance
\[\sumninf \frac{1}{n^2}\]
Converges while
\[\sumninf \frac{1}{n}\]
Diverges. \par
It is worth noting that thanks to the previously explored properties of series,
we know that any multiple of a harmonic \(p\) series has the same divergence
behaviour as it's base.

\subsubsection*{Divergence Test}
A test exists to determine whether a series diverges. If the limit of the 
sequence the series is based on is not zero, then the series diverges. More
precisely
\[\limit_{n\rightarrow\infty} a_n \neq 0 \Rightarrow \sumninf a_n 
\:\mathrm{diverges}\]
If the limit is exactly \(0\), the series may converge or diverge and another
test must be used to identify convergence or divergence.

\subsubsection*{Comparison Test}
For two positive series (all elements non-negative) of the forms
\[\sumninf a_n \mand \sumninf b_n\]
\begin{itemize}
    \item if \(a_n \leq b_n\) for all \(n\) and \(\sumninf b_n\) converges, 
        then \(\sumninf a_n\) converges.
    \item if \(a_n \geq b_n\) for all \(n\) and \(\sumninf b_n\) diverges,
        then \(\sumninf a_n\) diverges.
\end{itemize}
In general, we compare a given unknown series (\(a\) above) to a harmonic \(p\)
series or a geometric series, because we know the behaviour of these in some 
detail. As an example, we can try and identify the convergence or divergence
of the following series.
\[\sumninf \frac{3 + \frac{5}{n}}{2n^2 + n + 2}\]
\[\frac{3 + \frac{5}{n}}{2n^2 + n + 2} \approx \frac{3}{2n^2}\]
By identifying the most significant terms in the denominator and the numerator,
we have identified that we expect the series to converge.
\[\frac{3 + \frac{5}{n}}{2n^2 + n + 2} \leq \frac{8}{2n^2} = \frac{4}{n^2}\]
Here, we wanted to simplify the denominator into something that we recognise
and can deal with while making it larger. To do that, we would ideally like
to reduce the term to have only a single term in both the numerator and 
denominator. To simplify and enlarge the denominator, we can simply remove the
\(n\) denominator of the \(5\), as we know \(n \geq 1\). Removing terms from 
the denominator inherently makes the fraction larger, so we can just remove the
\(n + 2\) term, rearranging into a harmonic \(p\) series.
\[\sumninf \frac{4}{n^2} \:\mathrm{converges} \Rightarrow \sumninf \frac{3 + 
\frac{5}{n}}{2n^2 + n + 2} \:\mathrm{converges}\]

\subsubsection*{Ratio Test}
For a positive term series of the form
\[\sumninf a_n\]
With the limit
\[L = \limit_{n\rightarrow\infty} \frac{a_{n + 1}}{a_n}\]
Then, this series has the following properties.
\begin{itemize}
    \item If \(L < 1\), the series converges
    \item If \(L > 1\), the series diverges
    \item If \(L = 1\), the ratio test is inconclusive.
\end{itemize}
The ratio test is most useful in situations where \(a_n\) contains an 
exponential or factorial function of \(n\). For instance, we can figure out
whether the following series converges or diverges.
\[\sumninf \frac{10^n}{n!}\]
\[\limit_{n\rightarrow\infty} \frac{10^{n + 1}}{(n + 1)!}\div\frac{10^n}{n!} = 
\limit_{n\rightarrow\infty} \frac{10^{n + 1}}{(n + 1)!}\times\frac{n!}{10^n}
= \limit_{n\rightarrow\infty} \frac{10^1}{1} \times \frac{n!}{(n + 1)!}\]
\[ = \limit_{n\rightarrow\infty} \frac{10}{n + 1} = 0 \Rightarrow 
\sumninf \frac{10^n}{n!}\:\mathrm{converges}\]

\section*{Hyperbolic Functions}

To deal with hyperbolic functions, it can help to revise a few function 
properties. Even functions have the property that
\[f(-x) = f(x)\]
For example, \(x^2\) is an even function, as is \(cos(x)\). An odd function
has the property that
\[f(-x) = -f(x)\]
They have a kind of rotational symmetry, where a rotation by \(\pi\) doesn't
alter the graph. \(sin(x)\) and \(x^3\) are odd functions.

\bigskip
The first hyperbolic function is the hyperbolic cosine function, \(\cosh\), 
often pronounced as it's written (``cosh'').
\[\cosh(x) = \frac{1}{2}(e^x + e^{-x})\]
\(\cosh\) is an even function. At \(0\), it is equal to one, and the two sides
rise evenly on each side. It essentially looks like a parabola. 
The plot of \(\cosh\) is below.

\begin{plot}[
    xmin = -3,
    xmax = 3,
    ymin = -1,
    ymax = 6
]
    \addplot[thick, blue] {cosh(x)} 
    node[right, pos = 0.6, xshift = 0.2cm] {\(\cosh(x)\)};
\end{plot}

The second hyperbolic function is hyperbolic sine, \(\sinh\), often read as
``shine''. It is defined as
\[\sinh(x) = \frac{1}{2}(e^x - e^{-x})\]
\(\sinh\) has a \(x\)-intercept at \(0\). It is an odd function. The plot of
\(\sinh\) is below.

\begin{plot}[
    xmin = -3,
    xmax = 3,
    ymin = -6,
    ymax = 6
]
    \addplot[thick, blue] {sinh(x)} 
    node[right, pos = 0.6, xshift = 0.2cm] {\(\sinh(x)\)};
\end{plot}

To complete the set, we have hyperbolic tangent, \(\tanh\), often read as 
``than''. It is defined as the quotient of \(\sinh\) on \(\cosh\), i.e.
\[\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]
\(\tanh\) has the range \((-1, 1)\), with an \(x\)-intercept at \(0\). It is an
odd function, due to the presence of \(\cosh\) in it's definition. The plot of
\(\tanh\) is below.

\begin{plot}[
    xmin = -5,
    xmax = 5,
    ymin = -2,
    ymax = 2
]
    \addplot[thick, red] {tanh(x)}
    node[above, pos = 0.8] {\(\tanh(x)\)};
\end{plot}

The trigonometric functions have the trigonometric identity
\[\cos^2(\theta) + \sin^2(\theta) = 1\]
Which mirrors the form of the equation of a circle, \(x^2 + y^2 = r\). This
tells us that a \((\sin(\theta), \cos(\theta))\) pair encodes a position on
the unit circle. \par
The hyperbolic functions mirror this behaviour for a hyperbola 
(\(x^2 - y^2 = 1)\)), having the property that \(\cosh^2(t) - \sinh^2(t) = 1\).
i.e. for a pair \((\cosh(t), \sinh(t))\), a point on the hyperbola 
\(x^2 - y^2 = 1\) is uniquely encoded. Only points on the right hand side of 
the hyperbola are encoded in this way.

\begin{plot}[
    xmin = 0,
    xmax = 3,
    ymin = -3,
    ymax = 3
]
    \addplot[thick, red] {sqrt(x^2 - 1)};
    \addplot[thick, red] {-sqrt(x^2 - 1)};
    \node[above left, red] at (axis cs: 2, 2) {\(x^2 + y^2 = 1\)};

    \node[circle, fill, inner sep = 3pt] at (axis cs: 1, 0) {};
    \node[above left, yshift = 0.2cm] at (axis cs: 1, 0) 
    {\(\cosh(0), \sinh(0)\)};
\end{plot}

\subsubsection*{Hyperbolic Identities}

We can use the relationship between \(\cosh\) and \(\sinh\) to solve equations.
For example, in the following case we can find \(\sinh\) and \(\tanh\) from the
value of \(\cosh\) and the fact that \(x < 0\).
\[\cosh(x) = \frac{13}{12}\]
\[\cosh^2(x) - \sinh^2(x) = 1 \Rightarrow \sinh^2(x) = 
\left(\frac{13}{12}\right)^2 - 1 \Rightarrow \]
\[\sinh(x) = \sqrt{\frac{169}{144} - 1} = \sqrt{\frac{25}{144}} 
= \pm \frac{5}{12}\]
\[x < 0 \Rightarrow \sinh(x) = -\frac{5}{12}\]
\[\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{-\frac{5}{12}}{\frac{13}{12}} 
= -\frac{5}{13}\]

The key identity that allowed us to solve this was the fundamental hyperbolic
identity, that of

\begin{formulalist}
    \(\cosh^2(x) - \sinh^2(x) = 1\) \\
\end{formulalist}

A variety of other more specialised identities also exist, beginning with 
the addition formulae.

\begin{formulalist}
    \(\sinh(x + y) = \sinh(x)\cosh(y) + \cosh(x)\sinh(y)\) \\
    \(\cosh(x + y) = \cosh(x)\cosh(y) + \sinh(x)\sinh(y)\) \\
    \(\sinh(x - y) = \sinh(x)\cosh(y) - \cosh(x)\sinh(y)\) \\
    \(\sinh(x + y) = \cosh(x)\cosh(y) - \sinh(x)\sinh(y)\) \\
\end{formulalist}

We also have the double angle formulae, just as with trigonometric functions.

\begin{formulalist}
    \(\sinh(2x) = 2\sinh(x)\cosh(x)\) \\
    \(\cosh(2x) = \cosh^2(x) + \sinh^2(x)\) \\
    \(\cosh(2x) = 2\cosh^2(x) - 1\) \\
    \(\cosh(2x) = 2\sinh^2(x) + 1\) \\
\end{formulalist}

\subsubsection*{Reciprocal Hyperbolic Functions}

Three reciprocal hyperbolic functions exist. These are
\[\sech(x) = \frac{1}{\cosh(x)}\]
\[\cosech(x) = \frac{1}{\sinh}(x)\]
\[\coth(x) = \frac{1}{\tanh(x)} = \frac{\cosh(x)}{\sinh(x)}\]
The plots of these functions are below.

\FloatBarrier
\begin{figure}[h!]
    \makebox[\textwidth]{
        \hspace{-2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, blue] {1/cosh(x)}
                node[above right, pos = 0.6] {\(\sech(x)\)};
            \end{smallplot}
        \end{minipage}
        \hspace{2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, blue] {1/sinh(x)}
                node[above right, pos = 0.8] {\(\cosech(x)\)};
            \end{smallplot}
        \end{minipage}
        \hspace{2.5cm}
        \begin{minipage}{.3\textwidth}
            \begin{smallplot}
                \addplot[thick, red] {1/tanh(x)}
                node[above right, pos = 0.8] {\(\coth(x)\)};
            \end{smallplot}
        \end{minipage}
    }
\end{figure}
\FloatBarrier

These functions have the associated identities

\begin{formulalist}
    \(\cosh^2(x) - \sinh^2(x) = 1\) \\
    \(\coth^2(x) - 1 = \cosech^2(x)\) \\
    \(1 - \tanh^2(x) = \sech^2(x)\) \\
\end{formulalist}

\subsubsection*{Derivatives of Hyperbolic Functions}

\begin{formulalist}
    \(\derivx{\cosh(x)} = \sinh(x)\) \\
    \(\derivx{\sinh(x)} = \cosh(x)\) \\
    \(\derivx{\tanh(x)} = \sech^2(x)\) \\
    \(\derivx{\sech(x)} = -\sech(x)\tanh(x)\) \\
    \(\derivx{\cosech(x)} = -\cosech(x)\coth(x) \:(x \neq 0)\) \\
    \(\derivx{\coth(x)} = -\cosech^2(x) \:(x \neq 0\) \\ 
\end{formulalist}

These derivatives bear some resemblance to trigonometric derivatives,
however it is worth noting that no negative signs appear when taking
derivatives of \(\sinh\) and \(\cosh\) as they do with trigonometric
functions.

\subsubsection*{Inverse Hyperbolic Functions}

Finally, inverses of some of the hyperbolic functions exist. For
\(\sinh(x)\), we have the inverse function \(\arcsinh\). This function can
also be written in terms of a logarithm, which makes sense as it is the inverse
of an exponential function.
\[\arcsinh = \log(x + \sqrt{x^2 + 1})\]
Because \(\sinh\) is a nice one-to-one function, it has a simple inversion 
function, pictured below.

\begin{plot}
    \addplot[thick, blue, dashed, <->] {sinh(x)}
    node[left, pos = 0.3] {\(\sinh(x)\)};
    \addplot[thick, blue, <->] {ln(x + sqrt(x^2 + 1))}
    node[above left, pos = 0.9] {\(\arcsinh(x)\)};
\end{plot}

For \(\cosh\), the domain of \(\cosh\) must be restricted to yield a one-to-one
function. The domain is therefore restricted to \([0, \infty)\). Because the 
range of \(\cosh\) is \([1, \infty)\), this is the domain of \(\arccosh\).
\[\arccosh = \log(x + \sqrt{x^2 - 1}), \:(x \geq 1)\]
The plot of \(\arccosh\) is below.

\begin{plot}
    \addplot[thick, blue, dashed, <->] {cosh(x)}
    node[left, pos = 0.4, xshift = -0.2cm] {\(\cosh(x)\)};
    \addplot[thick, blue, ->] {ln(x + sqrt(x^2 - 1))}
    node[below right, pos = 0.6] {\(\arccosh(x)\)};
\end{plot}

This means that \(\arccosh(\cosh(x)) = \abs{x}\) rather than \(x\). The final
inverse is \(\arctanh\). As \(\tanh\) is one-to-one, this function is a perfect
inverse.
\[\frac{1}{2}\log\left(\frac{1 + x}{1 - x}\right), \:(-1 < x < 1)\]
The range of \(\tanh\) because the domain of \(\arctanh\), so \(\arctanh\)
accepts values in the range \((-1, 1)\) and spits out values in \(\R\).

\begin{plot}
    \addplot[thick, red, dashed, <->] {tanh(x)}
    node[above, pos = 0.8] {\(\tanh(x)\)};
    \addplot[thick, red, <->] {0.5*ln((1 + x)/(1 - x))}
    node[left, pos = 0.1] {\(\arctanh(x)\)};
\end{plot}

The inverse reciprocal hyperbolic functions are also defined, though they are 
seldom used. The formulas for the inverse functions are obtained in the usual
way of obtaining inverse functions, by taking \(y = \arcsinh(x)\) and then 
solving for \(x\) in terms of \(y\).

\subsubsection*{Derivatives of Inverse Hyperbolic Functions}

\begin{formulalist}
    \(\derivx{\arcsinh(x)} = \frac{1}{\sqrt{x^2 + 1}}\) \\
    \(\derivx{\arccosh(x)} = \frac{1}{\sqrt{x^2 - 1}}, \:(x > 1)\) \\
    \(\derivx{\arctanh(x)} = \frac{1}{1 - x^2}, \:(-1 < x < 1)\) \\
\end{formulalist}

\section*{Complex Numbers}

Complex numbers extend the natural, integer, rational and real numbers. By 
introducing the number \(i\) with the property that \(i^2 = -1\), we gain
access to another set of numbers of the form
\[\C = \set{x + yi | x, y \in \R}\]
This allows us to do some powerful things, like to solve any polynomial
function with complex coefficients. Their basic operations follow. The most
basic is of course addition, where one just adds the relevant terms.
\[(2 + 3i) + (4 + i) = 6 + 4i\]
Multiplication is performed piecewise in F.O.I.L. style.
\[(1 + i)(2 + i) = 2 + i + 2i + i^2 = 1 + 3i\]
The complex conjugate of a complex number is found by multiplying the imaginary
part (the multiple of \(i\)) by \(-1\).
\[z = x + iy \Rightarrow \bar{z} = x - iy\]
The modulus of a complex number can be thought of as the vector norm of the
number and is calculated as
\[z = x + iy \Rightarrow \abs{z} = \sqrt{x^2 + y^2}\]
The argument of a complex number is the angle it forms with the positive real
axis (usually the \(x\)-axis). This argument \(\theta\) has the property that
\[\tan(\theta) = \frac{y}{x}\]
Using this argument we can write a complex number in trigonometric polar form,
so where \(r\) is the norm of the complex number it can be written as
\[z = r(\cos(\theta) + i\sin(\theta))\]
The argument \(\theta\) is non-unique, and so the concept of the principal 
argument as the single possible \(\theta\) which lies in the interval 
\((-\pi, \pi]\) is called the \textit{principal argument} and is unique.

\subsection*{The Complex Exponential}

The complex exponential is an alternate way of representing a complex number.
According to Euler's formula, a complex number has the following property.
\[re^{i\theta} = r(\cos(\theta) + i\sin(\theta))\]
According to this formula, we can see Euler's identity.
\[e^{i\pi} = -1\]
Multiplication of the complex exponential results in addition of angles.
\[r_1e^{i\theta}r_2e^{i\phi} = r_1r_2e^{i(\theta + \phi)}\]
Likewise division results in subtraction.
\[\frac{z}{w} = \frac{r_1}{r_2}e^{i(\theta - \phi)}\]
To raise a complex exponential to a positive power, we can use De Moivre's
theorem, which states.
\[z^n = \left(re^{i\theta}\right)^n = r^ne^{in\theta}\]

\subsubsection*{Exponential Form of \(\sin\) and \(\cos\)}

The real and imaginary parts of complex numbers have certain interesting 
interactions with the conjugate operation.
\[z + \bar{z} = (x + iy) + (x - iy) = 2x = 2 \mathrm{Re}(z) \Rightarrow 
\mathrm{Re}(z) = \frac{1}{2}(z + \bar{z})\]
\[z - \bar{z} = (x + iy) - (x - iy) = 2iy = 2i\mathrm{Im}(z) \Rightarrow
\mathrm{Im}(z) = \frac{1}{2i}(z - \bar{z})\]
Applying this to the exponential form, we can find some interesting 
representations of \(\sin\) and \(\cos\)
\[e^{i\theta} = \cos(\theta) + i\sin(\theta) \Rightarrow \cos(-\theta) + 
i\sin(\theta) \Rightarrow e^{-i\theta} = \cos(\theta) - i\sin(\theta)\]
This tells us that
\[e^{i\theta} + e^{-i\theta} = 2\cos(\theta) \Rightarrow \cos(\theta) = 
\frac{1}{2}\left(e^{i\theta} + e^{-i\theta}\right)\]
Applying the same working to \(\sin\) we find
\[\sin(\theta) = \frac{1}{2i}\left(e^{i\theta} - e^{-i\theta}\right)\]
Referring back to our imaginary and real part representations earlier, we
see that this tells us what we already knew in a different way; \(\cos\) is
the real part of a complex number and \(\sin\) is the imaginary part. \par
These formulas bear an interesting resemblance to the hyperbolic functions
we explored earlier. Plugging \(i\theta\) into \(\cosh\) and \(\sinh\) one 
finds
\[\cosh(i\theta) = \frac{1}{2}\left(e^{i\theta} + e^{-i\theta}\right) =
\cos(\theta)\]
\[\sinh(i\theta) = \frac{1}{2}\left(e^{i\theta} - e^{-i\theta}\right) =
\sin(\theta)\]

\subsubsection*{Differentiation via the Complex Exponential}

For a complex number \(z = x + iy, x, y \in \R\), we have
\[e^z = e^{x + iy} = e^xe^{iy} = e^x(\cos(y) + i\sin(y))\]
To take a derivative of a complex number, we take the derivative of real parts
and imaginary parts seperately. For example.
\[\derivx{\cos(x) + (x^2 + 1)i} = -\sin(x) + 2xi\]
For a complex exponential function of the form \(e^{zx}\) where \(z\) the 
derivative is
\[\derivx{e^{zx}} = ze^{zx}\]
This can be proven by splitting the function as follows
\[e^{(a + ib)x} = e^{ax}e^{ibx} = e^{ax}(\cos(bx) + i\sin(bx))\]
And using the product rule to solve the derivative. This can be used to solve
some fairly complex derivatives quite efficiently.
\[\frac{\mathrm{d}^{56}}{\mathrm{d}t^{56}}\left[e^{-t}\cos(t)\right]\]
\[e^{-t}\cos(t) = e^{-t}\mathrm{Re}\left(e^{it}\right) = 
\mathrm{Re}\left(e^{-t}e^{it}\right) = \mathrm{Re}\left(e^{(-1 + i)t}\right)\]
\[\frac{\mathrm{d}^{56}}{\mathrm{d}t^{56}}\left[\mathrm{Re}
\left(e^{(-1 + i)t}\right)\right] = 
\mathrm{Re}\left((-1 + i)^{56}e^{(-1 + i)t}\right)\]
\[\left(-1 + i\right)^{56} = \left(\sqrt{2}e^{\frac{3\pi}{4}i}\right)^{56}
= \sqrt{2}^{56}e^{56\frac{3\pi}{4}i} = 2^{28}e^{0} = 2^{28}\]
\[\frac{\mathrm{d}^{56}}{\mathrm{d}t^{56}}\left[e^{-t}\cos(t)\right] = 
\mathrm{Re}\left(2^{28}e^{-(1 + i)t}\right) = 
\mathrm{Re}\left(2^{28}e^{-t}(\cos(t) + i\sin(t))\right) \]
\[= 2^{28}e^{-t}\cos(t)\]
Here, we used the fact that taking a complex derivative simply entails doing
the real and imaginary parts seperately, in combination with the fact that the
form of the original term was the real part of a complex number to perform the
derivative much more rapidly than would ordinarily be possible. \par
Knowledge of this derivative makes simple the inverse; it's integral.
\[\int e^{kx} \dx = \frac{1}{k}e^{kx} + D\]
Where \(k\) is a complex number and \(x\) is the real input to the function.
\(D\) may be a complex number. Like in the previous example, the real and 
imaginary parts of complex numbers can be considered distinctly.

\section*{Integral Calculus}

\subsubsection*{Derivative Substitution}

For a function \(F^\prime(x)\) we may know \(f(x) = F^\prime(x)\). We then know
that
\[\int f(x) \dx = F(t) + C\]
Applying the chain rule, we also know that
\[\derivx{F(g(x))} = F^\prime(g(x))g^\prime(x)\]
In reverse, this means that
\[\int f(g(x))g^\prime(x) \dx = F(g(x)) + C\]
\[\int f(u)\dd{u}{x}\dx = F(u) + c\]
This fact is known as \textit{derivative substitution}, and is a useful
technique for calculating integrals. The general process for using derivative
substitution is
\[\int f(g(x))g^\prime(x)\dx = \int f(u)\dd{u}{x}\dx 
= \int f(u) \:\mathrm{d}u\]
An example
\[\int (6x^2 + 10)\sinh(x^3 + 5x - 2)\dx\]
\[u = x^3 + 5x - 2\mcom u^\prime = 3x^2 + 5\]
\[\int 2(3x^2 + 5)\sinh(x^3 + 5x - 2)\dx = \int 2u^\prime\sinh(x)\dx
= 2\int\sinh(u) \:\mathrm{d}u = \]
\[2\cosh(u) + C = 2\cosh(x^3 + 5x - 2) + C\]
So with the key observation that \(6x^2 + 10\) is twice the derivative of the
\(\sinh\) term, we can rewrite the integal in terms of \(u\) and integrate the
simpler resultant function using a standard integral. Another example
\[\int\frac{\sech^2(3x)}{10 + 2\tanh(3x)}\dx\]
\[u = 10 + 2\tanh(3x)\mcom u^\prime = 12\sech^2(3x)\]
\[\int\frac{\sech^2(3x)}{10 + 2\tanh(3x)}\dx 
= \frac{1}{12}\int\frac{u^\prime}{u}\dx = \frac{1}{12}\int\frac{1}{u}
\:\mathrm{d}u\]
\[= \frac{1}{12}\log(\abs{u}) + C 
= \frac{1}{12}\log\left(\abs{12\sech^2(3x)}\right) + C\]

\subsubsection*{Trigonometric and Hyperbolic Substitution}

For expressions of the form
\[\sqrt{a^2 - x^2}\mcom \sqrt{a^2 + x^2}\mcom \sqrt{x^2 - a^2}\]
We can use a trick where we substitute \(x\) for a function \(g(\theta)\).
\[\int f(x)\dx = \int f(g(\theta))g^\prime(\theta) \:\mathrm{d}\theta\]
This entails replace \(x\) and then using derivative substitution to simplify
the resultant expression. The appropriate substitution in a few cases is
contained in the below table.

\renewcommand{\arraystretch}{2}
\begin{center}    
    \begin{tabular}{||c|c||}
        \(\sqrt{a^2 - x^2}\mcom \frac{1}{\sqrt{a^2 - x^2}},
        \:\:\: (a^2 - x^2)^\frac{3}{2}\mcom \mathrm{etc}\) &
        \(x = a\sin(\theta) \mor x = a\cos(\theta)\) \\
        \(\sqrt{a^2 + x^2}\mcom \frac{1}{\sqrt{a^2 + x^2}},
        \:\:\: (a^2 + x^2)^{-\frac{3}{2}}\mcom \mathrm{etc}\) &
        \(x = a\sinh(\theta)\) \\
        \(\sqrt{x^2 - a^2}\mcom \frac{1}{\sqrt{x^2 - a^2}},
        \:\:\: (x^2 - a^2)^\frac{5}{2}\mcom \mathrm{etc}\) &
        \(x = a\cosh(\theta)\) \\
        \(\frac{1}{a^2 + x^2}\mcom \frac{1}{(a^2 + x^2)^2}\mcom
        \mathrm{etc}\) &
        \(x = a\tan(\theta)\) \\
    \end{tabular}
\end{center}
\renewcommand{\arraystretch}{1}

This is useful because these functions have useful identities when squared. An
example

\[\int\frac{1}{\sqrt{x^2 - 25}}\dx\]
\[x = 5\cosh(\theta) \Rightarrow \theta = \arccosh\left(\frac{x}{5}\right)
\Rightarrow x \geq 5\]
\begin{center}
Because we are rewriting \(x\) as a hyperbolic function, we must be careful
that the values of \(x\) are valid for the function. Here, for 
\(g(\theta)\) to output \(x\), \(x\geq5\) must be true. In addition, for
the overall function to be valid we need \(\sqrt(x^2 - 25) \neq 0\), thus
\(x > 5 \Rightarrow \theta > 0\).
\[x = 5\cosh(\theta) \Rightarrow \dd{x}{\theta} = 5\sinh(\theta)\]
\[\frac{1}{\sqrt{x^2 - 25}} = \frac{1}{\sqrt{25\cosh^2(\theta) - 25}} =
\frac{1}{\sqrt{25(\cosh^2(\theta) - 1)}} = \frac{1}{5\sqrt{\sinh^2(\theta)}}\] 
\[= \frac{1}{5\abs{\sinh(\theta)}}\mcom \theta > 0 \Rightarrow
\frac{1}{5\sinh(\theta)}\]
Here, we were able to use the identity 
\(\cosh^2(\theta) - \sinh^2(\theta) = 1\) to simplify the denominator. We
then used the established fact of \(\theta > 0\) to remove the absolute
value from the denominator.
\[\int\frac{1}{\sqrt{x^2 - 25}}\dx 
= \frac{1}{5\sinh(\theta)}5\sinh(\theta)\:\mathrm{d}\theta
= \int 1 \:\mathrm{d}\theta = \theta + C 
= \arccosh\left(\frac{x}{5}\right) + C\]
Here we see the result of the derivative substitution, as we multiply by
\(5\sinh(\theta)\). We use the value of \(\theta\) we calculated earlier to
put the final answer back in terms of \(x\).        
\end{center}

Another example

\begin{center}
\[\int\sqrt{9 - 4x^2}\dx\mcom \abs{x} \leq \frac{3}{2}\]
\[\int\sqrt{9 - 4x^2}\dx = 2\int\sqrt{\frac{9}{4} - x^2}\]
\[x = \frac{3}{2}\sin^2(\theta) \Rightarrow \theta = 
\arcsin\left(\frac{2x}{3}\right)\] 
\[\frac{-\pi}{2} \leq \theta \leq \frac{\pi}{2} \Rightarrow \frac{-3}{2} \leq
x \leq \frac{3}{2}\]
\[\dd{x}{\theta} = \frac{3}{2}\cos(\theta)\]
\[\sqrt{\frac{9}{4} - x^2} = \sqrt{\frac{9}{4} - \frac{9}{4}\sin^2(\theta)} =
\sqrt{\frac{9}{4}\cos^2(\theta)} = \frac{3}{2}\abs{\cos(\theta)} 
= \frac{3}{2}\cos(\theta)\]
Because we know the range of \(\theta\) is restricted, we can know that
\(\cos\) will always be positive here, which allows us to remove the absolute
value.
\[2\int\sqrt{\frac{9}{4} - x^2} = 2\int\frac{3}{2}\cos(\theta)\cdot\frac{3}{2}
\cos(\theta)\:\mathrm{d}\theta 
= \frac{9}{2}\int\cos^2(\theta)\:\mathrm{d}\theta\]
\[\frac{9}{2}\int\cos^2(\theta)\:\mathrm{d}\theta = \frac{9}{2}\frac{1}{2}
\int\cos(2\theta) + 1\:\mathrm{d}\theta = \frac{9}{4}\frac{1}{2}\sin(2\theta)
+ \theta + C\]
\[= \frac{9}{4}\left(\sin(\theta)\cos(\theta) + \theta\right) + C = \frac{9}{4}
\left(\sin(\theta)\sqrt{1 - \sin^2(\theta)} + \theta\right) + C\]
\[= \frac{9}{4}\left(\frac{2x}{3}\sqrt{1 - \frac{4x^2}{9}} + 
\arcsin\left(\frac{2x}{3}\right)\right) + C = x\sqrt{\frac{9}{4} - x^2} 
+ \frac{9}{4}\arcsin\left(\frac{2x}{3}\right) + C\]
A lot going on here, some of which is explained by the application of a couple
of double angle formulae. The first instance is where we use the double angle
formula \(\cos(2x) = 2\cos^2(x) - 1\) to remove the \(\cos^2\) term and enable
integration. Later, the formula \(\sin(2x) = 2\sin(x)\cos(x)\) is used to
remove the \(\sin(2\theta)\) term. The \(\cos\) is then replaced according to
\(\sin^2 + \cos^2 = 1\) to enable substitution with the value for \(\theta\)
determined earlier, the \(\arcsin\) term of which annihilates with the 
\(\sin\) in the \(\sin(\theta)\) expressions to yield \(\frac{2x}{3}\).
\end{center}

\subsubsection*{Integration of Powers of Hyperbolic Functions}

\[\int \sinh^m(x)\cosh^n(x)\dx\]
Consider the above integral with two integer powers of \(\sinh\) and \(\cosh\).
There are two processes for this; one for the case of one power being odd and
another for the case where both are even. First, an example of an even case.
\[\int \cosh^4(\theta) \:\mathrm{d}\theta\]
\[\cosh^4(\theta) = \left(\cosh^2(\theta)\right)^2 
= \left(\frac{\cosh(2\theta) + 1}{2}\right)^2 = \]
\[\frac{1}{4}\left(\cosh^2(2\theta) + 2\cosh(2\theta) + 1\right) 
= \frac{1}{4}\left(\frac{\cosh(4\theta) + 1}{2} + 2\cosh(2\theta) + 1\right)
\Rightarrow \]
\[\int \cosh^4(\theta) \:\mathrm{d}\theta = \frac{1}{4}\int\frac{1}{2}
\cosh(4\theta) + 2\cosh(2\theta) + \frac{3}{2}  \:\mathrm{d}\theta\]
\[= \frac{1}{4}\left(\frac{1}{8}\sinh(4\theta) + \sinh(2\theta) 
+ \frac{3}{2}\theta\right) + C\]
Here, we rearranged an integral we didn't know (\(\cosh^4\)) to one that we did
know (\(\cosh\)) through repeated application of the double angle formula
\[\cosh^2(\theta) = \frac{\cosh(2\theta) + 1}{2}\]
For the odd power case, the approach is a little different.
\[\int\sinh^5(x)\cosh^6(x)\dx\]
\[\int\sinh\sinh^4(x)\cosh^6(x)\dx = \int\sinh\left(\sinh^2(x)\right)^2
\cosh^6(x)\dx = \]
\[\int\sinh(x)\left(\cosh^2(x) - 1\right)^2\cosh^6(x)\dx\]
\[u = \cosh(x) \Rightarrow \dd{u}{x} = \sinh(x)\]
\[\int\sinh(x)\left(\cosh^2(x) - 1\right)^2\cosh^6(x)\dx 
= \int(u^2 - 1)^2u^6\dd{u}{x} \:\mathrm{d}x = \]
\[\int(u^4 - 2u^2 + 1)u^6 \:\mathrm{d}x = \int u^{10} - 2u^8 + u^6
\:\mathrm{d}x = u^{11} - \frac{2u^9}{9} + \frac{u^7}{7} + C \Rightarrow\]
\[\int\sinh^5(x)\cosh^6(x)\dx = \frac{\cosh^11(x)}{11} 
- \frac{2\cosh^9(x)}{9} + \frac{\cosh^7(x)}{7} + C\]
Here, we took aside one \(\sinh\) as our sacrifice to the integral gods and
used it to change variable of integration from \(x\) to \(u\). We did this by
using the hyperbolic identity \(\cosh^2(x) - \sinh^2(x) = 1\) to convert all of
the other \(\sinh\) terms into \(\cosh\) terms and then substituting \(u\) for
\(\cosh(x)\).

\subsubsection*{Integration by Parts}

The product rule used during differentiation is
\[\derivx{uv} = \dd{u}{x}v + u\dd{v}{x}\]
To perform integration by parts, this experssion is integrated to yield the
rule
\[\int u\dd{v}{x} \dx = uv - \int v\dd{u}{x} \dx\]
An example
\[\int x^2\log(x)\dx\mcom (x > 0)\]
\[u = \log(x) \Rightarrow \dd{u}{x} = \frac{1}{x}\]
\[\dd{v}{x} = x^2 \Rightarrow v = \frac{x^3}{3}\]
\[\int x^2\log(x) \dx = \frac{x^3}{3}\log(x) -\int\frac{x^3}{3}\frac{1}{x}\dx\]
\[= \frac{x^3\log(x)}{3} - \frac{x^3}{9} + C\]
This can also be used cleverly in some settings to rearrange integrals one
doesn't know how to solve.
\[\int\log(x)\dx\]
\[u = \log(x) \Rightarrow \dd{u}{x} = \frac{1}{x}\]
\[\dd{v}{x} = 1 \Rightarrow v = x\]
\[\int\log(x)\dx = x\log(x) - \int \frac{x}{x} \dx = x\log(x) - \int 1 
= x\log(x) - x + C\]

\subsubsection*{Partial Fractions}

For two polynomials \(f(x)\), of degree \(n\) and \(g(x)\), of degree \(d\),
\[\frac{f(x)}{g(x)}\]
can be written as the sum of partial fractions. In the case that \(n < d\), the
process for this is to factorise \(g\), write down the partial fraction
expansion and finally to find unknown coefficients. To write down the partial
fraction expansion for a denominator \(g\), we use the expansions from the
below table.

\renewcommand{\arraystretch}{2}
\begin{center}    
    \begin{tabular}{||c|c||}
        Denominator Factor & Partial Fraction Expansion \\
        \hline
        \hline
        \((x - a)\) & \(\frac{A}{x - a}\) \\
        \((x - a)^r\) & \(\frac{A_1}{x - a} + \frac{A_2}{(x - a)^2} + \ldots +
        \frac{A_r}{(x - a)^r}\) \\
        \(x^2 + bx + c\) & \(\frac{Ax + B}{x^2 + bx + c}\) \\
        \((x^2 + bx + c)^r\) & \(\frac{A_1x + B_1}{x^2 + bx + c} +
        \frac{A_2x + B_2}{(x^2 + bx + c)^2} + \ldots 
        + \frac{A_rx + B_r}{(x^2 + bx + c)^r}\) \\
    \end{tabular}
\end{center}
\renewcommand{\arraystretch}{1}

As an example of using this process, let us attempt to integrate.
\[\int \frac{4}{x^2(x + 2)} \dx\mcom (x\neq0, -2)\]
\[\frac{4}{x^2(x + 2)} = \frac{A}{x} + \frac{B}{x^2} + \frac{C}{x + 2} = \]
\[\frac{Ax(x + 2) + B(x + 2) + Cx^2}{x^2(x + 2)}
= \frac{(A + C)x^2 + (2A + B)x + 2B}{x^2(x + 2)} \Rightarrow\]
\[A + C = 0 \mcom 2A + B = 0 \mcom 4 = 2B \Rightarrow\]
\[A = -1 \mcom B = 2 \mcom C = 1 \Rightarrow\]
\[\int \frac{4}{x^2(x + 2)} \dx = \int \frac{-1}{x} + \frac{2}{x^2} + 
\frac{1}{x + 2} \dx = \log(\abs{x}) - \frac{2}{x} + \log(\abs{x + 2}) + C\]

In the case that \(n \geq d\), we must first use polynomial long division to
reduce the degree of the denominator.

\section*{Differential Equations}

\subsection*{Ordinary Differential Equations}

An ordinary differential equation or ODE is an equation of the form
\[f\left(x, y \dd{y}{x} \dd{^2y}{x^2}, \ldots \dd{^ny}{x^n}\right) = 0\]
Here, there is only one independent variable \(x\), and one dependent variable
\(y\). The equation is then expressed in terms of functions and derivatives of
these two functions. The \textit{order} of a differential equation is the
highest derivative that appears in the equation (second, third, etc). \par
A solution of this equation is a function \(y(x)\) which satisfies the equation
for all \(x\) in some interval. For example
\[y(x) = x^2 + \frac{2}{x}\]
Is a solution for all \(x\in\R\backslash\set{0}\) to
\[\dd{y}{x} + \frac{y}{x} = 3x\]
Which can be shown via substitution, replacing all \(y\) with \(y(x)\).
\[\derivx{x^2 + \frac{2}{x}} + \frac{x^2 + \frac{2}{x}}{x} = 3x\]
\[2x - \frac{2}{x^2} + x + \frac{2}{x^2} = 3x\]
\[2x + x = 3x\]

\subsubsection*{First Order}

A first order differential equation is a differential equation containing only
at most first derivatives such as
\[\dd{y}{x} = x^3\]
This example can be easily solved through integration.
\[y = \int x^3 \dx = \frac{x^4}{4} + c\]
These solutions usually contain an arbritrary constant \(c\in\R\), and the
solutions represented by various values of \(c\) are known as the family of
solutions to this differential equation. It is interesting to note that these
solutions can never intersect, as the differ by some value at all \(x\). \par
To find a \textit{particular solution}, we need some starting conditions for
our differential equation, and we can then solve to find the value of \(c\).
If for instance we knew that our previous example had \(y(0) = 2\) we could
solve to find
\[y(0) = \frac{0^4}{4} + c = 2 \Rightarrow c = 2\]

\subsubsection*{Seperable First Order}

A seperable first order differential equation has the form
\[\dd{y}{x} = M(x)N(y) \mcom (M(x)\neq0, N(y)\neq0)\]
To solve, we seperate our variables like so
\[\dd{y}{x} = M(x)N(y) \Rightarrow \frac{1}{N(y)}\dd{y}{x} 
= M(x)\mcom N(Y\neq0) \Rightarrow\]
\[\int \frac{1}{N(y)}\dd{y}{x}\dx = \int M(x)\dx \Rightarrow 
\int \frac{1}{N(y)}\dy = \int M(x)\dx\]
This is essentially an application of derivative substitution to clean up the
equation and seperate the two variables.

\subsubsection*{Examples}
\[\dd{y}{x} = \frac{y}{1 + x}\mcom(x\neq -1)\]
\[\frac{1}{y}\dd{y}{x} = \frac{1}{1 + x} \Rightarrow
\int\frac{1}{y}\dd{y}{x}\dx = \int\frac{1}{1 + x}\dx = \int\frac{1}{y}\dy
\Rightarrow\]
\[\log(\abs{1 + x}) + c = \log(\abs{y}) + c \Rightarrow 
e^{\log(\abs{1 + x}) + c} = e^{\log(\abs{y})} \Rightarrow\]
\[\abs{y} = e^c\abs{1 + x} \Rightarrow y = \pm A(1 + x)\]
In this example we made the assumption that \(y\) was non-zero when we divided
by \(y\) initially. This works out in the end because we know\(A=\pm e^c\neq0\)
so our solution covers us, however we should also check whether \(y(x)\equiv0\)
is a solution, which in this case it is. We can incorporate this into our
previous solution by allowing \(A\) to be \(0\).
\[\dd{y}{x} = \frac{1}{2y\sqrt{1 - x^2}}\mcom(-1 < x < 1,y\neq 0)\mcom y(0)=3\]
\[2y\dd{y}{x} = \frac{1}{\sqrt{1 - x^2}} \Rightarrow\int2y\dy 
= \int\frac{1}{\sqrt{1 - x^2}}\dx \Rightarrow\]
\[x = \cos(\theta) \Rightarrow \theta = \arccos(x)\]
\[\dd{x}{\theta} = -\sin(\theta) \Rightarrow \int\frac{1}{\sqrt{1 - x^2}}\dx
= \int\frac{1}{\sqrt{1-\cos^2(\theta)}}\times-\sin(\theta)\:\mathrm{d}\theta\]
\[\frac{1}{\sqrt{1 - \cos^2(\theta)}} = \frac{1}{\sqrt{\sin^2(\theta)}} 
= \frac{1}{\sin(\theta)} \Rightarrow\]
\[\int2y\dy = \int\frac{1}{\sin(\theta)}\times-\sin(\theta)\:\mathrm{d}\theta
= \int -\frac{\sin(\theta)}{\sin(\theta)}\:\mathrm{d}\theta 
= \int-1\:\mathrm{d}\theta \Rightarrow\]
\[y^2 = -\theta + c = -\arccos(x) + c \Rightarrow \abs{y} 
= \sqrt{-\arccos(x) + c} \Rightarrow\]
\[y = \pm\sqrt{-\arccos(x) + c}\]
\[y(0) = 3 \Rightarrow 3 = \pm\sqrt{-\arccos(0) + c} 
= \pm\sqrt{-\frac{\pi}{2} + c} \Rightarrow\]
\[9 = -\frac{\pi}{2} + c \Rightarrow c = 9 + \frac{\pi}{2}\]
This is actually the wrong way to do it. The correct way to do it is to realise
that the right hand side is a standard integral of \(\arcsin\) and use that to
find \(c = 9\) for the solution
\[y = \pm\sqrt{\arcsin(x) + 9}\]
The two solutions are however equivalent so either works. \(y(0) = 3\) tells
us that the positive side of each of these two solutions are the correct one.

\subsubsection*{Integrating Factor and Linear First Order}

By working through the following differential equation, we can make an
observation which will be useful in solving a certain form of differential
equations.
\[x\dd{y}{x} + y = e^x\]
\[x\dd{y}{x} + y = \derivx{xy} \Rightarrow \derivx{xy} = e^x \Rightarrow\]
\[\int\derivx{xy}\dx = \int e^x\dx \Rightarrow xy = e^x + c \Rightarrow\]
\[y = \frac{e^x + c}{x}\mcom x\neq0\]
By obsering that \(x\dd{y}{x} + y\) is the result of the application of the
product rule in finding a derivative of \(xy\), we were able dramatically
simplify the solving of this equation. This process can be made more general
as well. \par
For an ordinary differential equation to be linear it must be of the form
\[f_1(x)\dd{y}{x} + f_0(x)y + q(x) = 0\]
for example, the equation solved at the start of this section was linear,
which becomes more clear when one rearranges it. 
\[x\dd{y}{x} + y = e^x \Rightarrow x\dd{y}{x} + y - e^x = 0\]
This is important, because it is the first step in solving differential
equations using the product rule. We start by arranging our differential
equation into the standrad form
\[\dd{y}{x} + P(x)y = Q(x)\]
To solve it, we then multiply by our integrating factor \(I(x)\), a function
which we don't yet know.
\[I(x)\dd{y}{x} + I(x)P(x)y = I(x)Q(x)\]
We want \(I\) to cause our equation to satisfy the condition
\[\derivx{y(x)I(x)} = Q(x)I(x)\]
To find an \(I\) with this property we need to solve such that
\[\dd{I}{x} = P(x)I \Rightarrow I = \pm e^{\int P(x) \dx}\]
We have observed that this condition yields a seperable differential equation
solved it for the general form and used this to generate our \(I\). Because
we need only a single integrating factor \(I\), we can discard the negative
solutions to yield
\[I(x) = e^{\int P(x) \dx}\]
For the same reason, we can ignore any absolute values or constants of
integration in the integral as well. Applying this to our first example:
\[x\dd{y}{x} + y = e^x \Rightarrow \dd{y}{x} + \frac{y}{x} = \frac{e^x}{x}
\Rightarrow P(x) = \frac{1}{x}\]
\[I(x) = e^{\int \frac{1}{x} \dx} = e^{\log(x)} = x\]
\[I(x)\dd{y}{x} + I(x)\frac{y}{x} + I(x)\frac{e^x}{x} = x\dd{y}{x} + y + e^x
\Rightarrow \derivx{xy} = e^x\]
Through our general formula, we found our way to the same solution we arrived
at through a leap of intuition in the initial example.
\[\dd{y}{x} + \frac{y}{x} = \sin(x)\mcom (x\neq 0)\]
\[I(x) = e^{\int P(x) \dx} = e^{\int \frac{1}{x} \dx} = e^{\log(x)} = x\]
\[x\dd{y}{x} + y = x\sin(x) = \derivx{xy} \Rightarrow\]
\[\int \derivx{xy} \dx = \int x\sin(x) \dx \Rightarrow xy 
= -x\cos(x) + \int\cos(x) \dx \Rightarrow\]
\[xy = -x\cos(x) + \sin(x) + c \Rightarrow y 
= -\cos(x) + \frac{\sin(x)}{x} + \frac{c}{x}\]
Here we used an integrating factor to solve the left side, and then integrated
the right side through derivative substitution.

\subsubsection*{Other First Order}

It is sometimes possible to apply a substitution to simplify a differential
equation down to a seperable or linear differential equation. A differentiable
equation of a \textit{homogenous type} has the form
\[\dd{y}{x} = f\left(\frac{y}{x}\right)\]
In this case, we can substitute \(y = \frac{y}{x}\) to reduce to a seperable
differential equation. Another case is differential equations with the form
of Bernoulli's equation, that is
\[\dd{y}{x} + P(x)y = Q(x)y^n\]
In this case, a substitution of \(u = y^{1 - n}\) reduces the equation to 
linear.

\subsubsection*{Homogenous Example}

\[\dd{y}{x} = \frac{y}{x} + \cos^2\left(\frac{y}{x}\right)
\mcom (-\frac{\pi}{2} < \frac{y}{x} < \frac{\pi}{2})\]
\[u = \frac{y}{x} \Rightarrow \dd{y}{x} = x\dd{y}{x} + u\]
\[x\dd{u}{x} + u = u + \cos^2(u) \Rightarrow \dd{u}{x} = \frac{\cos^2(u)}{x}
\Rightarrow\]
\[\sec^2(u)\dd{u}{x} = \frac{1}{x} \Rightarrow \int\sec^2\dd{u}{x}\dx 
= \int\frac{1}{x}\dx \Rightarrow \int\sec^2\dx = \int\frac{1}{x}\dx
\Rightarrow\]
\[\tan(u) = \log(\abs{x}) + C \Rightarrow \tan\left(\frac{y}{x}\right) 
= \log(\abs{x}) + C \Rightarrow\]
\[y = x\arctan(\log(\abs{x} + C))\]

\subsubsection*{Bernoulli Example}

\begin{center}
    \[\dd{y}{x} + y = e^{3x}y^4\mcom (y\neq0)\]
    \[u = y^{-3}\]
    We want to express \(y\) and \(\dd{y}{x}\) in terms of \(u\), \(x\) and 
    \(\dd{u}{x}\).
    \[\dd{u}{x} = \derivx{y^{-3}} \Rightarrow \dd{u}{x} = -3y^{-4}\dd{y}{x}\]
    Because we are diffentiating an equation of \(y\), we need to use implicit
    differentiation, hence the \(\dd{y}{x}\) term. Now that we have this
    equation, we can multiply our other equation by the coefficient to replace.
    \[-3y^{-4}\left(\dd{y}{x} + y\right) = -3y^{-4}\left(e^{3x}y^4\right)
    \Rightarrow\]
    \[\dd{u}{x} - 3y^{-3} = -3e^{3x} = \dd{u}{x} -3u\]
    At this stage we simple have a linear differential equation, and the
    solution is fairly easy to find.
\end{center}

\subsection*{Population Models}

\subsubsection*{Malthus (Doomsday) Model}

The Malthus or Doomsday model for population growth states that rate of growth
at a time \(t\) of a population \(p\) is proportional to the population.
\[\dd{p}{t}\propto p \Rightarrow \dd{p}{t} = kp\]
Here, \(k\) is a proportionality constant which corresponds to net births per
unit population per unit time. If we take \(p(0) = p_0\) we find that
\[p(t) = p_0e^{kt}\]
Thus, the model is clearly an exponential relationship. Obviously this model is
somewhat unrealistic; for a \(k > 0\) it implies unending exponential growth.

\subsubsection*{Equilibrium Solutions}

An equilibrium solution to a differential equation is a constant solution. For
example, in the Malthusian example \(p_0 = 0\) is an equilibrium solution of
\(p(t) \equiv 0\). For these to exist, we must have
\[\dd{x}{t} = 0\]
For a differential equation with variables \(x\) and \(t\). For example
\[\dd{x}{t} = 3x - 2\]
\[\dd{x}{t} = 0 \Rightarrow x = \frac{2}{3}\]
The equilibrium solution is \(x(t) \equiv \frac{2}{3}\). A \textit{phase plot}
is a plot of the derivative of a function against the value of the function. On
these plots, the equilibria are the horizontal axis intercepts. These plots are
generally only useful for differential equations of the form
\[\dd{x}{t} = f(x)\]
This is known as an \textit{autonomous} differential equation, because the
right hand side is independent of \(t\).

\begin{plot}[
    xmin = -0.5,
    xmax = 2,
    ymin = -2.5,
    ymax = 2.5
]
    \addplot[thick, blue, <->] {3*x - 2}
    node[below, right, pos = 0.6] {\(y(x) = \dd{x}{t}\)};
    \node[circle, fill, inner sep = 2pt] at (axis cs: 0.667, 0) {};
    \node[above left, yshift = 0.2cm] at (axis cs: 0.667, 0) {\(\frac{2}{3}\)};
    \addplot[thick, black, dashed] coordinates {(0.667, 2.5)(0.667, -2.5)};
    \node[right] at (axis cs: 0.667, 2) {\(\dd{x}{t} > 0\)};
    \node[left] at (axis cs: 0.667, -2) {\(\dd{x}{t} < 0\)};
\end{plot}

Examining this plot, we can see that the derivative of \(x\) will be
positive for all values of \(x > \frac{2}{3}\) while it will be negative
for values less than \(\frac{2}{3}\). We can also see that as \(x\) increases,
the gradient of the function increases, as well as the inverse. Using this
information, in combination with the constant solution learned from the
intercet, and the fact that no solution may cross any other solution,
we can plot the family of solutions to this differential equation.

\begin{plot}[
    xmin = -1,
    xmax = 0.5,
    ymin = -0.5,
    ymax = 2
]
    \addplot[thick, red, <->] {e^x + 2/3};
    \addplot[thick, red, <->] {0.5*e^x + 2/3};
    \addplot[thick, red, <->] {2/3}
    node[above, pos = 0.52] {\(x(t)\equiv\frac{2}{3}\)};
    \addplot[thick, red, <->] {-0.5*e^x + 2/3};
    \addplot[thick, red, <->] {-e^x + 2/3};
\end{plot}

We can see that for the initial condition of \(x(0) = \frac{2}{3}\), the value
will be constant, while for initial conditions greater than this value the
equation diverges to infinity and for values less, it diverges to negative
infinity. \par
It can be useful to examine if an equilibrium is stable. An equilibrium is
\textit{stable} if nearby solutions grow closer with increasing \(t\), while
it is unstable if solutions diverge, as they do in the above example. On a
phase plot, a stable equilibrium will have a negative slope passing through it.
On a phase plot, an unstable equilibrium has a positive slope through it,
as in the example. \par
A semistable equilibrium occurs when one side of solutions approach the
equilibrium. This occurs at a touching point in a phase plot.

\subsubsection*{Extending the Doomsday Model}

To extend the Malthusian Doomsday model explored earlier, we can add a notion
of ``harvesting'', the process of some quantity of the population being lost
every unit time.
\[\dd{p}{t} = kp - h \mcom h > 0\]
This has little effect on the exponential curve; because the curve is
exponential, we can subtract \(h\) without changing its behaviour in the long
term. \par
Another alteration we could make would be to add a ``competition'' term,
creating a logistic model. This added term accounts for competition for
resources by adding a negative term dependent on the population size.
\[\dd{p}{t} = kp - \frac{k}{a}p^2 = kp\left(1 - \frac{p}{a}\right)\]
Here, \(a\) is the \textit{carrying capacity}, the capacity for population in
the environment. In the case that \(p > a\), this yields a negative derivative
i.e. a decreasing population, steadily approaching \(a\). For example, a
logistic model with \(k = 1\) and \(a = 4\) looks like
\[\dd{p}{t} = p\left(1 - \frac{p}{4}\right)\]
\[\Rightarrow p(t) \equiv 0, p(t) \equiv 4\]
We can immediately see two equilibrium solutions thanks to the factorised form.
We know that other solutions may never cross these, therefore all other
solutions must be strictly less than \(0\), between \(0\) and \(4\) or strictly
greater than \(4\). \par
Drawing a phase plot, one finds that the derivative is positive for all values
between \(0\) and \(4\), and negative for outside values. It is at a maximum at
\(2\); this can all be derived from the parabolic shape of the derivative. This
showcases the classic logistic behaviour of an exponential curve tapering off
as the population nears capacity. For values above \(a\), the population decays
rapidly to \(a\). \par
In this case, \(0\) is an unstable equilibrium while \(4\) is a stable
equilibrium. The point of inflection of the plot is when the gradient is at a
maximum, i.e. at \(p = 2\). \par
We can combine the logistic model with harvesting to produce a combined model.
\[\dd{p}{t} = kp\left(1 - \frac{p}{a}\right) - h\]
As an example we can take
\[\dd{p}{t} = p\left(1 - \frac{p}{4}\right) - \frac{3}{4}\]
Solving the quadratic we find equilibria of \(p(t) \equiv 3, p(t) \equiv = 1\).

\begin{plot}[
    xmin = -0.5,
    xmax = 5,
    ymin = -1,
    ymax = 0.5,
    ylabel = \(\dd{p}{t}\),
    xlabel = \(p\)
]
    \addplot[blue, thick, ->, domain = 0:4] {(-1/4)*(x-3)*(x-1)};
\end{plot}

Examining the phase plot we once again find that population will increase while
between these equilibria, with the upper equilibrium being a stable equilibrium
and the lower unstable. The main difference is the narrowed bounds; because of
the harvesting term a starting population of \(1\) or less will die out and a
population will never grow beyond \(3\). \par
While this phase plot analysis is useful, it is of course much more useful to
actually solve the differential equation. We can take \(N(t) = 1\) to make this
a seperable integral.
\[\dd{p}{t} = -\frac{1}{4}(p - 3)(p - 1)\]
\[\int \frac{1}{-\frac{1}{4}(p - 3)(p - 1)} \dd{p}{t} \:\mathrm{d}t = \int        
1 \:\mathrm{d}t \Rightarrow \int \frac{4}{(p - 3)(p - 1)} \:\mathrm{d}p 
= \int 1 \:\mathrm{d}t\]
\[\frac{4}{(p - 3)(p - 1)} = \frac{A}{p - 3} + \frac{B}{p - 1} = \]
\[\frac{A(p - 1)}{(p - 3)(p - 1)} + \frac{B(p - 3)}{(p - 3)(p - 1)}
= \frac{A(p - 1) + B(p - 3)}{(p - 3)(p - 1)} = \]
\[\frac{Ap - A + Bp - 3B}{(p - 3)(p - 1)} \Rightarrow -A - 3B = 4
\mcom A + B = 0\]
\[A = -B \Rightarrow -A - 3B = 4 = B - 3B \Rightarrow B = 2 \Rightarrow A =-2
\Rightarrow\]
\[\int \frac{4}{(p - 3)(p - 1)} \:\mathrm{d}p = \int \frac{2}{(p - 1)} 
- \frac{2}{(p - 3)} \:\mathrm{d}p = \int 1 \:\mathrm{d}t \Rightarrow\]
\[2\log(\abs{p - 1}) - 2\log(\abs{p - 3}) = t + C\]
\[\log(\abs{p - 1}) - \log(\abs{p - 3}) = \frac{t}{2} + C\]
\[\log\left(\frac{\abs{p - 1}}{\abs{p - 3}}\right) = \frac{t}{2} + C\Rightarrow
\frac{p - 1}{p - 3} = \pm e^{\frac{t}{2} + C} =\pm Ae^\frac{t}{2}\]

\subsubsection*{Mixing Problems}

Differential equations can be used to consider the combination of liquids, for
example considering a pollutant entering a pond. If \(x\) represents the total
quantity of pollutant in the pool at time \(t\), then we can take
\(C = \frac{x}{V}\) where \(C\) is the concentration of pollutant in the pool
of volume \(V\) at time \(t\). We have \(\dd{x}{t}\) is equal rate of inflow
minus rate of outflow.
\[\mathrm{inflow} = C_\mathrm{in}\times F_\mathrm{in}\]
Where \(F_\mathrm{in}\) is the flow rate of liquid into the pool with pollutant
at concentration \(C_\mathrm{in}\) in that liquid.
\[\mathrm{outflow} = C\times F_\mathrm{out} =\frac{x}{V}\times F_\mathrm{out}\]
Here, because we have defined \(C\) in terms of \(x\) and \(V\) we can replace
it. For values \(V = 1000\mathrm{m^3}\), \(x_0 = 100\mathrm{g}\), 
\(F_\mathrm{in} = F_\mathrm{out} = 10\mathrm{m^3min^{-1}}\), 
\(C_\mathrm{in} = 2\mathrm{g\;m}^{-3}\) we find
\[\dd{x}{t} = 2\mathrm{g\;m}^{-3}\times10\mathrm{m^3min^{-1}} 
- \frac{x}{1000\mathrm{m}^3}\times10\mathrm{m^3min^{-1}}\]
\[\dd{x}{t} = 20\mathrm{g\;min}^{-1} - \frac{x}{100}\mathrm{min}\]
So if we say \(t\) is in terms of minutes and \(x\) in terms of grams we have
\[\dd{x}{t} = 20 - \frac{x}{100}\]
Solving this, one finds
\[x(t) = 2000 + ce^{\frac{-t}{100}}\]
\[x(0) = 100 \Rightarrow c = -1900\]
To find the concentration at a given time, we can use \(c(t) = \frac{x(t)}{V}\)
to find
\[c(t) = 2 - \frac{19}{10}e^{\frac{-t}{100}}\]
As \(t\) increases, the right hand term approaches \(0\), so we can say that in
the long term the concentration will be \(2\). We could describe this right
hand term as a \textit{transient term} and the left hand (constant) term as a
\textit{steady state term}. \par
What does the equation look like if the inflow rate is instead
\(5\mathrm{m^3min^{-1}}\)? Inflow will now be
\[C_\mathrm{in} \times F_\mathrm{in} 
= 2\mathrm{g\;m^{-3}}\times5\mathrm{m^3min^{-1}} = 10\mathrm{g\;min^{-1}}\]
We now have a changing rather than constant volume. The volume at time
\(t\) can be expressed as
\[V(t) = 1000 + 5t - 10t = 1000 - 5t\]
Thus the outflow is given by
\[\frac{x}{V} \times F_\mathrm{out} = \frac{x}{1000 - 5t}\times10 
= \frac{x}{100 - \frac{t}{5}}\]
And the overall equation is
\[\dd{x}{t} = 10 - \frac{x}{100 - \frac{t}{5}}\]
If \(t > 200\), \(V < 0\) which doesn't make much sense, so we can restrict the
domain. This is a linear equation, so it can be solved through the integrating
factor method.

\subsection*{Second Order Ordinary Differential Equations}

A second order differential equation is of the form
\[F\left(x, y, \dd{y}{x}, \dd{^2y}{x^2}\right) = 0\]
Generally a second order differential equation will have two arbritrary
constants. Therefore, a second piece of information is necessary to solve an
initial value problem. For example, \(y(x_0) = y_0\) and
\(y^\prime(x_0) = y_1\). By adding this second variable, we could instead have
two values for \(y\); if we have \(y(a) = y_0\) and \(y(b) = y_1\), we have a
boundary value problem; two points through which the solution passes.

\subsubsection*{Linear Second Order}

The general form of a linear second order ordinary differential equation is
\[\dd{^2y}{x^2} + P(x)\dd{y}{x} + Q(x)y = R(x)\]
If \(R(x) = 0\), the equation is homogenous. If \(R(x) \neq 0\) the equation is
inhomogenous. The general solution to a homogenous equation of this form is
\[y(x) = c_1y_1(x) + c_2y_2(x)\]
Here, \(y_1\) and \(y_2\) are \textit{linearly independent solutions} of the
equation and \(c_2\) and \(c_2\) are arbritrary constants. The term ``linearly
independent'' implies that
\[c_1y_1(x) + c_2y_2(x) = 0 \Rightarrow c_2 = c_2 = 0\]
Another way of stating this is that neither function is a non-zero constant
multiple of the other function. For instance, while \(e^{2x}\) and \(xe^{2x}\)
are linearly independent, \(x^2\) and \(2x^2\) are not. \par
The reason this works is that both solutions can be understood to exist in the
same space of solutions to the equation, and as linear solutions, they can be
added while still being a solution to the equation.

\subsubsection*{Homogenous Second Order Linear}

Equations of this form look like
\[ay^\pp + by^\prime + cy = 0\]
Where \(a, b, c\) are constants. To find \(y(x)\), we need to find two linearly
independent solutions. Once we have these, we can use our theorem for the
general solution. \par
The best way to start out this process is by considering exponential solutions.
If we first try \(y(x) = e^{\lambda x}\), we find
\[y^\prime = \lambda e^{\lambda x}\mcom y^\pp 
= \lambda^2e^{\lambda x}\]
Substituting into the equation, we find
\[a\lambda^2e^{\lambda x} + b\lambda e^{\lambda x} + ce^{\lambda x} = 0\]
\[\Rightarrow e^{\lambda x}(a\lambda^2 + b\lambda + c) = 0\]
Because \(e^{\lambda x} \neq 0\), we need the term in brackets (known as the
\textit{characteristic equation} of the differential equation) to equal \(0\)
and thus be a solution. To find \(\lambda\) we can simply use the quadratic
formula, which conveniently yields a positive and negative value; i.e. two
linearly independent solutions.
\[\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]
In the case that the discriminant, \(b^2 - 4ac > 0\), this formula yields two
distinct real values, each of which is a solution. Thus, the overall general
solution is
\[y(x) = Ae^{\lambda_1 x} + Be^{\lambda_2 x}\]

\subsubsection*{Example}

\[y^\pp + 7y^\prime + 12y = 0\]
\[\Rightarrow a = 1, b = 7, c = 12\]
\[\Rightarrow \lambda = \frac{-7 \pm \sqrt{(-7)^2 -4\times1\times12}}{2}
= \frac{-7 \pm \sqrt{49 - 48}}{2} = \frac{-7 \pm 1}{2}\]
\[\Rightarrow \lambda = -4\mcom\lambda = -3\]
\[\Rightarrow y(x) = Ae^{-4x} + Be^{-3x}\]

\subsubsection*{Zero Discriminant}

If \(b^2 - 4ac = 0\), we will find a single real solution
\[\lambda = \frac{-b}{2a}\]
In this case, there is a second solution of \(xe^{\lambda x}\) as the second
solution (to be accepted as gospel; determined through ``variation of
parameters.'').
\[y(x) = Ae^{\lambda x} + Bxe^{\lambda x}\]
The reason this works is that we end up with a solution looking like
\[xe^{\lambda x}(a\lambda^2 + b\lambda + c) + (2\lambda a + b)e^{\lambda x}\]
Which is a solution because we know that \(2\lambda a + b = 0\).

\subsubsection*{Example}

\[y^\pp + 2y^\prime + y = 0\]
\[\Rightarrow a = 1, b = 2, c = 1\]
\[\Rightarrow \lambda = \frac{-2 \prime \sqrt{(-2)^2 + 4\times1\times1}}{2}
= \frac{-2}{2} = -1\]
\[\Rightarrow y(x) = Ae^{-x} + Bxe^{-x}\]

\subsubsection*{Negative Discriminant}

Finally, we come to \(b^2 - 4ac < 0\). In this case, we will end up with two
complex conjugate values of \(\lambda\).
\[\lambda_1 = \alpha + i\beta\mcom \lambda_2 = \alpha - i\beta\]
This yields two complex linearly independent solutions
\[e^{(\alpha + i\beta)x}\mcom e^{(\alpha - i\beta)x}\]
And a general solution of 
\[y(x) = c_1e^{(\alpha + i\beta)x} + c_2e^{(\alpha - i\beta)x}\]
This can then be rewritten as
\[y(x) = c_1e^{\alpha x}(\cos(\beta x) + i\sin(\beta x))
+ c_2e^{\alpha x}(\cos(\beta x) - i\sin(\beta x))\]
\[y(x) = (c_1 + c_2)e^{\alpha x}\cos(\beta x) + (c_1i - c_2i)e^{\alpha x}
\sin(\beta x)\]
If we then take \(A = c_1 + c_2\) and \(B = (c_1 - c_2)i\), and assume that
\(A\) and \(B\) are real, then we will find two real and linearly independent
solutions of 
\[e^{\alpha x}\cos(\beta x)\mcom e^{\alpha x}\sin(\beta x)\]
\[y(x) = Ae^{\alpha x}\cos(\beta x) + Be^{\alpha x}\sin(\beta x)\]
This only works as long as \(c_1\) and \(c_2\) are complex conjugates, because
it relies on the identities \(\bar{z} + z = 2\mathrm{Re}(z)\) and 
\(\bar{z} - z = 2\mathrm{Im}(z)i\).

\subsubsection*{Example}

\begin{center}
    \[y^\pp - 4y^\prime + 13y = 0\]
    \[\Rightarrow a = 1, b = -4, c = 13\]
    \[\Rightarrow \lambda = \frac{4 \pm \sqrt{(-4)^2 - 4\times1\times13}}{2}
    = \frac{4 \pm \sqrt{16 - 52}}{2} = \frac{4 \pm 6i}{2}\]
    \[\Rightarrow \lambda = 2 + 3i\mcom \lambda = 2 - 3i\]
    \[\Rightarrow \alpha = 2\mcom \beta = 3\]
    \[\Rightarrow y(x) = Ae^{2x}\cos(3x) + Be^{2x}\sin(3x)\]
    Given the initial values of \(y(0) = 1\) and \(y^\prime(0) = 6\) we can
    find
    \[1 = Ae^{2\times0}\cos(3\times0) + Be^{2\times0}\sin(3\times0) = A\]
    \[y^\prime = A\derivx{e^{2x}\cos(3x)} + B\derivx{e^{2x}\sin(3x)}\]
    \[= A(2e^{2x}\cos(3x) - 3\sin(3x)e^{2x}) + B(2e^{2x}\sin(3x) 
    + 3\cos(3x)e^{2x})\]
    \[y^\prime(0) = 2e^{2\times0}\cos(3\times0) - 3\sin(3\times0)e^{2\times0}
    + B(2e^{2\times0}\sin(3\times0) + 3\cos(3\times0)e^{2\times0})\]
    \[= 2 + B(3) = 6 \Rightarrow B = \frac{4}{3}\]
\end{center}

\bigskip
We can rewrite a solution of this form in terms of a single trigonometric
function using the fact that it has an equivalent form to
\(e^{2x}(a\cos(\theta) + b\sin(\theta))\) as
\[\sqrt{a^2 + b^2}\cos(\theta - \phi)\]
Where \(\phi = \arctan\left(\frac{b}{a}\right)\). This can be useful for
graphing or further manipulation.

\subsubsection*{Inhomogenous Second Order Linear}

To solve these differential equations, we have a theorem
\[y^\pp + P(x)y^\prime + Q(x)y = R(x)\]
\[\Rightarrow y(x) = y_H(x) + y_P(x)\]
Where \(y_H(x) = c_1y_1(x) + c_2y_2(x)\) is the general solution of the
homogenous differential equation, the left hand side, as examined previously,
and \(y_P(x)\) is a particular solution of the inhomogenous equation. \par
We know this works because \(y_H(x)\) must solve the homogenous solution, while
\(y_P(x)\) must solve the inhomogenous siutation. \par
The general form of these inhomogenous equations is 
\[ay^\pp + by^\prime + cy = R(x)\]

\subsubsection*{Examples}

\begin{center}
    \[y^\pp + 2y^\prime - 8y = 1 - 8x^2\]
    We begin by finding the general solution of the homogenous equation.
    \[y^\pp + 2y^\prime - 8y = 0\]
    \[\Rightarrow a = 1, b = 2, c = -8\]
    \[\Rightarrow \lambda
    = \frac{-2 \pm \sqrt{2^2 - 4\times1\times-8}}{2\times1}
    = \frac{-2\pm6}{2}\]
    \[\Rightarrow \lambda = -4\mcom \lambda = 2\]
    \[\Rightarrow y_H(x) = Ae^{-4x} + Be^{2x}\]
    We then find a particular solution to the equation. Here, we are trying
    to find a \(y_P(x)\) for which the left hand side will work out to a
    quadratic, such that it matches the right hand side. Thus, we can try
    a general quadratic solution.
    \[y_P = ax^2 + bx + c\]
    \[y_P^\prime = 2ax + b\]
    \[y_P^\pp = 2a\]
    \[2a + 2(2ax + b) - 8(ax^2 + bx + c) = 1 - 8x^2\]
    \[2a + 2b - 8c + (4a - 8b)x - 8ax^2 = 1 - 8x^2\]
    \[\Rightarrow 2a + 2b - 8c = 1\mcom 4a - 8b = 0\mcom 8a = 8\]
    \[\Rightarrow a = 1 \Rightarrow b = \frac{1}{2} \Rightarrow
    c = \frac{1}{4}\]
    \[y_P = x^2 + \frac{1}{2}x + \frac{1}{4}\]
    \[\Rightarrow y(x) = Ae^{-4x} + Be^{2x} + x^2 + \frac{1}{2}x +\frac{1}{4}\]
\end{center}

In this question, we noticed that the right hand side was a quadratic, so after
solving for the homogenous solution, we simply needed to find a fitting
quadratic as our particular solution.

\begin{center}
    \[y^\pp + 2y^\prime - 8y = e^{3x}\]
    This is the same homogenous equation as before, so we have our \(y_H\).
    \[y_H(x) = Ae^{-4x} + Be^{2x}\]
    Because we have an exponential on the left hand side, and we know that
    exponentials have exponential derivatives, so we want to try exponential
    \(y_P\)s. Because the equation needs to look like \(e^{3x}\), we need an
    exponential with \(3x\) as the exponent.
    \[y_P = \alpha e^{3x}\]
    \[y_P^\prime = 3\alpha e^{3x}\]
    \[y_P^\pp = 9\alpha e^{3x}\]
    \[9\alpha e^{3x} + 6\alpha e^{3x} - 8\alpha e^{3x} = 7\alpha e^{3x}\]
    \[7\alpha e^{3x} = e^{3x} \Rightarrow \alpha = \frac{1}{7}\]
    \[\Rightarrow y(x) = Ae^{-4x} + Be^{2x} + \frac{1}{7}e^{3x}\]
\end{center}

Here, we saw an exponential on the right hand side, so we knew an exponential
was needed on the left. By using a constant \(\alpha\), we were able to find a
particular solution which fit the equation.

\begin{center}
    \[y^\pp + 2y^\prime - 8y = 85\cos(x)\]
    Once again, this is the same \(y_H\). To find a solution for the left hand
    side here, we have a slightly more complex process. Because \(\cos\)
    differentiates to \(-\sin\), we use a general trigonometric equation of the
    form
    \[y_P(x) = \alpha\cos(x) + \beta\sin(x)\]
    \[y_P^\prime(x) = -\alpha\sin(x) + \beta\cos(x)\]
    \[y_P^\pp = -\alpha\cos(x) - \beta\sin(x)\]
    \[(-\alpha\cos(x) - \beta\sin(x)) + 2(-\alpha\sin(x) + \beta\cos(x))
    -  8(\alpha\cos(x) + \beta\sin(x))\]
    \[2\beta\cos(x) - 9\alpha\cos(x) - 9\beta\sin(x) - 2\alpha\sin(x)
    = 85\cos(x)\]
    \[\Rightarrow 2\beta - 9\alpha = 85\mcom -9\beta - 2\alpha = 0\]
    \[\Rightarrow \beta = \frac{-2}{9}\alpha 
    \Rightarrow \frac{-4}{9}\alpha - 9\alpha 
    = 85 \Rightarrow \frac{-85}{9}\alpha = 85 \Rightarrow \alpha = -9\]
    \[\Rightarrow \beta = 2 \Rightarrow y_P(x) = -9\cos(x) + 2\sin(x)\]
    \[\Rightarrow y(x) = Ae^{-4x} + Be^{2x} - 9\cos(x) + 2\sin(x)\]
\end{center}

For a trigonometric function, one must use a trigonometric function. Using
unknown coefficients allows us to find out particular solution once again.

\subsubsection*{Superposition of Solutions}

Thus far we have looked at how to solve three types of \(R(x)\) functions, we
now look at ``superimposing'' these solutions to find a solution for a
combination of \(R(x)\)s.
\[ay^\pp + by^\prime + cy = \alpha R_1(x) + \beta R_2(x)\]
\[\Rightarrow y_P(x) = \alpha y_1(x) + \beta y_2(x)\]
Where \(y_1(x)\) is an inhomogenous particular solution of the left side to
\(R_1(x)\) and \(y_2(x)\) is the same for \(R_2(x)\). Using the example from
above:

\begin{center}
    \[y^\pp + 2y^\prime - 8y = 3 - 24x^2 + 7e^{3x}\]
    We have already found a \(y_H(x)\) for this equation, in addition we have
    solved the following two \(R(x)\) functions:
    \[R_1(x) = 1 - 8x^2 \Rightarrow y_P(x) = x^2 + \frac{1}{2}x + \frac{1}{4}\]
    \[R_2(x) = e^{3x} \Rightarrow y_P(x) = \frac{1}{7}e^{3x}\]
    We can now \textit{superimpose} these to find a solution to the new
    \(R(x)\).
    \[R(x) = 3R_1(x) + 7R_2(x)\]
    \[y_P(x) = 3(x^2 + \frac{x}{2} + \frac{1}{4}) + 7(\frac{1}{7}e^{3x})\]
    \[y(x) = Ae^{-4x} + Be^{2x} + 3x^2 + \frac{3}{2}x + \frac{3}{4} + e^{3x}\]
\end{center}
We simply find the values of \(\alpha\) and \(\beta\), and multiply the \(y_P\)
solutions assocaited with thesse to find the general solution to the new
equation.

\subsubsection*{\(R(x)\) in the General Solution}

In an equation like \(y^\pp - y = e^x\), a \(y_P = \alpha e^x\) will result in
a left hand side equal to \(0\), which \(e^x\) can never equal. Therefore we
need another approach for this case. Therefore, we instead try \(\alpha xe^x\)
\[y_P = axe^x\]
\[y_P^\prime = \alpha x^e + \alpha xe^x\]
\[y_P^\pp = 2\alpha e^x + \alpha xe^x\]
\[\Rightarrow 2\alpha e^x + \alpha xe^x - \alpha xe^x = \alpha e^x\]
\[\Rightarrow y_P = \frac{1}{2}xe^x\]
\[\Rightarrow y(x) = Ae^x + Be^{-x} + \frac{1}{2}xe^x\]
Despite the fact the right hand side was part of the homogenous solution, we
are able to find a general solution. When there is only one solution to the
homogenous solution, like in the case \(y^\pp + 2y^\prime + y = e^{-x}\), we
instead try \(y_P = ax^2e^{-x}\). \par
Finally, in the case that there is no real
solution, we need to read for trigonometric functions. For example in
\(y^\pp + 49y = 28\sin(7t)\), we have the general solution
\(A\cos(7t) + B\sin(7t)\), which is a no-go. Intead we try 
\(y_P(t) = at\cos(7t) + bt\sin(7t)\), adding a \(t\) factor which cancels out
as the equation gets solved.

\subsection*{Applications of Second Order Linear}

A spring has a natural length which it wants to return to when stretched. In an
ideal scenario, a stretched spring under a gravitational force will oscillate
sinusoidally up and down. If we include damping due to effects like friction
and air resistance, the sinusoidal motion will converge to \(0\). Systems like
this can be effectively modelled using second order linear ordinary
differential equations. \par
To consider these systems we take an object of mass \(m\mathrm{kg}\) attached
to a spring hanging from a fixed support. The spring has natural length
\(L\mathrm{m}\), and is stretched downward by \(s\mathrm{m}\) due to the weight
force \(W = mg\) of the mass. Opposing the weight force is the restorative
force of the spring \(T = -ks\), given by Hooke's law. \par
At the state described, the equilibrium, the net force is \(0\).
\[W + T = 0 = mg - ks \Rightarrow mg = ks\]
To model the motion of the mass around the equilibrium, we can define a
distance \(y(t)\), the distance of the mass from the equilibrium at time
\(t\mathrm{s}\). Thus, we can rewrite \(T\) as \(-k(s + y)\). This is an
opportune time to introduce damping forces as well, as \(R = -\beta \dot{y}\)
where \(\dot{y}\) means ``the time derivative of \(y\)'' i.e. \(y^\prime\).
This is essentially stating that damping is proportional to velocity. \par
We also have Newton's second law \(F = ma\), which can be equivalently
expressed as \(F = m\ddot{y}\). Applying this to the object, we find
\[m\ddot{y} = W + T + R = mg - k(s + y) - \beta\dot{y}
= mg - ks - y - \beta \dot{y}\]
\[mg - ks = 0 \Rightarrow mg - ks - y - \beta \dot{y} = -y - \beta \dot{y}\]
\[\Rightarrow m\ddot{y} + \beta\dot{y} + ky = 0\]
Conveniently, this is a homogenous second order linear differential equation
with constant coefficients. Thus to solve it, we can simply use the process
explored above trying equations of the form \(y(t) = e^{\lambda t}\).
\[\lambda = \frac{-\beta \pm \sqrt{\beta^2 -4mk}}{2m}\]
Because we have related the coefficients to real world values, we can use the
outcome of our equation to describe the real world situation.

\begin{itemize}
    \item When \(\beta = 0\) we find \(\lambda = \pm ib\). In this case, the
        system is undergoing simple harmonic motion of the form
        \(y = A\cos(bt) + B\sin(bt)\).
    \item When \(0 < \beta < 2\sqrt{mk}\) we find \(\lambda = a \pm ib\), a
        situation where the spring is underdamped or shows weak damping. In
        this case, the motion will take the form
        \(y = e^{at}(A\cos(bt) + B\sin(bt))\), with \(a < 0\). The spring will
        oscillate with decreasing amplitude, slowly returning to equilibrium.
    \item When \(\beta = 2\sqrt{mk}\) i.e. the discriminant is \(0\), we find
        \(\lambda = a, a\) yielding \(y = Ae^{at} + Bte^{at}\) a situation
        where the system undergoes critical damping and approaches equilibrium
        without oscillation.
    \item In the final case of \(\beta > 2\sqrt{mk}\) we have two real routes,
        \(\lambda = a, b\) and find a solution of \(y = Ae^{at} + Be^{bt}\),
        resulting in a slow approach to the equilibrium position. 
\end{itemize}

\subsubsection*{Example}

A \(\frac{40}{49}\mathrm{kg}\) mass stretches a spring hanging from a fixed
support by \(0.2\mathrm{m}\). The mass is released from the equilibrium
position with a downward velocity of \(3\mathrm{ms}^{-1}\). Find the position
\(y\) below equilibrium at a time \(t\) for the damping constant \(\beta\)
equal to each of \(0\), \(\frac{160}{49}\).

\begin{center}
    \[m = \frac{40}{49}, s = 0.2, y(0) = 0, \dot{y}(0) = 3\]
    \[mg = ks \Rightarrow k = \frac{mg}{s} 
    = \frac{\frac{40}{49} \times 9.8}{0.2} = 40\]
    \[\Rightarrow \frac{40}{49}\ddot{y} + \beta\dot{y} + 40y = 0\]
    To clean this up a little, we can multiply by \(\frac{49}{40}\).
    \[\ddot{y} + \frac{40}{49}\beta\dot{y} + 49y = 0\]
    \[\beta = 0 \Rightarrow \ddot{y} + 49y = 0\]
    We will try solutions of the form \(e^{\lambda t}\)
    \[y = e^{\lambda t} \Rightarrow \dot{y} = \lambda e^{\lambda t}
    \Rightarrow \ddot{y} = \lambda^2e^{\lambda t}\]
    \[\lambda^2e^{\lambda t} + 49e^{\lambda t} = 0 \Rightarrow
    e^{\lambda t}(\lambda^2 + 49) = 0\]
    \[e^{\lambda t} > 0 \Rightarrow \lambda^2 + 49 = 0 \Rightarrow \lambda
    = \pm 7i\]
    \[\Rightarrow y = A\cos(7t) + B\sin(7t)\]
    \[\Rightarrow \dot{y} = -7A\sin(7t) + 7B\cos(7t)\]
    \[y(0) = 0 \Rightarrow 0 = A\cos(0) + B\sin(0) \Rightarrow A = 0\]
    \[\dot{y}(0) = 3 \Rightarrow 7B\cos(0) = 3 \Rightarrow B = \frac{3}{7}\]
    \[y(t) = \frac{3}{7}\sin(7t)\]
    Now for the next value of \(\beta\)
    \[\beta = \frac{160}{49} \Rightarrow \ddot{y} + 4\dot{y} + 49y = 0\]
    \[\lambda^2e^{\lambda t} + 4\lambda e^{\lambda t} + 49e^{\lambda t} = 0
    = e^{\lambda t}(\lambda^2 + 4\lambda + 49) = 0\]
    We can solve this quadratic with the quadratic formula.
    \[\lambda = \frac{-4 \pm \sqrt{4^2 - 4\times 1 \times49}}{2}
    = -2 \pm \frac{\sqrt{-180}}{2} = -2 \pm 3\sqrt{5}i\]
    \[\Rightarrow y(t) = Ae^{-2t}\cos(3\sqrt{5}t) + Be^{-2t}\sin(3\sqrt{5}t)\]
    \[\Rightarrow \dot{y} = -2Ae^{-2t}\cos(3\sqrt{5}t) 
    - 3\sqrt{5}Ae^{-2t}\sin(3\sqrt{5}t)\]
    \[- 2Be^{-2t}\sin(3\sqrt{5}t) + 3\sqrt{5}Be^{-2t}\cos(3\sqrt{5}t)\]
    \[\Rightarrow A = 0, B = \frac{1}{\sqrt{5}}\]
    This is a system with weak damping or underdamping.
\end{center}

\subsubsection*{Downward External Force}

What happens if we apply an external force \(f\), dependent on \(t\) to the
spring-mass system? We now have another force on the spring, making the total
force on the object
\[m\ddot{y} = W + T + R + f(t) = mg -k(s + y) -\beta\dot{y} + f(t)\]
\[m\ddot{y} + \beta\dot{y} + ky = f(t)\]
We now have an inhomogenous equation rather than a homogenous one.

\subsubsection*{Example}

We can consider a system similar to that of the above example, with an external
force applied.

\begin{center}    
    \[f(t) = \frac{160}{7}\sin(7t)\]
    \[\frac{40}{49}\ddot{y} + \beta\dot{y} + 40y = \frac{160}{7}\sin(t)
    \Rightarrow \ddot{y} + \frac{49}{40}\beta\dot{y} + 49y = 28\sin(t)\]
    \[y(0) = 0\mcom \dot{y}(0) = 3\]
    We will use a value \(\beta = \frac{80}{7}\), which corresponds to critical
    damping, and yields a general solution of the homogenous equation of
    \[y(t) = (A + Bt)e^{-7t}\]
    We then find a particular solution by finding solutions of the form
    \(y_p = \alpha\cos(7t) + \beta\sin(7t)\) which yields
    \[y_p(t) = -\frac{2}{7}\cos(7t)\]
    Solving this we find
    \[A = \frac{2}{7}\mcom B = 5\]
    \[\Rightarrow y(t) = \left(\frac{2}{7} + 5t\right)e^{-7t} 
    - \frac{2}{7}\cos(7t)\]
\end{center}

In this, we have the left term as transient; due to the negative
exponential it rapidly approaches \(0\), while the right hand term is a
steady state term, which defines most of the motion of the system in the
long term. From this we can deduce that the external force is more significant
than the force due to gravity and the spring in the long term. \par
In the case were the particular solution has a factor of \(t\), we have a
situation where the system is undergoing resonance.

\subsubsection*{RLC Series Circuits}

An RLC series ciruit is a ciruit with \(4\) components connected in series.
These are a resistor, an inductor, a battery and a capacitor. The analogue to
displacement in this example is \(q\), the charge on the capacitor. The
analogue to velocity is \(I\), the current. The resistor has resistance of
\(R\Omega\), the inductor has inductance \(L\) Henrys, the capacitor has
capacitance \(C\mathrm{F}\) and the battery has voltage \(V\mathrm{V}\). \par
The voltage drops across the components are given by
\[IR = R\dd{q}{t}\]
\[L\dd{I}{t} = L\dd{^2q}{t^2}\]
\[\frac{q}{C}\]
We thus have a differential equation for the charge on the capacitor of
\[L\dd{^2q}{t^2} + R\dd{q}{t} + \frac{q}{C} = V\]
Using Kirchoff's Voltage Law. This can be solved in essentially the same
way as a spring problem.

\subsubsection*{Example}

Let us consider a capacitance of \(C = \frac{1}{30}\) a resistance of
\(R = 10\) an inductance of \(L = \frac{5}{3}\) and a voltage of \(V = 300\).

\begin{center}    
    Thus we have voltage drops across the resistor, capacitor and inductor of
    \[IR = 10I\mcom \frac{q}{C} = 30q\mcom L\dd{i}{t} = \frac{5}{3}\dd{i}{t}\]
    Yielding an equation of
    \[\frac{5}{3}\dd{i}{t} + 10i + 30q = 300 \Rightarrow
    \frac{5}{3}\dd{^2q}{t^2} + 10\dd{q}{t} + 30q = 300\]
    \[\Rightarrow \dd{^2q}{t^2} + 6\dd{q}{t} + 18q = 180\]
\end{center}

\section*{Functions of Two Variables}

A function of two variables defines a mapping of two real numbers to a single
real number, i.e.
\[z = f(x, y)\]
For functions of two variables, the domain can be thought of as a subset of the
real plane \(\R^2\), while the range is \(\R\). \par
To plot a function of two inputs we need a three dimensional plot to plot the
inputs \(x\) and \(y\) and the output \(z\).
\[\set{(x, y, z) \in \R^3 : (x, y) \in D \mand z = f(x, y)}\]
For a continuous function, this graph will be a surface lying directly above or
below the area mapped by the domain. The right-hand coordinate system used in
this subject dictates that the \(xy\) plane will be on the flat plane surface
with \(x\) pointing out of the page and the \(z\) axis pointing vertically
upward from this plane.

\subsection*{Equation of a Plane}

The Cartesian equation for a plane in \(\R^3\) is
\[ax + by + cz = d\]
Where \(a, b, c, d\) are real constants. The vector \(n = (a, b, c)\) is at
a normal to the plane. This equation comes from the fact that if \((a, b, c)\)
is perpendicular to the plane then \((a, b, c)\) must be perpendicular to all
vectors internal to the plane. So if we take \((x_0, y_0, z_0)\) as the point
at which the normal vector intersects the plane, all internal vectors to the
plane must satisfy
\[(a, b, c)\cdot(x - x_0, y - y_0, x - z_0) = 0\]
\[\Rightarrow a(x - x_0) + b(y - y_0) + c(z - z_0) = 0\]
\[\Rightarrow ax + by + cz = ax_0 + by_0 + cz_0 = d\]
So this essentially uses the properties of the normal vector to define the
rest of the plane. \par
To sketch a line in two dimensions, we would find two points (probably the
\(x\) and \(y\) intercepts) and draw a line between them. Likewise to sketch
a plane, we find three points, often the intercepts and plot them. For
example for \(f(x, y) = 2 - 4x - 3y\) we can find \(x\)-intercept of
\(\frac{1}{2}, 0, 0\), \(y\)-intercept \(0, \frac{2}{3}, 0\) and \(z\)
-intercept \(0, 0, 2\) yielding something like

\begin{plot3}[
    xmin = 0,
    xmax = 10,
    ymin = 0,
    ymax = 10,
    zmin = 0,
    zmax = 10,
    view = {115}{80},
    samples = 50
]
    \addplot3[surf] {2 - 4*x - 3*y};
\end{plot3}

\subsection*{Level Curves}

A curve on a surface defined by \(z = f(x, y)\) where \(z\) is constant is a
contour; this can be thought of like a contour line on a map, a region of
constant altitude. A level curve is a region of the \(xy\) plane which
represents a projection of a contour of a function. \par
To sketch a function of two variables we first draw the three axes. We could
then draw cross sections and level curves. We can then label intercepts and
other key points. \par
For example, for the function \(z = \sqrt{1 - x^2 - y^2}\) we can find its
level curves by taking \(z = c\).
\[\Rightarrow c = \sqrt{1 - x^2 - y^2}\]
Because the right hand side is a square root, we know \(c \geq 0\).
\[c^2 = 1 - x^2 - y^2 \Rightarrow x^2 + y^2 = 1 - c^2\]
\[1 - c^2 \geq 0 \Rightarrow c^2 \leq 1 \Rightarrow \abs{c} \leq 1\]
\[\Rightarrow 0 \leq c \leq 1\]
Here we can see that the level curves of this function are a variety of circles
of radii between \(0\) and \(1\) with centre \((0, 0)\).

\begin{plot}[
    xmin = -2,
    xmax = 2,
    ymin = -2,
    ymax = 2,
    width = 9cm,
    samples = 1000
]
    \addplot[blue, thick] {-(1 - x^2)^0.5};
    \addplot[blue, thick] {(1 - x^2)^0.5};
    \addplot[blue, thick] {-(0.5 - x^2)^0.5};
    \addplot[blue, thick] {(0.5 - x^2)^0.5};
    \addplot[blue, thick] {-(0.25 - x^2)^0.5};
    \addplot[blue, thick] {(0.25 - x^2)^0.5};
    \addplot[blue, thick] {-(0.1 - x^2)^0.5};
    \addplot[blue, thick] {(0.1 - x^2)^0.5};
    \addplot[blue, thick] {-(0.02 - x^2)^0.5};
    \addplot[blue, thick] {(0.02 - x^2)^0.5};

    \node[circle, fill, inner sep = 2pt] at (axis cs: -1, 0) {};
    \node[above left] at (axis cs: -1, 0) {\((-1, 0)\)};
    \node[circle, fill, inner sep = 2pt] at (axis cs: 0, 1) {};
    \node[above right] at (axis cs: 0, 1) {\((0, 1)\)};
    \node[circle, fill, inner sep = 2pt] at (axis cs: 1, 0) {};
    \node[above right] at (axis cs: 1, 0) {\((1, 0)\)};
    \node[circle, fill, inner sep = 2pt] at (axis cs: 0, -1) {};
    \node[below right] at (axis cs: 0, -1) {\((0, -1)\)};
\end{plot}

We can also tell that the two cross sections (\(x\)-\(z\) and
\(z\)-\(y\)) will be hemispheres. Knowing thanks to our level curves that as
\(z\) gets larger the radius will grow smaller, and that \(z\) is never
negative we can tell that this is a hemisphere of radius \(1\). \par
In similar functions
one might find that the cross-sections look like parabolas, indicating
something like parabolic bowl or absolute value functions, indicating
a cone. If \(x\) or \(y\) has a coefficient, the level curves will be
elliptical rather than circular.

\subsection*{Limits of Two Variable Functions}

If \(f: \R^2 \rightarrow \R\) we say that \(f\) has a limit \(L\) as \(x, y\)
approaches \(x_0, y_0\).
\[\limit_{(x, y) \rightarrow (x_0, y_0)} f(x, y) = L\]
If when \((x, y)\) approaches \((x_0, y_0)\) along any path in the domain
\(f(x, y)\) grows arbritrarily close to \(L\). This is in contrast to a single
variable limit, where it can only approach from left or right rather than from
any direction. All of the usual conditions and laws around limits apply. \par
Thus a two variable function is continuous at \((x, y) = (x_0, y_0)\) if
\[\limit_{(x, y) \rightarrow (x_0, y_0)} f(x, y) = f(x_0, y_0)\]

\subsubsection*{Example}

\begin{center}
    \[\limit_{(x, y) \rightarrow (2, 1)} \log(1 + 2x^2 + 3y^2)\]
    \(\log\) is continuous, so we can bring the limit inside.
    \[\log\left(\limit_{(x, y) \rightarrow (2, 1)} 1 + 2x^2 + 3y^2\right)\]
    \[= \log(12)\]
\end{center}

\subsection*{First Order Partial Derivatives}

For a function \(f: \R^2 \rightarrow \R\) the first order partial derivatives
are defined as the limits
\[f_x = \ddel{f}{x} = \limit_{h \rightarrow 0}\frac{f(x + h, y) - f(x, y)}{h}\]
\[f_y = \ddel{f}{y} = \limit_{h \rightarrow 0}\frac{f(x, y + h) - f(x, y)}{h}\]
Here \(f_x\) measures the rate of change of \(f\) with respect to \(x\) when
\(y\) is held constant, while \(f_y\) does the inverse.

\subsubsection*{Example}
\[f(x, y) = 3x^3y^2 + 3xy^4\]
\[\ddel{f}{x} = 9x^2y^2 + 3y^4\]
\[\ddel{f}{y} = 6x^3y + 12xy^3\]

\subsubsection*{Tangent Planes and Differentiability}

A single variable function exists if the fundamental limit exists for that
function at the point of differentiation. This is essentially equivalent to
requiring that a tangent line must exist on the function at the point of
differentiation. This tangent line has the function
\(y = f(a) + f^\prime(a)(x - a)\) for a function differentiated at \(x = a\).
\par
To extend this to two variable functions we can introduce the notion of a
tangent plane. This essentially asks that each of the partial derivatives
at the point exists and is continuous near the point of differentiation.
\[z - z_0 = \ddelat{f}{x}{(x_0, y_0)}(x - x_0)\]
Is the equation for a line with gradient \(f_x(x_0, y_0)\). The \(y\) variant
is similar and combining them we find 
\[z - z_0 = \ddelat{f}{x}{(x_0, y_0)}(x - x_0) 
+ \ddelat{f}{y}{(x_0, y_0)}(y - y_0)\]
Where the vertical bar notation indicates that these derivatives are calculated
at \((x_0, y_0)\). This is a specification of the general form of a plane
passing through the point \((x_0, y_0, z_0)\)
\[z - z_0 = \alpha(x - x_0) + \beta(y - y_0)\]

\subsubsection*{Example}

\begin{center}
    Find the equation of the tangent plane to the surface
    \(z = f(x, y) = 2x^2 + y^2\) at the point \((1, 1, 3)\).
    \[(x_0, y_0, z_0) = (1, 1, 3)\]
    \[\ddel{f}{x} = 4x\mcom \ddel{f}{y} = 2y \Rightarrow
    \ddelat{f}{x}{(x_0, y_0)} = 4\mcom \ddelat{f}{y}{(x_0, y_0)} = 2\]
    \[z - 3 = 4(x - 1) + 2(y - 1) \Rightarrow z = 4x + 2y - 3\]
\end{center}

The tangent plane is simply found by calculating the two partial derivatives,
calculating their values at the point and finding the resultant plane.

\subsubsection*{Linear Approximations}

With single variable functions we can approximate the behaviour of a function
over a small area using the tangent line to the function at that point.
\[f(a + \Delta x) \approx f(a) + f^\prime(a)(\Delta x)\]
For two variable functions, we can use the tangent plane function as our
linear approximation. If we take \(\Delta x = x - x_0\) and likewise for the
other variables we can find
\[\Delta f \approx \ddelat{f}{x}{(x_0, y_0)}\Delta x
+ \ddelat{f}{y}{(x_0, y_0)}\Delta y\]

\subsubsection*{Example}

\begin{center}
    \(z = f(x, y) = x^2 + 3xy - y^2\). If \(x\) changes from \(2\) to \(2.05\)
    and \(y\) from \(3\) to \(2.96\), estimate the change in \(z\).
    \[z_0 = f(2, 3) \Rightarrow (x_0, y_0, z_0) = (2, 3, 9)\]
    \[\ddel{f}{x} = 2x + 3y\mcom \ddel{f}{y} = 3x - 2y \Rightarrow
    \ddelat{f}{x}{(x_0, y_0)} = 13\mcom \ddelat{f}{y}{(x_0, y_0)} = 0\]
    \[\Delta f \approx 13 \times 0.05 + 0 \times -0.04 = 0.65\]
\end{center}

\subsection*{Second Order Partial Derivatives}

For a given function \(f : \R^2 \rightarrow \R\) there are four possible second
partial derivatives.

\[f_{xx} = (f_x)_x = \ddel{}{x}\left(\ddel{f}{x}\right) = \ddel{^2f}{x^2}\]
\[f_{yy} = (f_y)_x = \ddel{}{y}\left(\ddel{f}{y}\right) = \ddel{^2f}{y^2}\]
\[f_{xy} = (f_x)_y = \ddel{}{y}\left(\ddel{f}{x}\right)
= \ddel{^2f}{y\partial x}\]
\[f_{yx} = (f_y)_x = \ddel{}{x}\left(\ddel{f}{y}\right)
= \ddel{^2f}{x\partial y}\]

While \(f_{xy}\) and \(f_{yx}\) are not necessarily equal, we have a theorem
which states that \(f_{xy} = f_{yx}\) when both exist and are continuous.

\subsubsection*{Example}

\[\ddel{^2}{x^2}\left[x\sin(x + 2y)\right]\]
\[f_x = \sin(x + 2y) + x\cos(x + 2y)\]
\[f_{xx} = \cos(x + 2y) + cos(x + 2y) - x\sin(x + 2y)\]

\subsubsection*{Two Variable Chain Rule}

For a function \(z = f(x, y)\) with \(x = g(t)\) and \(y = h(t)\) where \(x\)
and \(y\) are differentiable functions, then \(z = f(g(t), h(t))\) is a
function of \(t\) and we have
\[\dd{z}{t} = \ddel{z}{x}\dd{x}{t} + \ddel{z}{y}\dd{y}{t}\]
i.e. because \(x\) and \(y\) are solely dependent on \(t\) and \(z\) is a
composition of these two, \(z\) is solely dependent on \(y\). The rate of
change of \(z\) is given by the change in \(x\) plus the change in \(y\).

\subsubsection*{Example}

\[z = x^2 - y^2\mcom x = \sin(t)\mcom y = \cos(t)\]
\[\dd{z}{t}\left(\frac{\pi}{6}\right) = ?\]
\[\dd{z}{t} = \ddel{z}{x}\dd{x}{t} + \ddel{z}{y}\dd{y}{t}\]
\[\dd{x}{t} = \cos(t)\mcom \dd{y}{t} = -\sin(t)\]
\[\ddel{z}{x} = 2x\mcom \ddel{z}{y} = -2y\]
\[\dd{z}{t} = 2x\cos(t) + 2y\sin(t) = 2\sin(t)\cos(t) + 2\cos(t)\sin(t)
= 4\sin(t)\cos(t)\]
\[\dd{z}{t}\left(\frac{\pi}{6}\right) =
4\sin\left(\frac{\pi}{6}\right)\cos\left(\frac{\pi}{6}\right)
= 4\times\frac{1}{2}\times\frac{\sqrt{3}}{2} = \sqrt{3}\]

\bigskip

In the case that \(z = f(x, y)\) and \(x = g(s, t), y = h(s, t)\) then \(z\) is
a function of both \(s\) and \(t\), with partial derivatives given by

\[\ddel{z}{s} = \ddel{z}{x}\ddel{x}{s} + \ddel{z}{y}\ddel{y}{s}\mcom
\ddel{z}{t} = \ddel{z}{x}\ddel{x}{t} + \ddel{z}{y}\ddel{y}{t}\]

So the change is \(z\) with respect to \(s\) is given by the sum of changes in
\(x\) and \(y\) with respect to \(s\) multiplied by the change in \(z\) with
respect to these functions.

\subsection*{Directional Derivatives}

Thus far we have looked at partial derivatives in the \(x\) and \(y\)
directions, but as we are now in three-dimensional space, we can consider a
derivative in any direction. To do this we take a unit vector \(\hat{u}\) and
then derfine the directional derivative of \(f\) to be
\[D_{\hat{u}}f\at{P_0}\]
Where \(D\) signifies that this is a directional derivative of \(f\) at the
point \(P\). With this definition of \(P_0 = (x_0, y_0)\) and
\(\hat{u} = (u_1, u_2)\) we can define parametric equations for \(x\) and \(y\)
at a ``time'' \(t\)
\[x = x_0 + tu_1\mcom y = y_0 + tu_2\]
We can then find \(D_{\hat{u}}f\at{P_0}\) as the rate of change of \(f\)
travelling along the line at time \(t = 0\).
\[\dd{}{t}f(x_0 + tu_1, y_0 + tu_2)(0)\]
\[= f_x(x_0, y_0)x^\prime(0) + f_y(x_0, y_0)y^\prime(0)\]
\[= f_x(x_0, y_0)u_1 + f_y(x_0, y_0)u_2\]
Rearranging this into a dot product it can be expressed as
\[D_{\hat{u}}f\at{P_0} = \left(\ddelat{f}{x}{P_0}, \ddelat{f}{y}{P_0}\right)
\cdot(u_1, u_2)\]
The partial derivative in this expression has a special name; the gradient
vector of \(f\), sometimes known as \(\mathrm{grad}f\). The notation for this
is \(\nabla f\).
\[\nabla f = \left(\ddel{f}{x}, \ddel{f}{y}\right)\]
Using this notation we can express the directional derivative at \(P_0\) as
\[D_{\hat{u}}f\at{P_0} = \nabla f\at{P_0}\cdot\hat{u}\]

\subsubsection*{Example}

Find the directional derivative of \(f(x, y) = xe^y\) at \((2, 0)\) pointing
towards \(\left(\frac{1}{2}, 2\right)\).

\begin{center}
    To start, we need to find our direction vector \(\hat{u}\).
    \[\left(\frac{1}{2}, 2\right) - (2, 0) = \left(-\frac{3}{2}, 2\right) = u\]
    \[\norm{u} = \sqrt{\left(-\frac{3}{2}\right)^2 + 2^2}
    = \sqrt{\frac{25}{4}} = \frac{5}{2}\]
    \[\hat{u} = \left(-\frac{3}{5}, \frac{4}{5}\right)\]
    \[\ddel{f}{x} = e^y\mcom \ddel{f}{y} = xe^y\]
    \[\Rightarrow D_{\left(-\frac{3}{5}, \frac{4}{5}\right)}f = 
    -\frac{3}{5}e^y + \frac{4}{5}xe^y\]
    \[D_{\left(-\frac{3}{5}, \frac{4}{5}\right)}f\at{(2, 0)} = 
    -\frac{3}{5}e^0 + \frac{4}{5}2e^0 = 1\]
\end{center}

\subsubsection*{Properties of Dierctional Derivatives}

A consequence of the definition of these directional derivatives is that
\[D_{\hat{i}}f = \ddel{f}{x}\mcom D_{\hat{j}} = \ddel{f}{y}\]
i.e. the directional derivative in the \(x\) direction is simply the partial
derivative with respect to \(x\), with the same true of \(y\). Some formulas
related to the directional derivative are
\[D_{\hat{u}}f = \nabla f\cdot\hat{u}
= \norm{\nabla f}\norm{\hat{u}}\cos(\theta) = \norm{\nabla f}\cos(\theta)\]
i.e. the directional derivative in direction \(\hat{u}\) of \(f\) is given by
the length of \(\nabla f\) (the vector gradient of \(f\)) multiplied by the
cosine of the angle between \(\nabla f\) and \(\hat{u}\). \par
This tells us that the directional derivative is at a maximum when
\(\cos(\theta) = 1\), which occurs when \(\theta = 0\) or when the direction
is parallel to the gradient vector. \(f\) decreases most rapidly when the two
are anti-parallel. When the two are perpendicular the directional derivative
is \(0\) and this defines a level curve of the surface. \par
Thus moving perpendicular to a level curve implies increasing or decreasing the
function most rapidly.

\subsubsection*{Stationary Points}

For a function of two variables, a stationary point is one for which
\[\nabla f = 0\]
i.e. both partial derivatives are \(0\). This occurs at points where the
tangent plane is flat. Just like in single input functions, we have local
minima and local maxima. However by adding another variable which introduce
saddle points, where the function increases with respect to one variable
and decreases with respect to the other as you move away from the stationary
point. \par
A critical point is a point that is either a stationary point or a point where
one of the partial derivatives does not exist, such as the point of a cone.

\subsubsection*{Second Derivative Test}

For single variable functions, a positive second derivative implies upward
concavity or convex, a negative second derivative implies downward concavity,
and a \(0\) second derivative tells us little. For two variable functions there
are four possible second derivatives, so we use the Hessian function instead.
\[H(x, y) = f_{xx}f_{yy} - (f_{xy})^2\]
We use this function for a point where \(\nabla f(x_0, y_0) = 0\). Depending
on the value of \(H(x_0, y_0)\) we can make some statements about the behaviour
of the function

\begin{itemize}
    \item If \(H(x_0, y_0) > 0\) and \(f_{xx}(x_0, y_0) > 0\), the point is a
        local maximum, because for the
        function to be positive the first term must be positive and if both
        \(f_{xx}\) and \(H\) are positive then \(f_{yy}\) must be positive.
    \item If \(H(x_0, y_0) > 0\) and \(f_{xx}(x_0, y_0) < 0\), the point is a
        local minimum, because for the function to be positive both \(f_{xx}\)
        and \(f_{yy}\) must be negative.
    \item If \(H(x_0, y_0) < 0\) the point is a saddle point, as one of
        \(f_{xx}\) or \(f_{yy}\) are mathematically guaranteed to be positive
        while the other is negative.
    \item If \(H(x_0, y_0) = 0\) the test is inconclusive.
\end{itemize}

\end{flushleft}
\end{document}
