\documentclass[12pt]{report}

\usepackage{import}
\import{../}{preamble.tex}

\newenvironment{amatrix}[1]{
    \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{
    \end{array}\right]
}

\begin{document}
\begin{flushleft}

\section*{Linear Equations}

\subsection*{Systems of Linear Equations}
Systems of equations and row operations.

For example, consider a network of flows; nodes with a given
inflow and outflow. To compute flows in this network we can use a
system linear equations.

Data fitting using a polynomial. Sometimes, we want to find a
function of a certain form which fits to a set of data points. To
find the relevant coefficients, we can use a system of linear
equations.

In general, we will take the variables in a linear equation to
be \(x_1, x_2, \ldots, x_n\) and the coefficients to be
\(a_1, \ldots, a_n\).

A finite collection of linear equations of a given set of variables
is called a \textit{system of linear equations} or a \textit{linear system}.
\[x_1 + 5x_2 + 6x_3 = 100\]
\[x_2 - x_3 = -1\]
\[-x_1 + x_3 = 11\]
Here, despite missing \(x_1\) and \(x_2\) respectively, the second and third
equations are still part of the same system as they implicitly have a term
with a \(0\) coefficient. \par
The organisation of the above system, with all variables on the right and
constants at left is the standard form of presenting a system.

A \textit{homogenous} linear system is one where all of the constants at right
are \(0\). These systems are easier to solve, and by solving a homogenous
version of a non-homogenous system, we can find a solution to the non-homogenous
variant.

A solution to a linear system is a set of values for variables that cause all
equations in the system to be true.

\subsubsection*{Solving by Elimination}

\[(1):\: 2x - y = 3\]
\[(2):\: x + y = 0\]

\[(2) \Rightarrow y = -x\]
\[(2) \& (1) \Rightarrow 2x - (-x) = 3 \Rightarrow 3x = 3\]
\[x = 1 \Rightarrow y = -1\]

A key to the applicability of this method is that we can divide by the
coefficients, which will not always be a valid assumption. This method can
be implemented algorithmically and will always either yield a solution or tell
you there is none.

\subsubsection*{Matrices}

Really, the variables in a linear system aren't really important; it is simply
the coefficients which define their relations. A matrix, a rectangular array of
numbers, can be used to store these values. A \(p \times q\) matrix has \(p\)
rows and \(q\) columns.

A \textit{augmented matrix} for a linear system is the matrix formed from the
coefficients in the equations and the constant terms, separated by a vertical
line. For example

\[
    \begin{array}{r}
        2x - y = 3 \Rightarrow 2x + -1y = 3 \\
        x + y = 0 \Rightarrow 1x + 1y = 0
    \end{array}
    =
    \begin{amatrix}{2}
        2 & -1 & 3 \\ 1 & 1 & 0
    \end{amatrix}
\]

With coefficients at left and constants at right. The number of rows should be
equal to the number of equations. Each column corresponds to a given variable
in the equations.

We can perform some \textit{elementary row operations} to such a matrix without
changing its solutions. These are
\begin{itemize}
    \item Interchanging two rows
    \item Multiplying a row by a non-zero constant
    \item Adding a multiple of a row to another row
\end{itemize}

\subsubsection*{Row-echelon Form}

A leading entry is the leftmost non-zero entry in a row of the matrix. A matrix
is in row-echelon form if and only if
\begin{itemize}
    \item For any row with a leading entry, all elements below that entry in the
        same column as it are zero.
    \item For any two rows, the leading entry of a lower row is further right than
        the leading entry of the higher row.
    \item Any row consisting solely of zeros is lower than any row with non-zero
        entries.
\end{itemize}
For example, of the below the first and second are in row-echelon form, while
the third

\[
    \left[\begin{array}{ccccc}
        1 & -2 & 3 & 4 & 5
    \end{array}\right]
    \begin{amatrix}{3}
        1 & 0 & 0 & 3 \\
        0 & 4 & 1 & 2 \\
        0 & 0 & 0 & 3 \\
    \end{amatrix}
    \begin{amatrix}{4}
        0 & 0 & 0 & 2 & 4 \\
        0 & 0 & 3 & 1 & 6 \\
        0 & 0 & 0 & 0 & 0 \\
        2 & -3 & 6 & -4 & 9 \\
    \end{amatrix}
\]

is not in row-echelon form. The first condition fails, as while the first row
has a leading entry, non-zero entries follow in the same column. In addition,
the rows are not correctly ordered by earliest leading entry.

\subsubsection*{Gaussian Elimination}

Gaussian elimination is a recursive algorithm for putting matrices into
row-echelon form. 

\begin{itemize}
    \item First switch rows around to bring a non-zero entry to the topmost
        leftmost position possible.
    \item After this, use suitable multiples of the first row to zero all
        entries in the same column.
    \item Repeat from step one, ignoring the first row.
\end{itemize}

\subsubsection*{Example}


\[
    \begin{array}{c}
        x - 3y + 2z = 11 \\
        2x - 3y - 2z = 13 \\
        4x - 2y + 5z = 31 \\ 
    \end{array}
    \Rightarrow
    \begin{amatrix}{3}
        1 & -3 & 2 & 11 \\
        2 & -3 & -2 & 13 \\
        4 & -2 & 5 & 31 \\
    \end{amatrix}
    \begin{array}{c}
        R_2 \rightarrow R_2 - 2R_1 \\
        R_3 \rightarrow R_3 - 4R_1
    \end{array}
    \rightarrow
    \begin{amatrix}{3}
        1 & -3 & 2 & 11 \\
        0 & 3 & -6 & -9 \\
        0 & 10 & -3 & -13 \\
    \end{amatrix}
\]
\[
    R_3 \rightarrow R_3 - \frac{10}{3}R_2
    \rightarrow
    \begin{amatrix}{3}
        1 & -3 & 2 & 11 \\
        0 & 3 & -6 & -9 \\
        0 & 0 & 17 & 17 \\
    \end{amatrix}
    \Rightarrow
    \begin{array}{c}
        x -3y + 2z = 11 \\
        0x + 3y -6z = -9 \\
        0x + 0y + 17z = 17            
    \end{array}
\]
\[\Rightarrow z = 1 \Rightarrow 3y - 6 = -9 \Rightarrow y = -1\]
\[y = -1 \:\&\: z = 1 \Rightarrow x = 6\]

While here we converted the matrix back to equations before solving, we could
instead perform back substitution while still in matrix form. In this case, we
are converting the matrix to \textit{reduced row-echelon form}. A matrix in this
form has the following properties
\begin{itemize}
    \item It is in row-echelon form
    \item Each leading entry is equal to \(1\)
    \item In each column containing a leading \(1\), all other entries are \(0\)
\end{itemize}
Note that for a matrix, the reduced row-echelon form is unique.

\subsubsection*{Gauss-Jordan Elimination}

Gauss-Jordan elimination is an algorithm to convert matrices to reduced
row-echelon form. 
\begin{itemize}
    \item Use Gaussian elimination to reduce to row-echelon form.
    \item Multiply each non-zero row by an appropriate number to create a
        leading \(1\).
    \item Add row multiples to create zeros above leading entries.
\end{itemize}

\subsubsection*{Example}

Continuing from the earlier example, where we found a row-echelon form for a
linear sytem.
\[
    \begin{amatrix}{3}
        1 & -3 & 2 & 11 \\
        0 & 3 & -6 & -9 \\
        0 & 0 & 17 & 17 \\
    \end{amatrix}
    \begin{array}{c}
        R_2 \rightarrow \frac{1}{3} R_2 \\
        R_3 \rightarrow \frac{1}{17} R_3
    \end{array}
    \begin{amatrix}{3}
        1 & -3 & 2 & 11 \\
        0 & 1 & -2 & -3 \\
        0 & 0 & 1 & 1 \\
    \end{amatrix}
    \begin{array}{c}
        R_2 \rightarrow R_2 + 2R_3 \\
        R_1 \rightarrow R_1 - 2R_3
    \end{array}
\]
\[
    \begin{amatrix}{3}
        1 & -3 & 0 & 9 \\
        0 & 1 & 0 & -1 \\
        0 & 0 & 1 & 1 \\
    \end{amatrix}
    R_1 \rightarrow R_1 + 3R_2
    \begin{amatrix}{3}
        1 & 0 & 0 & 6 \\
        0 & 1 & 0 & -1 \\
        0 & 0 & 1 & 1 \\
    \end{amatrix}
    \Rightarrow
    \begin{array}{c}
        x = 6 \\
        y = -1 \\
        z = 1 \\
    \end{array}
\]
In this case, it is clear that the reduced row echelon form is unique.
With this, we have our general process for solving linear systems.

\begin{itemize}
    \item First, create the augmented matrix from the equations.
    \item Use Gaussian elimination to get to row-echelon form.
    \item From here one can either directly back-substitute or use Gauss-Jordan
        elimination to get to reduced row-echelon form.
    \item At this stage, either the solutions will be evident, or it will be
        clear either that there are no or infinitely many solutions.
\end{itemize}

\subsubsection*{Solutions}

\begin{itemize}
    \item If a system of linear equations has no solutions, the system is
        inconsistent. In this case, row operations can be used to rearrange for a
        clear logical failure.
    \item If the system has exactly one or infinitely many solutions, it is
        consistent. Every homogenous system is consistent as their is the
        solution of zeroing all variables.
\end{itemize}

An inconsistent system can be identified by reducing to row-echelon form. In
this case, if there is a row with a leading entry of the solution (i.e. all
coefficients are zero and the solution is non-zero). \par
For a consistent system with one solution, there will be \(n\) non-zero rows for
an \(n\) coefficient matrix. For a consistent system with an infinite number of
solutions, there will be one or more rows of all zeros upon conversion to
reduced row-echelon form. \par
In the case of an infinite number of solutions, the columns lacking a leading
entry in will appear in some other equations. The values for these
variables can then be chosen abritrarily, implying the values of other
variables. Some examples

\[
    \begin{amatrix}{4}
        1 & 0 & 0 & 1 & 9 \\
        0 & 1 & 0 & -1 & 1 \\
        0 & 0 & 1 & 1 & 6 \\
        0 & 0 & 0 & 0 & 0 \\
    \end{amatrix}
    \begin{amatrix}{5}
        1 & 2 & 0 & 0 & 5 & 1 \\
        0 & 0 & 1 & 0 & 6 & 2 \\
        0 & 0 & 0 & 1 & 7 & 3 \\
        0 & 0 & 0 & 0 & 0 & 0 \\
    \end{amatrix}
\]

Here, in the left matrix the \(x_4\) variable can be chosen as any value, and
will set the values of the other columns. Likewise in the right matrix, \(x_2\)
and \(x_5\) lack singly defined values. \par
In general, for a consistent system with \(n\) variables, there is one free
variable for each variable without a leading entry.

\section*{Matrices}

A matrix is a rectangular array of numbers with \(m\) rows and \(n\) columns.

\[  A = 0_{2,3} =
    \left[\begin{array}{ccc}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
    \end{array}\right]
    \hspace{2cm}
    A_{1,2} = 3 \Rightarrow
    A = \left[\begin{array}{ccc}
        0 & 3 & 0 \\
        0 & 0 & 0 \\
    \end{array}\right]
\]

\subsubsection*{Special Matrices}

Some special matrix arrangements exist.
\begin{itemize}
    \item A \textit{square matrix} has the same number of rows and columns.
    \item A matrix with a single row is a \textit{row matrix} or \textit{row
        vector}
    \item A square matrix with \(A_{ij} = 0\) for \(i \neq j\) is called a
        diagonal matrix.
    \item A square matrix with \(A_{ij} = 0\) for \(i > j\) is called an
        \textit{upper triangular matrix}.
    \item A square matrix with \(A_{ij} = 0\) for \(i < j\) is a \textit{lower
        triangular matrix}.
    \item A matrix where all entries are \(0\) is a \textit{zero matrix}, for
        a specific size denoted \(0_{m,n}\).
    \item An identity matrix is a diagonal matrix with all non-zero fields equal
        to \(1\), denoted \(I_{m,n}\).
\end{itemize}

\subsection*{Operations}

\subsubsection*{Trace}

The \textit{trace} of a \(n \times n\) matrix is given by
\[\mathrm{Tr}(A) = A_{11} + A_{22} + \ldots + A_{nn}\]
i.e. the sum of the diagonal elements. \par

\subsubsection*{Scalar Multiplication}

Scalar multiplication of a matrix is performed by multiplying every entry by
the relevant scalar.
\[
    2 \cdot
    \left[\begin{array}{cc}
        1 & 2 \\
        1 & -3 \\
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        2 & 4 \\
        2 & -6 \\
    \end{array}\right]
\]
Naturally \(1 \cdot A = A\) and \(0 \cdot A = 0_{m,n}\).

\subsubsection*{Matrix Addition}

For two matrices of the same size, we add entrywise to find the sum. To subtract
we simply add \(-1 \cdot B\).

\[
    A = 
    \left[\begin{array}{ccc}
        2 & 0 & -3 \\
        1 & -1 & 3 \\
    \end{array}\right]
    , B =
    \left[\begin{array}{ccc}
        -1 & -1 & 1 \\
        0 & 1 & 2 \\
    \end{array}\right]
\]
\[
    A + 3B =
    \left[\begin{array}{ccc}
        2 & 0 & -3 \\
        1 & -1 & 3 \\
    \end{array}\right]
    +
    \left[\begin{array}{ccc}
        -3 & -3 & 3 \\
        0 & 3 & 6 \\
    \end{array}\right]
    =
    \left[\begin{array}{ccc}
        -1 & -3 & 0 \\
        1 & 2 & 9 \\
    \end{array}\right]
\]

Matrix addition has the same properties as addition of scalar addition including
commutativity and associativity.

\subsubsection*{Matrix Multiplication}

For two matrixes \(A\) of size \(m \times n\) and \(B\) of size \(n \times q\),
we can perform multiplication as \(B\) has the same number of rows as \(A\) does
columns. Unlike addition it is not performed elementwise. It is performed
according to
\[(AB)_{ij} = \sum_{k = 1}^n A_{ik}B_{kj}\]
This can be thought of as a dot product between the relevant row of \(A\) with
the relevant column of \(B\).
\[(AB)_{ij} = (\mathrm{row}\:i\:\mathrm{of}\:A) \cdot
(\mathrm{column}\:j\:\mathrm{of}\:B)\]
The output size will be the number of rows from the first matrix \(m\) and the
number of columns in the second, \(q\).

\[
    \left[\begin{array}{cc}
        1 & -1 \\
        3 & 0 \\
        0 & 1 \\
    \end{array}\right]
    \cdot
    \left[\begin{array}{cc}
        4 & 0 \\
        -7 & 1 \\
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        1 \cdot 4 + -1 \cdot -7 & 1 \cdot 0 + -1 \cdot 1 \\
        3 \cdot 4 + 0 \cdot -7 & 3 \cdot 0 + 0 \cdot 1 \\
        0 \cdot 4 + 1 \cdot -7 & 0 \cdot 0 + 1 \cdot 1 \\
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        11 & -1 \\
        12 & 0 \\
        -7 & 1 \\
    \end{array}\right]
\]

Note that this process is not commutative. \(AB \neq BA\). For example
\[
    A =
    \left[\begin{array}{cc}
        1 & 0 \\
        2 & 3 \\
    \end{array}\right]
    , B =
    \left[\begin{array}{cc}
        4 & 3 \\
        2 & 1 \\
    \end{array}\right]
\]
\[
    AB =
    \left[\begin{array}{cc}
        1 \cdot 4 + 0 \cdot 2 & 1 \cdot 3 + 0 \cdot 1 \\
        2 \cdot 4 + 3 \cdot 2 & 2 \cdot 3 + 3 \cdot 1 \\
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        4 & 3 \\
        14 & 9 \\
    \end{array}\right]
\]
\[
    BA =
    \left[\begin{array}{cc}
        4 \cdot 1 + 3 \cdot 2 & 4 \cdot 0 + 3 \cdot 3 \\
        2 \cdot 1 + 1 \cdot 2 & 2 \cdot 0 + 1 \cdot 3 \\
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        10 & 9 \\
        4 & 3 \\
    \end{array}\right]
\]

Matrix multiplication has the properties of
\begin{itemize}
    \item Left distributivity, that \(A(B + C) = AB + AC\).
    \item Right distributivity, that \((A + B)C = AC + BC\).
    \item Associative, that \(A(BC) = ((AB)C)\).
    \item \(A(\alpha B) = \alpha(AB)\)
    \item \(AI_n = I_mA = A\) where \(A\) has size \(m \times n\).
\end{itemize}
A square matrix can be multiplied by itself to effect exponentiation.

\subsection*{Adjacency Matrices}

An application of matrices is as a representation of a graph. We can use an
\(n \times n\) matrix to show the edges in a graph of \(n\) vertices.

\[
    \left[\begin{array}{c|ccccc}
        & v_1 & v_2 & v_3 & v_4 & v_5 \\
        \hline
        v_1 & 0 & 1 & 0 & 0 & 1 \\
        v_2 & 1 & 0 & 1 & 1 & 1 \\
        v_3 & 0 & 1 & 0 & 1 & 0 \\
        v_4 & 0 & 1 & 1 & 0 & 1 \\
        v_5 & 1 & 1 & 0 & 1 & 0 \\
    \end{array}\right]
\]

Here we have a \(5\) vertex graph, and we can tell from the matrix that for
instance \(v_1\) has edges to \(v_2\) and \(v_5\). A disconnected graph will
simply be \(0_n\). The adjacency matrix is always symmetrical across the
diagonal. If there are no edges looping back to their start, the diagonal will
be zeroes.

\end{flushleft}
\end{document}
